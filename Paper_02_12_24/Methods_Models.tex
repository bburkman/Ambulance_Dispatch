\subsection{Models}
\label{models}

See \verb|Ambulance_Dispatch_07_Build_Models.ipynb| for more details.  

%%%%%
\FloatBarrier
\subsubsection{Binary Classification Algorithms and Hyperparameters}
\label{algorithms}

For each of the three sets of features we used eight binary classification algorithms, three of which take class weight $\alpha$ and one of which takes the focal loss parameter $\gamma$.  (See Table \ref{models}) We learned models for various values of the hyperparameters, giving $3 \times 13 = 39$ different models.  The $\alpha=0.5$ class weight is the default, and the $\alpha = 0.85$ class weight balances the effect of the negative and positive class in the loss function, as 85\% of the samples are in the negative class.  Focal loss \citep{lin2017focal} puts more weight in the loss function on the samples that are badly classified, much like least squares regression puts more weight on the points furthest from the line.  Setting $\gamma=0.0$ has no effect; Lin's paper tested from $\gamma=0.5$ to $\gamma = 5.0$ and recommended $\gamma = 2.0$.  

\

\begin{table}[h]
\label{models}
\caption{\normalsize\normalfont Models Tested for Recommendation System.  Table accompanies \S \ref{algorithms}}
\centering
\normalsize\normalfont
\begin{tabular}{lllcc}
&&& \bf Class &\bf Focal  \cr
\bf Model & \bf Abbreviation&\bf Source &\bf Weight $\alpha$ &\bf Loss $\gamma$ \cr\hline

AdaBoost  Classifier & AdaBoost & Scikit-Learn &  \cr\hline

Balanced Bagging Classifier & Bal Bag & Imbalanced-Learn &  \cr\hline

Balanced Random Forest Classifier & Bal RF & Imbalanced-Learn & 0.5 \cr
	&&& 0.85 \cr\hline

Easy Ensemble Classifier with AdaBoost Estimator & Easy Ens & Imbalanced-Learn &  \cr\hline

KerasClassifier with the  & Keras & Keras & 0.5 & 0.0 \cr
\qquad Binary Focal Crossentropy loss function &&& 0.5 & 1.0  \cr
	&&& 0.5 & 2.0 \cr
	&&& 0.85 & 0.0 \cr\hline

Logistic Regression Classifier & Log Reg & Scikit-Learn & 0.5 \cr
	&&& 0.85 \cr\hline

Random Forest Classifier & RF & Scikit-Learn &  \cr\hline

RUSBoost Classifier & RUSBoost & Imbalanced-Learn &  \cr

\end{tabular}
\end{table}

\FloatBarrier

\subsubsection{Hyperparameter Effects}
\label{hyperparameters}

Our experience with varying the class weight and focal loss parameters was that they shifted the entire $p$ distribution, both the positive and negative class, but did not do a better job of separating the positive and negative class, as measured by the area under the curve (AUC) of the receiver operating characteristic (ROC), as illustrated in Figure \ref{hyperparameters_figure} below.  

The KerasClassifier with the Binary Focal Crossentropy loss function takes both a class weight hyperparameter $\alpha$ and a focal loss parameter $\gamma$.  Varying these hyperparameters gave us different shapes of distributions of $p$, as shown in Figure \ref{hyperparameters_figure}, which also gives the ROC AUC for two runs of each model with different random seeds.  That the model on the left has both the least and the greatest ROC AUC illustrates that, between models of the same algorithm with different class and focal weights, the effectiveness of the model in separating the positive and negative classes depends as much on the random seed as on the choice of weights.  

If one is using the default decision threshold $\theta = 0.5$, then these hyperparameters are useful for shifting the distribution to meet the threshold, but since we are taking the liberty to move the threshold, varying the hyperparameters may be of little use.  Since the ROC AUC quantifies how well the algorithm separates the positive and negative classes over the whole range of $p$, and we are most interested in a small range of $p$ on the right end, the weights may yet have some useful effect, a topic for future investigation (\S\ref{simplifying_assumptions}).

%%%
\begin{figure}[h]
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.3in} }
	\vskip 0pt
	\hfil {\normalfont\normalsize $\alpha = 0.5$, $\gamma = 0.0$}
	\vskip 4pt
	
	{\normalfont\normalsize \ $\text{ROC AUC} \in \{ 0.777402, 0.778653 \}$ }
	
	
	\input{../Keras/Images/KBFC_alpha_0_5_gamma_0_0_Hard_Run_0_Pred.pgf}	
&
	\vskip 0pt
	\hfil {\normalfont\normalsize $\alpha = 0.5$, $\gamma = 2.0$}
	
	\vskip 4pt
	{\normalfont\normalsize \ $\text{ROC AUC} \in \{0.777903, 0.777626\}$}
		
	\input{../Keras/Images/KBFC_alpha_0_5_gamma_2_0_Hard_Run_0_Pred.pgf}	
&
	\vskip 0pt
	\hfil {\normalfont\normalsize $\alpha = 0.85$, $\gamma = 0.0$}
	
	\vskip 4pt
	{\normalfont\normalsize \ $\text{ROC AUC} \in \{0.7779, 0.778199\}$}
	
	\input{../Keras/Images/KBFC_alpha_balanced_gamma_0_0_Hard_Run_0_Pred.pgf}	
\cr
\end{tabular}
	  \caption{\normalfont\normalsize KerasClassifier with Different Hyperparameters with ROC AUC for Two Model Runs with Different Random Seeds.  Figure accompanies \S\ref{hyperparameters}}\label{hyperparameters_figure}
\end{figure}

\FloatBarrier


%%%%%
\subsubsection{Five-Fold Cross Validation}
\label{cross_validation}

As mentioned in \S\ref{political_decisions_probability}, 
we need enough samples in each band of $p$ to smooth out the randomness.  If we had more samples total, we could use smaller bands and have more numerical precision in our specification of the decision threshold $\theta$.  If we used a typical 70/30 train/test split, we would only have $p$ values for the 30\% samples in the test set.  Instead we used five-fold cross validation, having an 80/20 split five times, giving us $p$ values for all of the samples.  

%%%%%
\FloatBarrier
\subsubsection{Interpreting Supervised Learning Binary Classification Results}
\label{interpreting_ideal}

In supervised learning binary classification, a model predicts, for each sample in the test set, whether the sample is in the negative or positive class.  The model returns a value $p \in [0,1]$, that increases with the probability that the sample is in the positive class.  Additionally in supervised learning, we already know the answer to the question of whether the sample was in the negative or positive class, with $y \in \{0,1\}$ given in the dataset but hidden from the model during the test phase.  In the code, $p$ is often called \verb|y_proba| and $y$ is called \verb|y_test|.  Using these two numbers, $p$ and $y$, we can study, quantify, and illustrate how well the model predicts the actual values.  

A perfect model would entirely separate the negative and positive classes, but the ideal we can hope for is that most of the negative elements are towards the left and most of the positive elements are towards the right of the distribution.  In Figure \ref{ideal}, the data has the same class ratio as our CRSS data, with 85\% in the negative class and 15\% in the positive class.  If we choose discrimination threshold $\theta = 0.5$, the value of $p$ for which most models algorithms are optimized, the elements of the negative class with $p<\theta$ are True Negatives (TN), the elements of the negative class with $p > \theta$ are False Positives (FP), the elements of the positive class with $p < \theta$ are False Negatives (FN), and the elements of the positive class with $p > \theta$ are True Positives (TP).  

If this ideal model were our recommendation system with $\theta = 0.5$, then 38.5\% of the ambulances we immediately dispatched would be needed (Precision), and 73\% of the needed ambulances would be immediately dispatched (Recall).  If we chose a higher value of $\theta$, we would increase TN, decrease FP, increase FN, and decrease TP.  Recall would decrease, but the effect on precision is uncertain as FP and TP would both decrease.

The ROC (Receiver Operating Characteristic) curve is a parameterized curve showing the True Positive Rate (TP/P) versus the False Positive Rate (FP/N) as $p$ varies from 0 (upper right) to 1 (lower left).  The area under the curve (AUC) is widely used to compare the quality of models in terms of how well they separate the negative and positive classes over the entire range of $p \in [0,1]$.  For our work, however, given real-life budgetary constraints on expanding ambulance fleets, we are only interested in a small range of $p$ on the right side of the distribution, so the ROC AUC is not the primary measure we will use to choose the best model.

\begin{figure}[h]
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\hfil {\normalfont\normalsize Raw Model Output}
	
	\input{../Keras/Images/Ideal_Pred.pgf}	
&
	\vskip 0pt
	\hfil {\normalfont\normalsize ROC Curve}
	
	\input{../Keras/Images/Ideal_ROC.pgf}
&
	\normalfont\normalsize 
	\vskip 0pt
	Choosing decision
	
	\quad threshold $\theta = 0.5$:
	
	\
	
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
%66.473	18.527	3.406	11.594
66.5\% & 18.5\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
3.4\% & 11.6\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
%0.384914179	0.772933333	0.51560575
0.385 & Precision \cr	0.773 & Recall \cr	%0.516 & F1 \cr
\end{tabular}

\cr
\end{tabular}
\caption{\normalfont\normalsize Example Model Results.  Figure accompanies \S\ref{interpreting_ideal}}
\label{ideal}
\end{figure}

%%%%%
\FloatBarrier
\subsubsection{Comparing Outputs of Different Models}
\label{comparing_outputs}

The eight model algorithms not only give different results, but different kinds of results, and we have to find a way to compare them.  Ideally, we will find ways to evaluate the results that will apply to all of the kinds of model output. See Figure \ref{raw_output_figure}.  For the illustrations we have used models built on the Hard features with no class balance nor focal loss.

The ranges and shapes of the distributions are significantly different.  The Balanced Bagging and  Balanced Random Forest classifiers gives a nice spread from 0.0 to 1.0, but the AdaBoost, Easy Ensemble, and RUSBoost results are clustered in the middle, and the Random Forest on the left.  If we used the Random Forest results with the default decision threshold $\theta = 0.5$, we would never immediately dispatch an ambulance.  The KerasClassifier and Logistic Regression Classifier tend towards the left with long tails to the right.  

To give appropriate results for Balanced Bagging with a $p$ range of width 1.0 and RUSBoost with a range of width 0.002, we built into the code a variable for numerical precision that depends on the range of $p$ for that model, $\text{num\_prec} = - \lceil \log_{10} ( \text{max}(p) - \text{min}(p) )  \rceil$, to which we can then add the number of digits appropriate for the purpose.  

The numerics in the model results are also different.  Most of the models give, for the 713,566 samples, at least 100,000 unique values of $p \in [0,1]$, making the discrete results nearly continuous.  The AdaBoost, KerasClassifier, Logistic Regression, and RUSBoost algorithms are fairly consistent with about 140,000 unique values of $p$ for results on the Easy features, 640,000 for the results on the Medium features, and 700,000 on the Hard features.  The Random Forest classifier gives about half as many unique results, but proportionally for the three features sets.

The outliers in the numerics are the Balanced Random Forest, Balanced Bagging, and Easy Ensemble.  The Balanced Random Forest method on the Hard features gives less than 4,000 unique values of $p$ on the Hard features because 93\% of its $p$ values are rounded to two decimal places.  The Balanced Bagging results on the Hard features have less than 300 unique values because 99\% of its $p$ values are rounded to one decimal place.  While the other algorithms had fewer unique values for the Medium features and far fewer for the Easy, the Balanced Random Forest and Balanced Bagging have more for the Medium and far more for the Easy, with the number of unique Easy results being comparable to those of the other algorithms.  The Easy Ensemble results for all three features sets have about 50\% of the results rounded to two decimal places.  For full details see \verb|Analyze_Proba/Value_Counts_y_proba.csv|.





\input{Methods_Models_Raw_Figure_Short}

\FloatBarrier

