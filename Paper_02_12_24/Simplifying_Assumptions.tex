\section{Simplifying Assumptions and Opportunities for Future Research}
\label{simplifying_assumptions}

All models are simplifications, and we should acknowledge the most egregious of our simplifying assumptions.  Some of the simplifying assumptions may hint at opportunities for future research.  

\

\begin{tabular}{rp{5in}}
%\begin{longtable}{rp{5in}}
	\bf Section & \bf Simplifying Assumption \cr \hline
	\S\ref{political_decisions} & All ambulances sent based on calls from eyewitnesses are actually needed. \cr
	& All automated crash notifications from cell phones refer to an actual crash, not just hard braking, {\it i.e.} the notifications have no false positives. \cr
	\S\ref{dataset} & The class ratio (P/N) in the automated crash notification from cell phones will be close to that in the CRSS data, $1/5$. \cr
	\S\ref{dataset} & The crash persons in the CRSS data are representative of the future crash persons whose cell phones send a crash notification.  (Several layers to unpack here) \cr
	\S\ref{features} & The data features we seek for each automated crash notification will be available and accurate. \cr
\end{tabular}	
%\end{longtable}


\

\begin{tabular}{rp{5in}}
%\begin{longtable}{rp{5in}}
	\bf Section & \bf Opportunities for Future Research \cr \hline
	\S\ref{dataset} & Find data on crashes that spawned an automated notification from a cell phone. \cr
	\S\ref{features} & We tested only three sets of features and did not test to see which features or combinations of features were most useful in predicting needing an ambulance.  \cr
	\S\ref{hyperparameters} & We found that varying the hyperparameters for class weight and focal loss did not significantly vary the ROC AUC, a measure of how well the model separates the positive and negative classes over the whole range $p \in [0,1]$.  Do those hyperparameters make a difference in how well the model separates the positive and negative classes on the right tail of the $p$ distribution, the part relevant to our work? \cr
	\S\ref{finding_theta} & Find a better way to slice the $p$-values into intervals such that the target metrics are monotonic functions of the intervals.  \cr
	\S\ref{scenario_reprise} & Can we find better combinations of data and model algorithms that will better separate the positive and negative classes? \cr
\end{tabular}	
%\end{longtable}

