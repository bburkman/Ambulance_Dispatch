%%%%%
\subsection{Interpreting Model Results}
\label{interpreting}

%%%
\subsubsection{Finding $\theta$ for Each Budgetary Decision Metric}
\label{finding_theta}

For each budgetary metric and each model we need to find a corresponding decision threshold $\theta$.  For each new crash notification the model outputs a prediction value $p$, and if $p \ge \theta$ the system recommends immediately dispatching an ambulance.  

For each of the 713,566 samples we use from the CRSS dataset we have a target value $y \in \{0,1\}$ that tells whether the sample is in the Neg or Pos class.  Each model returns for each of those historical samples a value $p \in [0,1]$ that indicates whether the model predicts that the sample is in the positive and negative class.  For each model we need to find the value of $p$ that corresponds to each of the budgetary decision criteria to set as the decision threshold $\theta$.  

To illustrate our methods for finding $\theta$ for each budgetary decision metric and each model we use the results of the Random Forest Classifier on the hard features, which returns values  $p \in [0.10619,0.311074]$.  Table \ref{RFC_Hard_0_Slices}  shows some of the results.  We have rounded $p$ to four decimal places and sorted model results by increasing $p$, giving the number of elements of the negative and positive classes for each $p$.  From Neg and Pos we can calculate the TN, FP, FN, and TP we would have if we chose $\theta$ to be that value of $p$.  The number of negative and positive samples are constant at $N=605,610$ and $P=107,956$. TN is the cumulative sum of Neg, and $\text{FP} = \text{N} - \text{TN}$; similarly for FN and TP.  

In Table \ref{RFC_Hard_0_Slices}, we have shown the results for four bands of $p$.  The first and fourth are the beginning and end of the range of $p$.  The second range shows where $\text{FP/P} \approx 0.05$, the first budgetary decision metric.  The third range illustrates that while Precision generally increases from 0.15 (the class ratio) to 1, it is not perfectly nondecreasing, even with $p$ rounded to four decimal places.  Note in the fourth column, $m$Prob, the values generally increase from 0 to 1 but not in a smooth way.  We have to significantly smooth that metric to find the value of $p$ where it equals 50\%.  The full table is at 
\verb|Analyze_Proba/RFC_Hard_Run_0_Round_4_Rolling_Intervals|.

Our {\bf first budgetary decision metric}, $\text{FP}/\text{P}$, is a nonincreasing function of $p$ because $P$ is constant and FP is P minus the cumulative sum of nonnegative integers Neg.  Because $\text{FP}/\text{P}$ is a nonincreasing function of $p$, we can choose $\theta$ to be the value of $p$ where $\text{FP}/\text{P}$ is closest to $0.05$.  For this run of the Random Forest Classifier with the hard features we can choose $\theta = 0.2580$, and a second run with a different random seed gave $\theta = 0.2583$, so we can say with some confidence that we should choose $\theta = 0.258$ for this model.  

\begin{table}
\caption{
	\normalsize\normalfont
	Metrics on $p$ Output of Random Forest Classifier on the Hard Features.  Table accompanies \S\ref{finding_theta}
}
\label{RFC_Hard_0_Slices}

{\normalsize
\normalfont
\begin{tabular}{lrrlrrrrlll}
\toprule
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} &     
	\multicolumn{1}{c}{TN} &      
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{FN} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{Prec} &    
	\multicolumn{1}{c}{Rec} &   
	\multicolumn{1}{c}{FP/P} \\
\midrule
0.1062 & 1 & 0 & 0.0000 & 1 & 605,609 & 0 & 107,956 & 0.1513 & 1.0000 & 5.6098\cr
0.1063 & 86 & 2 & 0.0227 & 87 & 605,523 & 2 & 107,954 & 0.1513  & 1.0000 & 5.609\cr
0.1064 & 51 & 0 & 0.0000 & 138 & 605,472 & 2 & 107,954 & 0.1513 & 1.0000 & 5.6085\cr
%0.1065 & 110 & 0 & 0 & 248 & 605,362 & 2 & 107,954 & 0.1513 & 5.6075\cr
\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{l}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
%\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{c}{\vdots} \cr

0.2579 & 33 & 40 & 0.5479 & 600,161 & 5,449 & 102,100 & 5,856 & 0.518 & 0.0542 & 0.0505\cr
0.258 & 39 & 26 & 0.4000 & 600,200 & 5,410 & 102,126 & 5,830 & 0.5187 & 0.0540 & 0.0501\cr
0.2581 & 39 & 25 & 0.3906 & 600,239 & 5,371 & 102,151 & 5,805 & 0.5194 & 0.0538 & 0.0498\cr
0.2582 & 42 & 31 & 0.4247 & 600,281 & 5,329 & 102,182 & 5,774 & 0.5200 & 0.0535 & 0.0494\cr

\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{l}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
%\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{c}{\vdots} \cr

0.2858 & 4 & 8 & 0.6667 & 605,179 & 431 & 106,858 & 1,098 & 0.7181 & 0.0102 & 0.0040\cr
0.2859 & 1 & 9 & 0.9000 & 605,180 & 430 & 106,867 & 1,089 & 0.7169 & 0.0101 & 0.0040\cr
0.286 & 4 & 11 & 0.7333 & 605,184 & 426 & 106,878 & 1,078 & 0.7168 & 0.0100 & 0.0039\cr
0.2861 & 1 & 10 & 0.9091 & 605,185 & 425 & 106,888 & 1,068 & 0.7153 & 0.0099 & 0.0039\cr
0.2862 & 5 & 6 & 0.5455 & 605,190 & 420 & 106,894 & 1,062 & 0.7166 & 0.0098 & 0.0039\cr


\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{l}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{r}{\vdots} & 
\multicolumn{1}{c}{\vdots} & 
%\multicolumn{1}{c}{\vdots} & 
\multicolumn{1}{c}{\vdots} \cr

%0.3083 & 0 & 1 & 1 & 605,609 & 1 & 107,950 & 6 & 0.8571 & 0\cr
%0.3086 & 1 & 0 & 0 & 605,610 & 0 & 107,950 & 6 & 1 & 0\cr
%0.3087 & 0 & 1 & 1 & 605,610 & 0 & 107,951 & 5 & 1 & 0\cr
%0.3092 & 0 & 1 & 1 & 605,610 & 0 & 107,952 & 4 & 1 & 0\cr
0.3097 & 0 & 1 & 1.0000 & 605,610 & 0 & 107,953 & 3 & 1.0000 & 0.0000 & 0.0000\cr
0.3108 & 0 & 1 & 1.0000 & 605,610 & 0 & 107,954 & 2 & 1.0000 & 0.0000 & 0.0000\cr
0.3111 & 0 & 2 & 1.0000 & 605,610 & 0 & 107,956 & 0  & nan & 0.0000 & 0.0000\cr

\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

Table \ref{RFC_Hard_0_Slices} shows that our {\bf second budgetary decision metric}, $\text{Precision} = \text{TP}/(\text{FP} + \text{TP})$, generally increases from 0.15 to 1 (0.15 is the class ratio $\text{P}/(\text{N} + \text{P})$), but is not an increasing function of $p$ at that level of granularity.  

To go from one $p$ to the next, or from one band of $p$ to the next, 

\begin{equation} \label{eq:delta_p}\hfil
\frac{\Delta \text{TP}}{\Delta p} = -\text{Neg} 
\qquad \text{ and }\qquad
\frac{\Delta \text{FP}}{\Delta p} = -\text{Pos} 
\end{equation}

\FloatBarrier

For large changes in $p$, $\text{Prec}(p)$ is an increasing function, but what do we mean by ``large''?  In Equation \ref{eq:delta_p_2} we assume that $\text{Prec}(p)$ increases on an interval of length $\Delta p$, then we derive a relationship that tells us the proportion of Neg to Pos we need in that interval:  

\begin{equation} \label{eq:delta_p_2}\hfil
\begin{aligned}
	\text{Prec}(p) &< \text{Prec}(p + \Delta p) \cr
	\frac{\text{TP}}{\text{FP} + \text{TP}} &< 
	\frac{\text{TP} - \text{Neg}}{(\text{FP} - \text{Pos}) + (\text{TP} - \text{Neg})} \cr 
	&\vdots \cr
	\text{FP}\cdot \text{Pos} &< \text{TP} \cdot \text{Neg} \cr
	\frac{\text{FP}}{\text{TP}} &< \frac{\text{Neg}}{\text{Pos}} \cr
\end{aligned}
\end{equation}

\FloatBarrier

Having no elements of the negative class in a $p$-interval would make precision decrease, so we cut the $p$-values into bands with some minimum number of elements of each class.  For each model, the \verb|Keras/Analyze_Proba| directory has a spreadsheet like Table \ref{RFC_Hard_50_Slices} for minimum number of class elements 5, 10, 25, 50, 100, 200, 400, 800, 1600, 3200, and 6400.  An opportunity for future research (\S\ref{simplifying_assumptions}) is to develop a method to cut the $p$ output into the smallest possible bands such that $\text{Prec}(p)$ is an increasing function.  


In Table \ref{RFC_Hard_50_Slices}, we have cut $p$ into bands such that each band has at least fifty elements of each of the negative and positive classes.  The $p$ values in the table are the average of the minimum and maximum values of $p$ in the band.  The full table is at \verb|Analyze_Proba/RFC_Easy_Run_0_50_Slices.csv|.

At this level of granularity and in this range of $p$, the precision is an increasing function of $p$, and we can say that to get $\text{Precision} = 2/3$ we should set $\theta \approx 0.276$.   Another run of the same model with different random seed gave $\theta \approx 0.279$.



\begin{table}
\caption{
	\normalsize\normalfont
	Metrics on $p$ Output of Random Forest Classifier on the Hard Features with Minimum of 50 Elements of Each Class in Each Band.  Table accompanies \S\ref{finding_theta}
}
\label{RFC_Hard_50_Slices}

{\normalsize
\normalfont
\begin{tabular}{lrrrrrrrlll}
\toprule
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} &     
	\multicolumn{1}{c}{TN} &      
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{FN} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{Prec} &    
	\multicolumn{1}{c}{Rec} &   
	\multicolumn{1}{c}{FP/P} \\
\midrule
0.27565 & 50 & 51 & 0.505 & 604,454 & 1,156 & 105,702 & 2,254 & 0.661 & 0.0209 & 0.0107 \cr
0.27608 & 50 & 59 & 0.5413 & 604,504 & 1,106 & 105,761 & 2,195 & 0.665 & 0.0203 & 0.0102 \cr
0.27667 & 50 & 79 & 0.6124 & 604,554 & 1,056 & 105,840 & 2,116 & 0.6671 & 0.0196 & 0.0098 \cr
0.27732 & 56 & 50 & 0.4717 & 604,610 & 1,000 & 105,890 & 2,066 & 0.6738 & 0.0191 & 0.0093 \cr
0.27798 & 50 & 80 & 0.6154 & 604,660 & 950 & 105,970 & 1,986 & 0.6764 & 0.0184 & 0.0088 \cr
0.27869 & 50 & 67 & 0.5726 & 604,710 & 900 & 106,037 & 1,919 & 0.6807 & 0.0178 & 0.0083 \cr
\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

For the the {\bf third budgetary decision criterion}, that the minimum probability that each immediately dispatched ambulance is needed is at least 50\%, with the metric $m\text{Prob} = \text{Pos}/(\text{Neg}+\text{Pos}) \ge 0.50$, equivalent to $\text{Pos} \ge \text{Neg}$.  

Table \ref{RFC_Hard_6400_Slices} shows that, if we zoom out to where each band has at least 6,400 elements in each class, cutting $p$ into only seventeen intervals, $m\text{Prob}$ generally increases, but even at this high level the metric decreases between $p=0.1606$ and $p = 0.1689$.  This third metric is highly volatile.  


\begin{table}
\caption{
	\normalsize\normalfont
	Metrics on $p$ Output of Random Forest Classifier on the Hard Features with Minimum of 6400 Elements of Each Class in Each Band.  Table accompanies \S\ref{finding_theta}
}
\label{RFC_Hard_6400_Slices}

{\normalsize
\normalfont
\begin{tabular}{lrrrrrrrlll}
\toprule
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} &     
	\multicolumn{1}{c}{TN} &      
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{FN} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{Prec} &    
	\multicolumn{1}{c}{Rec} &   
	\multicolumn{1}{c}{FP/P} \\
\midrule
0.1166 & 132,612 & 5,396 & 0.0391 & 132,612 & 472,998 & 5,396 & 102,560 & 0.1782 & 0.95 & 4.3814 \cr
0.1293 & 79,551 & 6,400 & 0.0745 & 212,163 & 393,447 & 11,796 & 96,160 & 0.1964 & 0.8907 & 3.6445 \cr
0.1336 & 57,673 & 6,400 & 0.0999 & 269,836 & 335,774 & 18,196 & 89,760 & 0.2109 & 0.8314 & 3.1103 \cr
0.137 & 42,946 & 6,400 & 0.1297 & 312,782 & 292,828 & 24,596 & 83,360 & 0.2216 & 0.7722 & 2.7125 \cr
0.1399 & 36,922 & 6,400 & 0.1477 & 349,704 & 255,906 & 30,996 & 76,960 & 0.2312 & 0.7129 & 2.3705 \cr
0.1432 & 35,947 & 6,400 & 0.1511 & 385,651 & 219,959 & 37,396 & 70,560 & 0.2429 & 0.6536 & 2.0375 \cr
0.1469 & 32,431 & 6,400 & 0.1648 & 418,082 & 187,528 & 43,796 & 64,160 & 0.2549 & 0.5943 & 1.7371 \cr
0.1509 & 30,786 & 6,400 & 0.1721 & 448,868 & 156,742 & 50,196 & 57,760 & 0.2693 & 0.535 & 1.4519 \cr
0.1553 & 25,477 & 6,401 & 0.2008 & 474,345 & 131,265 & 56,597 & 51,359 & 0.2812 & 0.4757 & 1.2159 \cr
0.1606 & 23,300 & 6,400 & 0.2155 & 497,645 & 107,965 & 62,997 & 44,959 & 0.294 & 0.4165 & 1.0001 \cr
0.1689 & 26,951 & 6,400 & 0.1919 & 524,596 & 81,014 & 69,397 & 38,559 & 0.3225 & 0.3572 & 0.7504 \cr
0.1806 & 21,160 & 6,400 & 0.2322 & 545,756 & 59,854 & 75,797 & 32,159 & 0.3495 & 0.2979 & 0.5544 \cr
0.1979 & 17,362 & 6,400 & 0.2693 & 563,118 & 42,492 & 82,197 & 25,759 & 0.3774 & 0.2386 & 0.3936 \cr
0.2199 & 16,065 & 6,400 & 0.2849 & 579,183 & 26,427 & 88,597 & 19,359 & 0.4228 & 0.1793 & 0.2448 \cr
0.2366 & 10,969 & 6,400 & 0.3685 & 590,152 & 15,458 & 94,997 & 12,959 & 0.456 & 0.12 & 0.1432 \cr
0.249 & 9,058 & 6,400 & 0.414 & 599,210 & 6,400 & 101,397 & 6,559 & 0.5061 & 0.0608 & 0.0593 \cr
0.2835 & 6,400 & 6,559 & 0.5061 & 605,610 & 0 & 107,956 & 0 & nan & 0 & 0 \cr
\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

We tried two ways to estimate where $m\text{Prob} = 0.50$, and both gave the same result.  

Method 1:  If we slice the $p$ results into bands with at least 100 elements of each class, partly shown in Table \ref{RFC_Hard_100_Slices}, there is no band with $p<0.2692$ with $m\text{Prob} \ge 0.5$ and no band with $p > 0.2733$ with $m\text{Prob} \le 0.5$, so we can say with some confidence that if we chose $\theta \approx 0.271$, each ambulance immediately dispatched has at least a 50\% chance of being needed.


\begin{table}
\caption{
	\normalsize\normalfont
	Metrics on $p$ Output of Random Forest Classifier on the Hard Features with Minimum of 100 Elements of Each Class in Each Band.  Table accompanies \S\ref{finding_theta}
}
\label{RFC_Hard_100_Slices}

{\normalsize
\normalfont
\begin{tabular}{lrrrrrrrlll}
\toprule
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} &     
	\multicolumn{1}{c}{TN} &      
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{FN} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{Prec} &    
	\multicolumn{1}{c}{Rec} &   
	\multicolumn{1}{c}{FP/P} \\
\midrule
0.2692 & 118 & 100 & 0.4587 & 603,564 & 2,046 & 104,738 & 3,218 & 0.6113 & 0.0298 & 0.019 \cr
0.2698 & 100 & 101 & 0.5025 & 603,664 & 1,946 & 104,839 & 3,117 & 0.6156 & 0.0289 & 0.018 \cr
0.2704 & 127 & 100 & 0.4405 & 603,791 & 1,819 & 104,939 & 3,017 & 0.6239 & 0.0279 & 0.0168 \cr
0.2711 & 108 & 100 & 0.4808 & 603,899 & 1,711 & 105,039 & 2,917 & 0.6303 & 0.027 & 0.0158 \cr
0.2718 & 111 & 100 & 0.4739 & 604,010 & 1,600 & 105,139 & 2,817 & 0.6378 & 0.0261 & 0.0148 \cr
0.2725 & 100 & 109 & 0.5215 & 604,110 & 1,500 & 105,248 & 2,708 & 0.6435 & 0.0251 & 0.0139 \cr
0.2733 & 100 & 149 & 0.5984 & 604,210 & 1,400 & 105,397 & 2,559 & 0.6464 & 0.0237 & 0.013 \cr
\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier


Method 2:  We used a rolling sum of the Neg and Pos to intervals with enough Neg and Pos to smooth out the metric $m$Prob.  With $p$ rounded to four decimal places we still have volatility at each value of $p$, and the metric gets close to the target value (0.50) many times over a large range.  If we take the rolling average of the ten or twenty rows centered at that value of $p$, we see less volatility, but even in this small interval the $m$Prob increases and decreases.  With a window size of 20, however, the only values of $p$ where the metric is close to the target are in this range, so we can choose $\theta = 0.272$, and our two methods agree.  The full chart, up to a window of size 2,000, is in \verb|Analyze_Proba/RFC_Hard_Run_0_Round_4_Rolling_Intervals|.




\begin{table}
\caption{
	\normalsize\normalfont
	Smoothing $m$Prob with Rolling Sums of Different Window Sizes.
	Table accompanies \S\ref{finding_theta}
}
\label{RFC_Hard_Run_0_Round_4_Rolling_Intervals}

{\normalsize
\normalfont
\begin{tabular}{c | rrl | rrl | rrl | rrl}
\toprule
	\multicolumn{1}{c}{Window Size} &
	\multicolumn{3}{c}{1} &
	\multicolumn{3}{c}{10} &
	\multicolumn{3}{c}{20} &
	\multicolumn{3}{c}{50} \cr\cline{1-1}

	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{|c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} & 
	\multicolumn{1}{|c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} & 
	\multicolumn{1}{|c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} & 
	\multicolumn{1}{|c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} \cr
\midrule
0.2717 & 14 & 22 & 0.6111 & 144 & 166 & 0.4645 & 303 & 320 & 0.4864 & 754 & 771 & 0.4944 \cr
0.2718 & 12 & 18 & 0.6 & 153 & 159 & 0.4904 & 310 & 315 & 0.496 & 754 & 768 & 0.4954 \cr
0.2719 & 14 & 16 & 0.5333 & 143 & 164 & 0.4658 & 307 & 323 & 0.4873 & 758 & 757 & 0.5003 \cr
0.2720 & 12 & 13 & 0.52 & 152 & 156 & 0.4935 & 300 & 323 & 0.4815 & 756 & 745 & 0.5037 \cr
0.2721 & 26 & 13 & 0.3333 & 161 & 150 & 0.5177 & 296 & 312 & 0.4868 & 768 & 737 & 0.5103 \cr
0.2722 & 10 & 22 & 0.6875 & 159 & 149 & 0.5162 & 306 & 296 & 0.5083 & 755 & 726 & 0.5098 \cr
0.2723 & 17 & 7 & 0.2917 & 159 & 147 & 0.5196 & 309 & 291 & 0.515 & 756 & 717 & 0.5132 \cr
%0.2724 & 16 & 21 & 0.5676 & 151 & 153 & 0.4967 & 308 & 290 & 0.5151 & 755 & 702 & 0.5182 \cr
\hline
\multicolumn{6}{c}{} &&&&&&\cr
\multicolumn{6}{l}{
	Range of $p$ where $\left| m\text{Prob} - 0.50 \right| < 0.01$
} &&&&&&\cr
\multicolumn{6}{c}{} &&&&&&\cr

& \multicolumn{2}{r}{min($p$)} & 0.2472 
& \multicolumn{2}{r}{min($p$)} & 0.2718 
& \multicolumn{2}{r}{min($p$)} & 0.2718 
& \multicolumn{2}{r}{min($p$)} & 0.2711
\cr 
& \multicolumn{2}{r}{max($p$)} & 0.3076
& \multicolumn{2}{r}{max($p$)} & 0.2726 
& \multicolumn{2}{r}{max($p$)} & 0.2722 
& \multicolumn{2}{r}{max($p$)} & 0.2722
\cr

\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier


\begin{comment}
Method 2:  This method is worked out in the 

\verb|RFC_Hard_0_Slices_Find_theta_Budget_Criterion_3.csv| file in the \verb|/Analysis_Spreadsheets/| folder.  

The Random Forest Classifier on the Hard features returns 458,530 unique values of $p$ with between 1 and 83 samples per value of $p$.  For each value of $p$ we know how many samples are in the Neg and Pos classes.  Order the $p$ values increasing.  For each value of $p$, make an interval with the thousand nearest values of $p$ (500 above and 500 below), and find the total number of elements of the positive and negative classes in that neighborhood.  Using those Neg and Pos values, calculate  $\text{Pos}/(\text{Neg}+\text{Pos})$ for each $p$.  Then see whether there is a small interval where $\text{Pos}/(\text{Neg}+\text{Pos}) \approx 0.50$.  We chose ``within 0.001'' to mean ``approximately.''

The first value of $p$ with $|\text{Pos}/(\text{Neg}+\text{Pos}) - 0.50| < 0.001$ was at $p=0.271584$, and the last at $p = 0.27174$.  In that range of 49 values of $p$, 21 of them satisfied the criterion.  Given those results, it is reasonable to say that $\theta \approx 0.272$ satisfies the criterion.  
\end{comment}

%%%
\subsubsection{Choosing the Best Model for each Budgetary Decision Metric}
\label{choosing_model}

For each budgetary constraint we want to find the model that, within the constraint, immediately dispatches the most ambulances to crash persons who need them, which is given as a proportion in Recall = TP/P.  Using as an example our first budgetary constraint, $\text{FP}/\text{P} = 0.05$, we need to find, for each model whether there exists a value of $p$ where $\text{FP}/\text{P}$ is close to 0.05, then find the best $\theta$ interval in that neighborhood.  Of those valid results, find the model that gives the highest Recall.  

Table \ref{FP_P_0_05_hard} shows the best results for each model algorithm.  Within these, the Balanced Random Forest Classifier gives the best results, immediately dispatching the largest proportion (11.67\%) of needed ambulances while staying within the budgetary constraint.  The KerasClassifier with the Binary Focal Crossentropy loss function is a close second, and those two are clearly better than the other six models.  

The Balanced Bagging model here is an interesting case that we cover in the next section, \S \ref{Methods_Model_Failure}.

\begin{table}[h]
\caption{\normalfont\normalsize Comparing Models:  Best results for each algorithm on the Hard features for budgetary criterion $\text{FP}/\text{P}$ closest to $0.05$.  Table accompanies \S\ref{choosing_model}}
\label{FP_P_0_05_hard}

{\normalfont\normalsize
\begin{tabular}{lcclrrrrll}
\toprule
	Algorithm & 
	\multicolumn{1}{c}{$\alpha$} & 
	\multicolumn{1}{c}{$\gamma$} & 
	\multicolumn{1}{c}{$p$} & 
	\multicolumn{1}{c}{TN} & 
	\multicolumn{1}{c}{FP} & 
	\multicolumn{1}{c}{FN} & 
	\multicolumn{1}{c}{TP} & 
	\multicolumn{1}{c}{Recall} & 
	\multicolumn{1}{c}{$\text{FP} / \text{P}$} 
\cr
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}

Bal RF & 0.5 &  & 0.86 & 600,464 & 5,146 & 95,359 & 12,597 & 0.1167 & 0.048\cr
%Bal RF & 0.85 &  & 0.86 & 600,587 & 5,023 & 95,845 & 12,111 & 0.047\cr
Keras & 0.5 & 0.0 & 0.598 & 600,212 & 5,398 & 97,029 & 10,927 & 0.1012 & 0.05\cr
%Keras & 0.5 & 2.0 & 0.547 & 600,212 & 5,398 & 97,110 & 10,846 & 0.05\cr
%Keras & 0.5 & 1.0 & 0.56 & 600,212 & 5,398 & 97,282 & 10,674 & 0.05\cr
%Keras & 0.85 & 0.0 & 0.899 & 600,212 & 5,398 & 97,408 & 10,548 & 0.05\cr
Log Reg & 0.5 &  & 0.549 & 600,212 & 5,398 & 100,764 & 7,192 & 0.0666 & 0.05\cr
RUSBoost &  &  & 0.5 & 600,212 & 5,398 & 101,041 & 6,915 & 0.0641 & 0.05\cr
%Log Reg &  &  & 0.883 & 600,212 & 5,398 & 101,124 & 6,832 & 0.05\cr
AdaBoost &  &  & 0.501 & 600,212 & 5,398 & 101,131 & 6,825 & 0.0632 & 0.05\cr
Bal Bag &  &  & 0.9 & 602,185 & 3,425 & 101,272 & 6,684 & 0.0619 & 0.032\cr
RF &  &  & 0.258 & 600,212 & 5,398 & 102,135 & 5,821 & 0.0539 & 0.05\cr
Easy Ens &  &  & 0.549 & 600,155 & 5,455 & 102,322 & 5,634 & 0.0522 & 0.051\cr
\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

%%%
\subsubsection{Detecting Model Failure}
\label{Methods_Model_Failure}

We have found two ways a model can fail to give a decision threshold $\theta$ that satisfies the budgetary criteria.  The second one we consider is the usual problem, that the model does not sufficiently separate the positive and negative classes to give enough correct recommendations.  The other cause is more subtle, and we consider it first because we just saw it in Table \ref{FP_P_0_05_hard}, that for the Balanced Bagging algorithm, the value of FP/P closest to 0.05 (FP/P = 0.032) is not close to 0.05, and the cause of the failure is the numerics in the algorithm.

The RUSBoost algorithm on the hard features only gives values of $p$ in a tiny range, $p \in [0.499, 0.5011]$, but in that range of 00021 the 713,566 samples return 706,938 different values of $p$.  That's about as close to ``continuous'' as a discrete function can get.  Digging down into the results returned by the Balanced Bagging algorithm on the hard features, 99\% of the values of $p$ are rounded to the nearest tenth, making the results extremely discrete.  In Table \ref{BalBag_Hard_0_Slices}, going from $p=0.897$ to $p=0.9$ we add 19,052 new samples that change the FP/P value from 0.11 to 0.03.   The model results give no way to find a budgetary decision threshold $\theta$ that gives us a 5\% increase in the number of ambulances dispatched to automated notifications from cell phones.  This model may have too much volatility to be useful.  

The Balanced Random Forest algorithm on the hard features has a similar quirk, that 93\% of its $p$ values are rounded to the nearest hundredth, which explains the $\text{FP}/\text{P} = 0.048$ as the closest to 0.05 in Table \ref{FP_P_0_05_hard}.  These two algorithms, Bal Bag and Bal RF, interestingly, had more detail in the Medium Features results and even more detail for the Easy features, which is the opposite of what most of the other algorithms did.  The AdaBoost, Keras, Log Reg, and RUSBoost all had, from the 713,566 samples, about 700,000 unique values of $p$ for the hard features, 650,000 for the medium features, and 150,000 for the easy features.  A breakdown of those numbers is given in 

\noindent\verb|Value_Counts_y_proba.csv| in the \verb|Keras/Analyze_Proba| folder, and the value counts of $p$ are given for each algorithm in files such as \verb|BalBag_Hard_Value_Counts.csv|.


\begin{table}[h]
\caption{\normalfont\normalsize Model Failure:  Balanced Bagging Algorithm on the Hard Features with FP/P closest to 0.05.  Table accompanies \S\ref{Methods_Model_Failure}}
\label{BalBag_Hard_0_Slices}

{\normalfont\normalsize
\begin{tabular}{lrrrrrrl}
\toprule
	\multicolumn{1}{c}{$p$} & 
	\multicolumn{1}{c}{Neg} & 
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{TN} & 
	\multicolumn{1}{c}{FP} & 
	\multicolumn{1}{c}{FN} & 
	\multicolumn{1}{c}{TP} & 
	\multicolumn{1}{c}{$\text{FP} / \text{P}$} 
\cr
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}

0.896 & 1 & 0 & 593,725 & 11,885 & 90,679 & 17,277 & 0.110091 \cr
0.897 & 1 & 0 & 593,726 & 11,884 & 90,679 & 17,277 & 0.110082 \cr
0.9 & 8,459 & 10,593 & 602,185 & 3,425 & 101,272 & 6,684 & 0.031726 \cr
0.908 & 1 & 1 & 602,186 & 3,424 & 101,273 & 6,683 & 0.031717 \cr
0.913 & 1 & 0 & 602,187 & 3,423 & 101,273 & 6,683 & 0.031707 \cr

\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

The second kind of model failure is the usual problem with machine learning models, that the algorithm is not successful in detecting enough of a pattern in the data to build a robust model that separates the positive and negative classes well.  There are two likely reasons for the failure.  One is that the particular algorithm with its hyperparameters is not suited to the kind of patterns in the data, and it is for this reason that we have tried several models with different class and focal weights.  A second possible reason is that there just isn't enough of a pattern in the dataset, and it is for this reason that we have tested the Easy, Medium, and Hard feature sets.  

As an example of this kind of model failure, consider the Random Forest Classifier on the Medium features, shown in Table \ref{RFC_Medium_400_Slices}.  Here the $p$ values are cut into bands with at least one hundred elements of each of the Neg and Pos classes; the $p$ values in the first column are the average of the minimum and maximum $p$ value in the band.  

The Precision values generally increase with $p$, and the table shows the end of the $p$ range; there are no values of Precision close to $2/3$, not because it doesn't theoretically exist, but the model is not robust enough to sufficiently separate the positive and negative classes.  If the governmental leaders decided to change to $\text{Precision} = 1/2$, we could choose $\theta = 0.240$, but of the crash persons with an automated notification from a cell phone who needed an ambulance, we would only be immediately dispatching ambulances to $\text{Recall} = 1\%$ of them.  Scaling that to a medium-sized city, the administrative overhead of the program might outweigh any benefit.  Given a finite budget, a responsible government needs to choose the most cost-effective programs, and an immediate dispatch recommendation system might not be one of them.  

We can see the problem also in a histogram of the model output, in Figure \ref{RFC_Medium_Zoom_Figure}, where we compare the right tails of the output of the Random Forest model on the Medium features with the Example model from Figure \ref{intro_ideal}.  The model output has a long tail to the right, and zooming in on that tail, we see that in none of those intervals are there more elements of the positive class than the negative class.  

Note that this range of $p$ in Table \ref{RFC_Medium_400_Slices} is also where $\text{Pos}/(\text{Neg} + \text{Pos})$ crosses 0.50, meaning that there is a 50\% marginal probability that the last ambulance sent is needed, so we would have the same problem for that metric, that the model would recommend immediately dispatching ambulances to only about 1\% of the automated notifications that needed an ambulance.  

Fortunately we do have models with better results on the Medium features set.  



\begin{table}
\caption{
	\normalsize\normalfont
	Metrics on Partial $p$ Output of Random Forest Classifier on the Medium Features with Minimum of 400 Elements of Each Class in Each Band.  Table accompanies \S\ref{Methods_Model_Failure}
}
\label{RFC_Medium_400_Slices}

{\normalsize
\normalfont
\begin{tabular}{lrrlrrrrlll}
\toprule
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{$m$Prob} &     
	\multicolumn{1}{c}{TN} &      
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{FN} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{Prec} &    
	\multicolumn{1}{c}{Rec} &    
	\multicolumn{1}{c}{FP/P} \\
\midrule
0.2386 & 415 & 401 & 0.491 & 603,883 & 1,727 & 106,283 & 1,673 & 0.492 & 0.016 & 0.016\cr
0.2405 & 460 & 400 & 0.465 & 604,343 & 1,267 & 106,683 & 1,273 & 0.501 & 0.012 & 0.0117\cr
0.2429 & 466 & 400 & 0.462 & 604,809 & 801 & 107,083 & 873 & 0.522 & 0.008 & 0.0074\cr
0.2465 & 400 & 421 & 0.513 & 605,209 & 401 & 107,504 & 452 & 0.53 & 0.004 & 0.0037\cr
0.2599 & 401 & 452 & 0.53 & 605,610 & 0 & 107,956 & 0 & nan & 0 & 0\cr 
\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier

%%%
\begin{figure}[h]



\noindent\begin{tabular}{@{\hspace{0pt}}p{2.3in} @{\hspace{-6pt}}p{4.6in} }
	\vskip 0pt
	
	\hfil {\normalfont\normalsize Full range of $p$}
	
	
	\input{../Keras/Images/RFC_Medium_Run_1_Pred_Zoom.pgf}	
&
	\vskip 0pt
	
	\hfil {\normalfont\normalsize Right tail of $p$ for the Random Forest Model}
		
	\input{../Keras/Images/RFC_Medium_Run_1_Pred_Zoom_Wide_Right.pgf}	
\cr
	\vskip 0pt
	
	\hfil {\normalfont\normalsize Full range of $p$}
	
	
	\input{../Keras/Images/Ideal_Pred.pgf}	
&
	\vskip 0pt
	
	\hfil {\normalfont\normalsize Right tail of $p$ for the Example Model}
		
	\hskip 12pt\input{../Keras/Images/Ideal_Pred_Zoom_Wide_Right.pgf}	
\cr
\end{tabular}

	  \caption{\normalfont\normalsize Random Forest Classifier on the Medium Features (top) and Example model (bottom).  Figure accompanies \S\ref{Methods_Model_Failure}}\label{RFC_Medium_Zoom_Figure}
\end{figure}

\FloatBarrier



%%%%%
\subsubsection{Numerical Precision}
\label{Numerical_Precision}

We ran all of the models twice with different random seeds.  As an example of the difference in model results, consider the first budgetary decision metric, the values of $p$ that make FP/P closest to 0.05, because that metric is the most stable.  If there is a difference in those results between two runs of the same model for FP/P, not only can we only claim that much numerical precision in reporting our results for that metric, but we may need to limit ourselves to even less numerical precision in reporting results for the other budgetary decision metrics.  

Table \ref{FP_P_0_05_Numerical_Precision} shows the models with the largest changes between runs.  The median $\Delta p$ was 0.0003, the median $\Delta$ TP was 57, and the median $\Delta$ FP was 1, giving median $\Delta$ FP/P of $0.0000093$.


\begin{table}
\caption{
	\normalsize\normalfont
	Comparison of values of $p$, TP, FP, and FP/P between Runs of the Same Model with Different Random Seeds.
  Table accompanies \S\ref{Methods_Model_Failure}
}
\label{FP_P_0_05_Numerical_Precision}

{\normalsize
\normalfont
\begin{tabular}{lcclrrrrlll}
\toprule
	\multicolumn{1}{c}{Model} &     
	\multicolumn{1}{c}{Features} &    
	\multicolumn{1}{c}{Run} &     	 
	\multicolumn{1}{c}{$p$} &     
	\multicolumn{1}{c}{Neg} &   
	\multicolumn{1}{c}{Pos} & 
	\multicolumn{1}{c}{FP} &      
	\multicolumn{1}{c}{TP} &   
	\multicolumn{1}{c}{FP/P} &
	\multicolumn{1}{c}{Notes} \\
\midrule

Keras $\alpha = 0.5$, $\gamma = 1.0$  & Medium & 0 & 0.50556 & 1 & 0 & 5,398 & 6,294 & 0.05 & \multirow{2}{*}{ $\Delta p = 0.012$}\cr
Keras $\alpha = 0.5$, $\gamma = 1.0$  & Medium &  1 & 0.51764 & 1 & 0 & 5,398 & 6,105 & 0.05\cr\hline
Keras $\alpha = 0.5$, $\gamma = 1.0 $ & Hard & 0  & 0.56429 & 1 & 0 & 5,398 & 10,946 & 0.05 & \multirow{2}{*}{$\Delta \text{TP} = 409$}\cr
Keras $\alpha = 0.5$, $\gamma = 1.0$  & Hard & 1 & 0.55914 & 1 & 0 & 5,398 & 10,537 & 0.05\cr\hline
Easy Ens & Easy & 0 & 0.52846 & 263 & 107 & 5,286 & 2,682 & 0.049& $\Delta \text{FP} = 180$ \cr
Easy Ens & Easy & 1 & 0.52866 & 27 & 15 & 5,466 & 2,741 & 0.0506 & $\Delta \text{FP/P} = 0.0016$\cr

\bottomrule
\end{tabular}
}
\end{table}

\FloatBarrier












