%%%%% Methods

%%%
\subsection{Overview}

\begin{itemize}
	\item The dataset
	\item Binning the features
	\item Imputing missing data
	\item Order of Operations in Binning and Imputing
	\item Handling imbalanced data
	\item Choosing features for ``Easy,'' ``Medium,'' and ``Hard'' data sets
	\item Building models
	\item Choosing metrics to interpret the results
	\item Interpreting the results
\end{itemize}

%%%%%
\subsection{Metrics Old and New}

In our work, we are trying to build a binary classification model to predict, based on immediately available information, whether the crash person needs an ambulance.  To build a supervised-learning model, we start with labeled data, a dataset of crashes for which we know whether the person needed an ambulance.  We separate the data into two sets, a training set and test set.  A machine learning algorithm uses the training set to build (``learn'') a model that will return, for each crash person in a set, a value $p \in [0,1]$ that is a proxy for the probability that the crash person needs an ambulance.  We choose a decision threshold $\theta$ (by default $\theta=0.5$) and classify samples (crash persons) with $p>\theta$ as  ``Needs ambulance'' and samples with $p < \theta$ as ``Does not need ambulance.''  Then we apply the model to the test set, which the algorithm did not see in the learning phase.  The test results tell us how many of the crash person who needed an ambulance or did not need an ambulance were correctly classified, and we organize them in a confusion matrix.  

\

\hfil\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{Predicted Negative (PN)} & \multicolumn{1}{c}{Predicted Positive (PP)} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&Negative (N) &
True Negative (TN) & False Positive (FP)
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&Positive (P) & False Negative (FN) & True Positive (TP) 
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
\end{tabular}

\vskip 12pt

Obviously we would prefer a model that returned no false positives or false negatives, but barring that, we would like to come as close as possible, and we need to define ``close'' in a measurable way.  

\

\hfil\begin{tabular}{r >{$\displaystyle}c<{$\vrule width 0pt height 16pt depth 12pt} p{3in}}
	Metric & \text{Formula} & Meaning \cr\hline
	\bf Accuracy & \frac{ \text{TN} + \text{TP} }{ \text{TN} + \text{FP} + \text{FN} + \text{TP}} & Proportion correctly classified \cr
	\bf Precision & \frac{ \text{TP} }{ \text{TP} + \text{FP}} = \frac{ \text{TP} }{ \text{PP} } & Proportion of immediately-dispatched ambulances that are needed \cr
	\bf Recall & \frac{ \text{TP} }{ \text{TP} + \text{FN}} = \frac{ \text{TP} }{ \text{P} } & Proportion of needed ambulances that are immediately dispatched \cr
	\bf F1 & \frac{ 2 }{ \displaystyle \frac{1}{\text{Precision}} + \frac{1}{\text{Recall}} }  & Harmonic mean of Precision and Recall \\[3em]
	& = \frac{ 2 \cdot \text{Precision} \cdot \text{Recall} }{ \text{Precision} + \text{Recall} } \cr
\end{tabular}

\vskip 12pt

Accuracy and precision depend on the class balance, but recall does not.  Most reported crashes are fender benders that do not require an ambulance, so in our dataset, $\text{P} \ll \text{N}$.  If we artificially balance the dataset so that $\text{P} = \text{N}$, either by resampling the data or using class weights in the loss function, then we would change the proportions in accuracy and precision.  In the literature there is a ``balanced accuracy'' that is widely used.  One could make a ``balanced precision'' in the same way, which would lead to a ``balanced F1,'' but we have not seen those used in the literature.  Recall is not affected by class balance because all of its terms (TP and FN) are in the positive class.  

In combining precision and recall to make F1, why do we choose the harmonic mean instead of the arithmetic or geometric mean?  There is a Gmean in the literature, but it is not used as often.    If $a$ and $b$ are such that $0 < a < b$, then $$a < \text{Harm}(a,b) < \text{Geo}(a,b) < \text{Arith}(a,b) < b$$    The harmonic mean leans towards the worse of the two metrics (precision and recall), much like least squares regression emphasizes the points furthest from the line, telling us not only their central tendency but also whether the two metrics are in balance.
  
\

The predicted $p$ values, when paired with the actual $N=0$ and $P=1$ values, combine to make the False Positive Rate and True Positive Rate values that become the parameterized curve called (for historical reasons) the Receiver Operating Characteristic (ROC) curve.  The area under this curve (AUC) indicates how well the model has separated the positive and negative classes.  

The graphs below are for an artificial dataset to illustrate the metrics.  The histogram shows the number of elements of the positive and negative classes in ten ranges of $p$.  The dataset is imbalanced like crash data, with far more crash persons not needing an ambulance than needing one.  

The ROC curve is a parameterization over $p$ of the true positive rate versus the false positive rate, with $p=1$ in the lower left and $p=0$ in the upper right.  If the model perfectly separated the positive and negative classes, the parameterized curve would form a Gamma ($\Gamma$) from $(0,0)$ to $(0,1)$ to $(1,1)$ given $\text{AUC} = 1$.  Random noise would follow the dashed diagonal and give $\text{AUC} = 0.5$, as illustrated further below.  

\

{\bf Ideal Model}  
In this model, most of the negative class samples are on the left (TN, $p<0.5$) and most of the positive class samples are on the right (TP, $p>0.5$), which is what we want.  Under this model, we would immediately dispatch ambulances to 77.7\% of the crash persons who needed one (recall), and 38.7\% of the ambulances we immediately dispatched would be needed (precision).  The harmonic mean of those two numbers is $\text{F1} = 0.517$.  The area under the curve, which increases with how well the model has separated the two classes, is 0.840.

\

\verb|Ideal|

%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Ideal_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Ideal_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
66.5\% & 18.5\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
3.3\% & 11.7\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
	0.781 & Accuracy\cr
	0.387 & Precision \cr
	0.777 & Recall \cr
	0.517 & F1 \cr
	0.840 & AUC \cr
\end{tabular}

\cr
\end{tabular}
} % End parbox


\

{\bf Awful Model}  A model with these results has not successfully separated the positive and negative classes, possibly because the data has no pattern.  

\

\verb|Awful|

%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Awful_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Awful_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
%0.551741176	
% 41.8	43.2	6.5	8.5	
% 0.503	0.164410058	0.566666667	0.247818438
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
41.8\% & 43.2\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
6.5\% & 8.5\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
	0.503 & Accuracy\cr
	0.164 & Precision \cr
	0.567 & Recall \cr
	0.248 & F1 \cr
	0.552 & AUC \cr
\end{tabular}
\cr
\end{tabular}
} % End parbox

\

{\bf Ideal Model Tending Left} Under this model, using decision threshold $\theta = 0.5$, we would immediately dispatch no ambulances.  The model has separated the positive and negative classes just as well ($\text{AUC} = 0.840$), but the entire distribution is pushed to the left.  We did encounter this situation in our work.  

\

\verb|Ideal_Left|

%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Ideal_Left_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Ideal_Left_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
85\% & 0\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
15\% & 0\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
	0.85 & Accuracy\cr
	nan & Precision \cr
	0 & Recall \cr
	nan & F1 \cr
	0.840 & AUC \cr
\end{tabular}
\cr
\end{tabular}
} % End parbox

\

{\bf Ideal Model Tending Right} Under this related model, if we were using decision threshold $\theta = 0.5$, we would immediately dispatch an ambulance to every reported crash, which would save lives but be expensive and not possible with existing resources in the short term.  

\

\verb|Ideal_Right|

%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Ideal_Right_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Ideal_Right_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
0\% & 85\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
0\% & 15\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}
% 0.839808121
%	0	84.993	0	15.007	
%	0.150068774	0.150068774	1	0.150068774

	\hfil\begin{tabular}{ll}
	\cr
	0.15 & Accuracy\cr
	0.15 & Precision \cr
	1 & Recall \cr
	0.15 & F1 \cr
	0.840 & AUC \cr
\end{tabular}
\cr
\end{tabular}
} % End parbox

\

If a model gives results like the ``Ideal Model Tending Left,'' and we directly apply its recommendations to never immediately dispatch an ambulance, then we wasted our time building a recommendation system.  The model output can still be useful, however, if we approach the data in one of three ways.  

\

{\bf Shift the $p$ Values to the Right}

Here we have shifted all of the $p$ values so that the average of the median $p$ value for the negative class and the median $p$ value for the positive class is at $0.5$.

\

\verb|Ideal_Left_Shifted|

%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Ideal_Left_Shifted_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Ideal_Left_Shifted_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
66.3\% & 18.7\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
3.30\% & 11.7\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}
% 0.839808121	
%66.303	18.69	3.299	11.707	
% 0.780102823 	0.385141776	0.780135983	0.515686041

	\hfil\begin{tabular}{ll}
	\cr
	0.780 & Accuracy\cr
	0.385 & Precision \cr
	0.780 & Recall \cr
	0.516 & F1 \cr
	0.840 & AUC \cr
\end{tabular}
\cr
\end{tabular}
} % End parbox


\

{\bf Linearly Transform the $p$ Values}, mapping the min to 0.0 and the max to 1.0.  If there are far outliers, perhaps map the 1\% quantile to 0 and the 99\% quantile to 1.0.  This transformation is especially useful for visualizing the data.  

\

\verb|Ideal_Left_Linear_Transform|


%%%
\parbox{\linewidth}{
\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/Ideal_Left_Linear_Transform_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/Ideal_Left_Linear_Transform_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
66.6\% & 18.4\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
3.37\% & 11.6\%
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}
% 0.839832287	
%66.636	18.357	3.373	11.634	
%0.78269595	0.387906242	0.775215056	0.518728987	

\hfil\begin{tabular}{ll}
	\cr
	0.783 & Accuracy\cr
	0.388 & Precision \cr
	0.775 & Recall \cr
	0.518 & F1 \cr
	0.840 & AUC \cr
\end{tabular}
\cr
\end{tabular}
} % End parbox

\

Note that the ROC curve does not change under these transformations.  

\

{\bf Choose a Different Decision Threshold}

\

The table below shows the number of elements of the negative and positive classes in bands of values of $p$.  

\

\

\verb|Ideal_Left_20|

\

\begin{tabular}{rrrrrrrrrrrrrrr}
\toprule
p &     Neg &    Pos & mPrec &      TN &      FP &      FN &      TP &  Prec &   Rec &  FP/P & $\hat{p}$ \\
\midrule
0.00 &       0 &      0 &   nan &       0 &  85,000 &       0 &  15,000 &  0.15 &  1.00 &  5.67 &      1.00 \\
0.05 &   2,481 &    137 &  0.05 &   2,481 &  82,519 &     137 &  14,863 &  0.15 &  0.99 &  5.50 &      0.97 \\
0.10 &  13,268 &    260 &  0.02 &  15,749 &  69,251 &     397 &  14,603 &  0.17 &  0.97 &  4.62 &      0.84 \\
0.15 &  19,408 &    469 &  0.02 &  35,157 &  49,843 &     866 &  14,134 &  0.22 &  0.94 &  3.32 &      0.64 \\
0.20 &  18,054 &    941 &  0.05 &  53,211 &  31,789 &   1,807 &  13,193 &  0.29 &  0.88 &  2.12 &      0.45 \\
0.25 &  13,262 &  1,599 &  0.11 &  66,473 &  18,527 &   3,406 &  11,594 &  0.38 &  0.77 &  1.24 &      0.30 \\
0.30 &   8,302 &  2,338 &  0.22 &  74,775 &  10,225 &   5,744 &   9,256 &  0.48 &  0.62 &  0.68 &      0.19 \\
0.35 &   5,514 &  3,245 &  0.37 &  80,289 &   4,711 &   8,989 &   6,011 &  0.56 &  0.40 &  0.31 &      0.11 \\
0.40 &   2,660 &  3,354 &  0.56 &  82,949 &   2,051 &  12,343 &   2,657 &  0.56 &  0.18 &  0.14 &      0.05 \\
0.45 &   1,352 &  2,152 &  0.61 &  84,301 &     699 &  14,495 &     505 &  0.42 &  0.03 &  0.05 &      0.01 \\
0.50 &     699 &    505 &  0.42 &  85,000 &       0 &  15,000 &       0 &   nan &  0.00 &  0.00 &      0.00 \\
0.55 &       0 &      0 &   nan &  85,000 &       0 &  15,000 &       0 &   nan &  0.00 &  0.00 &      0.00 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots  \\
1.00 &       0 &      0 &   nan &  85,000 &       0 &  15,000 &       0 &   nan &  0.00 &  0.00 &      0.00 \\
\bottomrule
\end{tabular}

\vskip 12pt

Choosing decision thresholds $\theta=0.20$ and $\theta = 0.25$ give us these confusion matrices.  

\

\hfil	\begin{tabular}{cc|c|c|}
	$\theta = 0.20$ &\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
53,211 & 31,789
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
1,807 & 13,193
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}
	%
\hfil	\begin{tabular}{cc|c|c|}
	$\theta = 0.25$ &\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \\[0.4em]
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
66,473 & 18,527
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
3,406 & 11,594
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

\vskip 12pt

To choose $\theta = 0.25$ instead of $\theta = 0.20$ is to send 1,599 fewer ambulances to crash persons who need them (more false negatives), but also 13,262 fewer ambulances to crash persons who did not need them (fewer false positives).  Where to make that tradeoff of lives for money is a political question, not a technical one, but given the political decision we can choose $\theta$ to calibrate the recommendation system.  

For instance, in the short term the number of available ambulances and ambulance crews is fixed.  The FP/P column gives the amount of increase in the number of ambulances going to crash persons because we are immediately dispatching some ambulances that are not needed.  If the existing infrastructure could handle only a 5\% increase in the number of ambulances sent to crash persons, then we would choose $\theta = 0.45$, immediately dispatching 1,204 ambulances, 505 of which were needed and would have been sent anyway, but 699 of which are not needed, representing an increased cost.  If we could handle a 10\% increase, we would zoom in to the data to get a value of $\theta $ between $0.40$ and $0.45$ where $\text{FP/P} = 0.10$



{\bf This section repeats a lot of stuff from the Metrics section; put in Methods}

The binary classification process actually has more nuance that can lead to more interesting metrics.  The model does not tell us directly whether it predicts that the sample belongs to the positive or negative class; instead, the model assigns to each sample a value $p \in [0,1]$.  In code using scikit-learn, the value $p$ is often called \verb|y_proba| and is returned by the \verb|model.predict_proba()| function.  This value $p$ is a proxy for the probability that the crash person needs an ambulance.  It is not exactly the probability, but increases with the probability.  If we have enough data to smooth out the effects of randomness, and if there actually is an underlying pattern to the data that will predict the target variable, then we can can get rough probabilities from $p$ and measure marginal effects of changing the decision threshold $\theta$.

To map the probability that a crash person needs an ambulance as a function of $p$, we looked at ranges of $p$ of width $0.01$, and in each band divided the number of positive samples in the band by the total number of samples in the band.  We would like to call this metric ``marginal precision,'' but that term is used in statistics for something else.  It is the marginal probability, the probability that the a crash report with $p = \theta$ needs an ambulance, where Precision is the total probability, the proportion of crashes with $p > \theta$ that need an ambulance.  

To get enough samples in each band to smooth out the randomness, we needed more than the usual 70/30 train/test split, so we went to 5-fold cross validation.  In this approach, we divide the dataset into five sets, each with the same P/N ratio.  We then train the model five times with a 80/20 train/test split, which gives us a $p$ value for each sample in the whole set.  

There are two reasons why we chose a width of $0.01$ for our $p$ bands.  First, with a much smaller width we would have far fewer samples in each band, showing more of the effects of randomness.  Second, some of our model algorithms, like the Balanced Random Forest Classifier from Imbalanced-Learn, gave most (93\%) of the $p$ values rounded to two decimal places.  Intervals like $p \in [0.501,0.509)$ would have few samples, and thus not be useful.  The Balanced Bagging model algorithm gave 99\% of the values of $p$ rounded to one decimal place, so for that model we would need to choose a band width of $0.10.$

\


%%%
\subsection{The Dataset}

Ideally, we would use a dataset of crashes with an automated notification, but we have not found such a dataset that is publicly available.  Working with such a private dataset would be an important avenue of future research.

We will use the Crash Report Sampling System (CRSS) from 2016 to 2021.  The CRSS is a curated sample of crashes in the US, weighted to more serious crashes such that 17\% of the crash persons needed an ambulance, significantly more than the proportion of all reported crashes needing an ambulance, between 2 and 3 percent.  Since most low-speed crashes would have a crash profile similar to hard braking, they would not spawn an automated notification, so it is reasonable to assume that the set of crashes with automated notifications would have a higher percentage of persons needing an ambulance.  

We will use the CRSS as a proxy for the set of crashes with automatic crash notifications, acknowledging that we do not know how good of a proxy it is.  The primary merit of CRSS for our work is that it is publicly available so that our work can be critiqued, adapted, and expanded by others.  

We merged the \verb|accident.csv|, \verb|vehicle.csv|, and \verb|person.csv| files in the six years.  We dropped many features that were irrelevant, most because they were unique to each vehicle like a VIN (Vehicle Identification Number), with no predictive power, just random noise.  We also dropped the 33,776 persons in crashes involving a pedestrian, because the deceleration profile of hitting a pedestrian or bicycle would not be different enough from hard braking to trigger an automated crash notification.  

After removing repeated and irrelevant features and pedestrian crashes, we have 118 features describing 713,566 crash persons.  Later we removed more features that have more than 20\% of the values missing or only have data for some years, and features with imputed missing values, to get 78 features.

For full details, see the \verb|Ambulance_Dispatch_01_Get_Data.ipynb| file.  

%%%
\subsection{Binning Categories}

All of the CRSS data is discrete, but some features are ordered, like \verb|HOUR| and \verb|AGE|, and others are unordered, like \verb|MAKE_MOD|.  Reducing the dimensionality of the machine learning modeling by binning the categories into less than ten per feature is ideal.  

Some features like \verb|HOUR| we binned by hand.  We looked at the proportion of crash persons hospitalized at each hour and found clear places to break it into seven contiguous but not equal blocks.  When we looked at \verb|AGE|, we considered breaking it into decades, but found that ages 15-18 have a far lower hospitalization rate than those a little younger or older, and there was a shift at about age 53 and again around age 74, so we binned it accordingly.  

Some features like \verb|MAKE_MOD|m which has 1,210 unique values, we binned by imposing an order, ordering by the proportion of crash persons hospitalized, then cutting the ordered list into five new categories plus ``Unknown.''  

For full details, see 
\verb|Ambulance_Dispatch_02_Correlation.ipynb| and 

\verb|Ambulance_Dispatch_03_Bin_Data.ipynb|.

%%%
\subsection{Imputing Missing Data}

For reasons of historical consistency going back to 1982 with the predecessors of CRSS, CRSS imputes missing values for some features but not others, using IVEware, Imputation and Variation Estimation Software from the Institute for Social Research at the University of Michigan.  Fortunately, when CRSS gives a feature with imputed values, it also retains the original feature with values signifying ``Unknown.'' CRSS has a very helpful report on its imputation methods.  We have not seen in the literature where someone has used IVEware to impute the other features and compared it to other methods.  

At this point we have 78 unimputed features, and only 250,389 out of 713,566 samples (35\%) did not have missing values in those 78 features.  We compared three methods for imputing missing values.  

\begin{itemize}
	\item IVEware
	\item Imputation to Mode
	\item Round-Robin Random Forest using Imputation to Mode as the starting point
\end{itemize}

We found that the Random Forest method was best for our purposes.

For full details and analysis, see 
\verb|Ambulance_Dispatch_04_Impute_Missing_Data.ipynb|.

%%%
\subsection{Order of Operations}

We also considered whether the order of operations made a difference:  Should we bin first, then impute, or impute first, then bin?.  We tried both methods on the {\tt Ambulance} dataset with the IVEware imputation approach.  After several runs we found that the difference between methods was about the same as the difference between runs of the same method with different random seeds.  Since IVEware can only handle up to about forty categories in each categorical field, we had had to bin some fields first either way, so we chose to bin first, then impute.  

For full details and analysis, see 
\verb|Ambulance_Dispatch_05_IVEware_Order_of_Operations.ipynb|.

%%%
\subsection{Handling Imbalanced Data}

In our dataset only about fifteen percent of the people needed an ambulance.   If a recommendation system never sent an ambulance, the model would have 85\% accuracy, but be useless.  Most algorithms for training models are designed for balanced data, with half of the samples in each of the negative and positive classes.  With an imbalanced data set we can address the imbalance in five ways:  Resampling the dataset, modifying the loss function, choosing metrics other than accuracy, using learning methods that account for the imbalance, and manually moving the decision threshold.

\subsubsection{Resampling the Dataset}

We can balance the dataset by undersampling the majority class (negative, ``No ambulance'') or oversampling the minority class (positive, ``Send Ambulance'').  To balance by undersampling would mean throwing out eighty percent of the majority class, losing valuable information.  A very popular method for oversampling is SMOTE (Synthetic Minority Oversampling TEchnique), which creates new minority samples between existing minority samples, but the ``between'' requires continuous data, and all of our data is discrete or categorical (What is between a Buick and a Volvo?).

Tomek Links is one of the few resampling methods that works for categorical data.  It is a selective undersampling method that removes majority samples that seem out of place.  We did not see a significant improvement in the model metrics from the undersampling; the difference between no undersampling, one run of Tomek, and two runs turned out to be inconsequential, by which we mean that one approach was not consistently better (measured by the area under the ROC curve) when we ran the models with different random seeds.  

We considered a related method, Condensed Nearest Neighbor, but it is impractical for data sets of this size.  


\subsubsection{Modifying the Loss Function}

A popular and well established way to modify the loss function for imbalanced data is with class weights, which can have the same effect as na{\"i}ve oversampling.  

Three of our seven models take class weights, and for those we tried three different class weights.  

\

\hfil\begin{tabular}{c|l}
	$\alpha$ & Meaning \cr\hline
	1/2 & No class weight \cr
	2/3 & $\Delta FP/\Delta TP < 2.0$ goal \cr
	$0.85$ & Balanced classes \cr 
\end{tabular}

\


A related method is with focal loss, which has a modulating hyperparameter $\gamma$ that increases the penalty for low-confidence samples. \citep{lin2017focal}  We tried five values  of $\gamma$.

\

\hfil\begin{tabular}{c|c}
	$\gamma$  & Notes \cr\hline
	0.0 & Same as binary crossentropy \cr
	0.5 & Very light modulation \cr
	1.0 & Light modulation\cr
	2.0 & Recommended by Lin \cr
	5.0 & Heavy modulation \cr
\end{tabular}	

\

We did not see significant improvement using focal loss, measured by the area under the ROC curve.  ({\bf Put in Label Reference}).

%%%
\subsubsection{Metrics for Imbalance}


In the \nameref{Methods_Metrics} subsection above we defined the metrics recall, precision, and f1.  The most common metric in machine learning, the one that most algorithms are designed to maximize, is accuracy, the proportion of samples correctly classified.  In that section's example of transformed model output, we had 150,107 out of 177,392 test samples correctly classified, giving 84.6\% accuracy.  Is that good?  The model below, the raw results of the Logistic Regression model of the easy features set, recommends sending no ambulances, and it is correct in 150,771 of 177,392 test samples, giving 84.99\% accuracy.  Is that better?



\

%%%
\parbox{\linewidth}{
%{\bf Balanced Random Forest model, Hard features, No Tomek, $\alpha = 2/3$}

\noindent\begin{tabular}{@{\hspace{-6pt}}p{2.3in} @{\hspace{-6pt}}p{2.0in} p{1.8in}}
	\vskip 0pt
	\qquad \qquad Raw Model Output
	
	\input{../Keras/Images/LRC_Easy_Tomek_0_alpha_0_5_v1_Test_Pred.pgf}
&
	\vskip 0pt
	\qquad \qquad ROC Curve
	
	\input{../Keras/Images/LRC_Easy_Tomek_0_alpha_0_5_v1_Test_ROC.pgf}
	
&
	\vskip 0pt
	\begin{tabular}{cc|c|c|}
	&\multicolumn{1}{c}{}& \multicolumn{2}{c}{Prediction} \cr
	&\multicolumn{1}{c}{} & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{P} \cr\cline{3-4}
	\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}&N &
150,771 & 0
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	&P & 
26621 & 0
	\vrule width 0pt height 10pt depth 2pt \cr\cline{3-4}
	\end{tabular}

	\hfil\begin{tabular}{ll}
	\cr
	0.8499 & Accuracy\cr
und & Precision \cr	0.0 & Recall \cr	und & F1 \cr	0.659 & AUC \cr
\end{tabular}

\cr
\end{tabular}
} % End parbox

\

In this study, we  have arbitrarily decided that we are willing to trade off up to two false positives to get one more true positive.  Once we moved our decision thresholds to the ethical tradeoff point, the accuracy only varied from 0.836 to 0.854.  The difference in accuracy tells us how many more (or fewer) false positives than true positives we have, with them being equal at 0.8499, and we get the same information from precision being less than, more than, or equal to 0.5.    Therefore, we are not going to consider accuracy in evaluating our models. 

%%%
\subsubsection{ML Algorithms for Imbalanced Data}

{\bf [Expand this subsubsection]}

\begin{itemize}
	\item Random Undersampling Composite Models
	\item Bagging
	\item Boosting
\end{itemize}

%%%%%
\subsection{Models}

We used seven binary classification algorithms.  Three of them take class weights.

\

\hfil\begin{tabular}{llc}
&& Class \cr
Model & Source & Weights \cr\hline
KerasClassifier with the Binary Focal Crossentropy loss function & Keras & Yes \cr
Balanced Random Forest Classifier & Imbalanced-Learn & Yes \cr
Balanced Bagging Classifier & Imbalanced-Learn & No \cr
RUSBoost Classifier & Imbalanced-Learn & No \cr
Easy Ensemble Classifier with AdaBoost Estimator & Imbalanced-Learn & No \cr
Logistic Regression Classifier & Scikit-Learn & Yes \cr
AdaBoost  Classifier & Scikit-Learn & No \cr
\end{tabular}

\


For the focal loss function, we tried seven different combinations of the hyperparameters $\alpha$ for class weights and $\gamma$ for penalty on badly misclassified samples.  For the random forest and bagging models we tried three values of $\alpha$.  Altogether we had seventeen model/hyperparameter combinations.  We learned each of the seven models on datasets with the easy, medium, and hard features, and on the hard features we tested with Tomek undersampling 0, 1, and 2 times, for a total of five datasets, giving eighty-five model/hyperparameter/dataset combinations.    We learned each of those sixty-five with two different random seeds, for a total of one hundred seventy results.  

\

\hfil\noindent\begin{tabular}{ccc}
	\multicolumn{3}{c}{Seventeen Models} \cr
	Model & $\alpha$ & $\gamma$ \cr\hline
	Focal & 1/2 & 0.0 \cr
	Focal & 2/3 & 0.0 \cr
	Focal & 2/3 & 0.5 \cr
	Focal & 2/3 & 1.0 \cr
	Focal & 2/3 & 2.0 \cr
	Focal & 2/3 & 5.0 \cr
	Focal & 0.85 & 0.0 \cr
	Random Forest & 1/2 & \cr
	Random Forest & 2/3 & \cr
	Random Forest & 0.85 & \cr
	Bagging && \cr
	RUSBoost && \cr
	Easy Ens && \cr
	Log Reg & 1/2 & \cr
	Log Reg & 2/3 & \cr
	Log Reg & 0.85 & \cr
	AdaBoost && \cr
\end{tabular}
\quad
$\times$
\quad
\begin{tabular}{cc}
	\multicolumn{2}{c}{Seven Datasets} \cr
	Features & Tomek \cr\hline
	Hard & None \cr
	Hard & Once \cr
	Hard & Twice \cr
	Medium & None \cr
	Medium & Once \cr
	Medium & Twice \cr
	Easy & None \cr
\end{tabular}
\quad
$\times$
\quad
\begin{tabular}{cc}
	Run twice with \cr
	different \cr
	random seeds \cr\hline
	Random seed 1 \cr
	Random seed 2
\end{tabular}
\quad 
$=$ 
\quad 
\begin{tabular}{c}
	238  \cr Sets of  \cr
	Results \cr
\end{tabular}










