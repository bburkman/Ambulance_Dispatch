{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd5c45",
   "metadata": {},
   "source": [
    "# Ambulance_Dispatch_2024_Reduce_Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc57ea1",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- The goal of this notebook is to reduce the dimensionality of each dataset by identifying and dropping features that are well predicted by the remaining features by multicollinearity.\n",
    "\n",
    "- In the next notebook we will build models on three groups of features:\n",
    "    - Easy:  Features that the emergency dispatcher already has or can, without really precise maps, determine from the location\n",
    "    - Medium:  Additionally, features that the emergency dispatcher can determine from precise maps and information from the cell service provider about the primary user of the phone\n",
    "    - Hard:  Additionally, features about the vehicle that can be learned only by correlating information about the identity of the likely driver with vehicle registration and/or insurance records.  Not easily available in real time without lots of preparation, raises privacy concerns, and not likely to be very accurate\n",
    "\n",
    "- The outbook of this notebook is the three sets of features, expressed as a dummy variable for each binned value of the feature, after dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd8f20",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "- For each of [Easy, Medium, Hard]:\n",
    "    - For each feature:\n",
    "        - Create a linear model (LinearRegression from sklearn) mapping the other features onto this feature.\n",
    "        - Find the $R^2$ score from fitting the model:  r2 = LinearRegression().fit(X, y).score(X, y) where y is this feature and X is all of the other features.  \n",
    "        - If $R^2=1$, then this feature is perfectly predicted by the other features.  \n",
    "        - If this $R^2$ score is high, like greater than 0.9, then this feature is well predicted by other features, and we should consider dropping it.\n",
    "        - The reason to not drop all features with high $R^2$ scores in the same step is that two (or more) features could be highly collinear, and we may only want to drop one of them.  If we drop one of them and recalculate, we may see that the other(s) now have low $R^2$ scores.\n",
    "\n",
    "    - While the $\\max(R^2) > 0.90$, run the above method and drop the feature with the highest $R^2$ score.\n",
    "        - This choice of threshold of 0.90 is somewhat arbitary, perhaps arbitrarily high, and testing the results of different choices is an opportunity for future research.\n",
    "\n",
    "    - Transform the features, which have 2-10 values, into 1-9 dummy features.  We choose to drop the first value because each of the dummy features of an original feature would be perfectly collinear with the others.\n",
    "\n",
    "    - Repeat the ``While the $\\max(R^2) > 0.90$,...'' process to reduce the number of features.  \n",
    "    \n",
    "    - Write the reduced dummy-variable features to file to use in the next notebook, where we will build models predicting whether each crash person needs an ambulance.\n",
    "    \n",
    "- We considered using Principal Component Analysis, but decided on $R^2$ instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01dfa7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy version: 1.26.4\n",
      "Pandas version:  2.2.2\n",
      "sklearn version: 1.5.0\n",
      "Finished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import sklearn\n",
    "print ('sklearn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set Randomness.  \n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d0f6e",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d081c9",
   "metadata": {},
   "source": [
    "This function pulls in the saved output from Ambulance_Dispatch_2024_03_Impute_Missing_Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a5add5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Get_Data(target):\n",
    "    print ('Get_Data')\n",
    "    \n",
    "    data = pd.read_csv('../../Big_Files/CRSS_Imputed_Data.csv', index_col=None)\n",
    "    \n",
    "    print ('data.shape = ', data.shape)\n",
    "    data = data.reindex(sorted(data.columns), axis=1)\n",
    "    first_column = data.pop(target) \n",
    "    data.insert(0, target, first_column)     \n",
    "\n",
    "    return data\n",
    "\n",
    "#data =  Get_Data('HOSPITAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db659bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Hard_Features(data):\n",
    "    print ('Thin_to_Hard_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "        'PERMVIT',\n",
    "#        'PERNOTMVIT', # Pedestrians, which we have taken out\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "#        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION', # Pedestrian location; taken out\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "\n",
    "    print ('Removed Features')\n",
    "    for feature in data:\n",
    "        if feature not in Features:\n",
    "            print (feature)\n",
    "    print ()\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Hard_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Hard_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Hard_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Hard_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4385a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Medium_Features(data):\n",
    "    print ('Thin_to_Medium_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "#        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "    \n",
    "    print ('Removed Features')\n",
    "    for feature in data:\n",
    "        if feature not in Features:\n",
    "            print (feature)\n",
    "    print ()\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Medium_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Medium_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Medium_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Medium_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e799b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Easy_Features(data):\n",
    "    print ('Thin_to_Easy_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "#        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "#        'REL_ROAD',\n",
    "#        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "#        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "#        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "#        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "#        'VPROFILE',\n",
    "#        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "#        'VTRAFCON',\n",
    "#        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "#        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "#        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "#        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "\n",
    "    print ('Removed Features')\n",
    "    for feature in data:\n",
    "        if feature not in Features:\n",
    "            print (feature)\n",
    "    print ()\n",
    "        \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Easy_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Easy_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Easy_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Easy_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bface0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Dummies(data, target):\n",
    "    print ('Get_Dummies')\n",
    "    print (data.shape)\n",
    "    data = data.astype('category')\n",
    "    Target = data.pop(target)\n",
    "    \n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns, drop_first = True)\n",
    "\n",
    "    # Use this version if the dataset has \"99\" signifying \"Missing/Unknown\",\n",
    "    # but not if missing values have already been imputed.\n",
    "#    data_Dummies = pd.get_dummies(data, prefix = data.columns, drop_first = False)\n",
    "#    for feature in data_Dummies:\n",
    "#        if '99' in feature:\n",
    "#            data_Dummies.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    data_Dummies = data_Dummies.join(Target)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print (data_Dummies.shape)\n",
    "    print ()\n",
    "    \n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "#data = Get_Dummies(data, 'HOSPITAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "584c9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Principal_Component_Analysis(data):\n",
    "    print ('Principal_Component_Analysis()')\n",
    "    Features = [feature for feature in data]\n",
    "    n_components=300\n",
    "    pca = PCA(n_components)\n",
    "    print ('pca.fit()')\n",
    "    pca.fit(data)\n",
    "    \n",
    "    data_pca = pca.transform(data)\n",
    "    print (data_pca.shape)\n",
    "    print (data_pca[:10])\n",
    "    print ()\n",
    "    data_pca = np.ascontiguousarray(data_pca)\n",
    "    print (data_pca.shape)\n",
    "    print (data_pca[:10])\n",
    "    print ()\n",
    "    data_pca = pd.DataFrame(data_pca) #, columns=['PCA%i' % i for i in range(n_components)], index=data.index)\n",
    "    print (data_pca.head())\n",
    "    print (data_pca.shape)\n",
    "    print ()\n",
    "    \n",
    "\n",
    "    return data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86570e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f\n",
    "def calculate_vif(df, features):    \n",
    "    r2_Dict, tolerance, vif = {}, {}, {}\n",
    "    # all the features that you want to examine\n",
    "    for feature in features:\n",
    "        # extract all the other features you will regress against\n",
    "        X = [f for f in features if f != feature]        \n",
    "        X, y = df[X], df[feature]\n",
    "        # extract r-squared from the fit\n",
    "        r2 = LinearRegression().fit(X, y).score(X, y)\n",
    "        r2_Dict[feature] = r2\n",
    "        # calculate tolerance\n",
    "        tolerance[feature] = 1 - r2\n",
    "        # calculate VIF\n",
    "        if tolerance[feature] !=0:\n",
    "            vif[feature] = 1/(tolerance[feature])\n",
    "        else:\n",
    "            vif[feature] = 10000\n",
    "\n",
    "    return pd.DataFrame({'r2': r2_Dict, 'Tolerance': tolerance, 'VIF': vif}), tolerance, r2_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01df1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively remove the feature with the largest VIF ('Variance Inflaction Factor')\n",
    "# until the largest VIF is 10, or smallest Tolerance is 0.1, or largest R^2 is 0.9\n",
    "def Reduce_Dimensionality(data, target):\n",
    "    Target = data.pop(target)\n",
    "    Features = [feature for feature in data]\n",
    "    VIF, Tolerance_Dict, r2_Dict = calculate_vif(data, Features)\n",
    "    Max_r2_Feature = VIF['r2'].idxmax()\n",
    "    display(VIF)\n",
    "    print (Max_r2_Feature)\n",
    "    if r2_Dict[Max_r2_Feature] > 0.9:\n",
    "        data.drop(columns = [Max_r2_Feature], inplace=True)\n",
    "        print ('Drop ', Max_r2_Feature)\n",
    "    print ()\n",
    "    while r2_Dict[Max_r2_Feature] > 0.9:\n",
    "        Features = [feature for feature in data]\n",
    "        VIF, Tolerance_Dict, r2_Dict = calculate_vif(data, Features)\n",
    "        Max_r2_Feature = VIF['r2'].idxmax()\n",
    "        display(VIF)\n",
    "        print (Max_r2_Feature)\n",
    "        if r2_Dict[Max_r2_Feature] > 0.9:\n",
    "            data.drop(columns = [Max_r2_Feature], inplace=True)\n",
    "            print ('Drop ', Max_r2_Feature)\n",
    "        print ()\n",
    "\n",
    "    data = data.join(Target)\n",
    "        \n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdec6303",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data\n",
      "data.shape =  (817623, 67)\n",
      "Thin_to_Easy_Features\n",
      "Thin_to_Easy_Features()\n",
      "Removed Features\n",
      "ACC_TYPE\n",
      "AGE\n",
      "AIR_BAG\n",
      "ALC_STATUS\n",
      "BODY_TYP\n",
      "CARGO_BT\n",
      "DEFORMED\n",
      "DR_ZIP\n",
      "EJECTION\n",
      "HARM_EV\n",
      "HIT_RUN\n",
      "IMPACT1\n",
      "INJ_SEV\n",
      "INT_HWY\n",
      "J_KNIFE\n",
      "LGT_COND\n",
      "MAKE\n",
      "MAK_MOD\n",
      "MAN_COLL\n",
      "MAX_SEV\n",
      "MAX_VSEV\n",
      "MODEL\n",
      "M_HARM\n",
      "NUMOCCS\n",
      "NUM_INJ\n",
      "NUM_INJV\n",
      "PCRASH4\n",
      "PCRASH5\n",
      "PERMVIT\n",
      "PER_TYP\n",
      "PVH_INVL\n",
      "P_CRASH1\n",
      "P_CRASH2\n",
      "RELJCT1\n",
      "RELJCT2\n",
      "REL_ROAD\n",
      "REST_MIS\n",
      "REST_USE\n",
      "ROLINLOC\n",
      "ROLLOVER\n",
      "SEAT_POS\n",
      "SEX\n",
      "SPEC_USE\n",
      "SPEEDREL\n",
      "TOWED\n",
      "TOW_VEH\n",
      "TYP_INT\n",
      "VALIGN\n",
      "VEH_AGE\n",
      "VE_FORMS\n",
      "VE_TOTAL\n",
      "VPROFILE\n",
      "VSPD_LIM\n",
      "VSURCOND\n",
      "VTCONT_F\n",
      "VTRAFCON\n",
      "VTRAFWAY\n",
      "WRK_ZONE\n",
      "\n",
      "data.shape:  (817623, 9)\n",
      "End Thin_to_Easy_Features()\n",
      "\n",
      "(817623, 9)\n",
      "Get_Dummies\n",
      "(817623, 9)\n",
      "(817623, 40)\n",
      "\n",
      "(817623, 9)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DAY_WEEK_1</th>\n",
       "      <td>0.264405</td>\n",
       "      <td>0.735595</td>\n",
       "      <td>1.359444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_WEEK_2</th>\n",
       "      <td>0.274139</td>\n",
       "      <td>0.725861</td>\n",
       "      <td>1.377674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_WEEK_3</th>\n",
       "      <td>0.367973</td>\n",
       "      <td>0.632027</td>\n",
       "      <td>1.582211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_WEEK_4</th>\n",
       "      <td>0.291846</td>\n",
       "      <td>0.708154</td>\n",
       "      <td>1.412122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_1</th>\n",
       "      <td>0.507268</td>\n",
       "      <td>0.492732</td>\n",
       "      <td>2.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_2</th>\n",
       "      <td>0.517712</td>\n",
       "      <td>0.482288</td>\n",
       "      <td>2.073449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_3</th>\n",
       "      <td>0.722258</td>\n",
       "      <td>0.277742</td>\n",
       "      <td>3.600467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_4</th>\n",
       "      <td>0.868739</td>\n",
       "      <td>0.131261</td>\n",
       "      <td>7.618424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_5</th>\n",
       "      <td>0.870694</td>\n",
       "      <td>0.129306</td>\n",
       "      <td>7.733611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR_6</th>\n",
       "      <td>0.788340</td>\n",
       "      <td>0.211660</td>\n",
       "      <td>4.724565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH_1</th>\n",
       "      <td>0.304607</td>\n",
       "      <td>0.695393</td>\n",
       "      <td>1.438036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH_2</th>\n",
       "      <td>0.321844</td>\n",
       "      <td>0.678156</td>\n",
       "      <td>1.474586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH_3</th>\n",
       "      <td>0.332401</td>\n",
       "      <td>0.667599</td>\n",
       "      <td>1.497906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH_4</th>\n",
       "      <td>0.336382</td>\n",
       "      <td>0.663618</td>\n",
       "      <td>1.506891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH_5</th>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.783270</td>\n",
       "      <td>1.276699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_1</th>\n",
       "      <td>0.479834</td>\n",
       "      <td>0.520166</td>\n",
       "      <td>1.922463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_2</th>\n",
       "      <td>0.515377</td>\n",
       "      <td>0.484623</td>\n",
       "      <td>2.063460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_3</th>\n",
       "      <td>0.788083</td>\n",
       "      <td>0.211917</td>\n",
       "      <td>4.718827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_4</th>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.204306</td>\n",
       "      <td>4.894628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_5</th>\n",
       "      <td>0.821904</td>\n",
       "      <td>0.178096</td>\n",
       "      <td>5.614964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_6</th>\n",
       "      <td>0.795035</td>\n",
       "      <td>0.204965</td>\n",
       "      <td>4.878877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_7</th>\n",
       "      <td>0.796742</td>\n",
       "      <td>0.203258</td>\n",
       "      <td>4.919845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_8</th>\n",
       "      <td>0.818443</td>\n",
       "      <td>0.181557</td>\n",
       "      <td>5.507913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PJ_9</th>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.334405</td>\n",
       "      <td>2.990387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_1</th>\n",
       "      <td>0.708226</td>\n",
       "      <td>0.291774</td>\n",
       "      <td>3.427306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_2</th>\n",
       "      <td>0.755966</td>\n",
       "      <td>0.244034</td>\n",
       "      <td>4.097793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_3</th>\n",
       "      <td>0.876574</td>\n",
       "      <td>0.123426</td>\n",
       "      <td>8.102049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_4</th>\n",
       "      <td>0.857701</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>7.027440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_5</th>\n",
       "      <td>0.815576</td>\n",
       "      <td>0.184424</td>\n",
       "      <td>5.422301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_6</th>\n",
       "      <td>0.781098</td>\n",
       "      <td>0.218902</td>\n",
       "      <td>4.568252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_7</th>\n",
       "      <td>0.754047</td>\n",
       "      <td>0.245953</td>\n",
       "      <td>4.065821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_8</th>\n",
       "      <td>0.823526</td>\n",
       "      <td>0.176474</td>\n",
       "      <td>5.666563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSU_9</th>\n",
       "      <td>0.673789</td>\n",
       "      <td>0.326211</td>\n",
       "      <td>3.065504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION_1</th>\n",
       "      <td>0.715585</td>\n",
       "      <td>0.284415</td>\n",
       "      <td>3.515989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION_2</th>\n",
       "      <td>0.632264</td>\n",
       "      <td>0.367736</td>\n",
       "      <td>2.719345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION_3</th>\n",
       "      <td>0.594590</td>\n",
       "      <td>0.405410</td>\n",
       "      <td>2.466640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URBANICITY_1</th>\n",
       "      <td>0.210509</td>\n",
       "      <td>0.789491</td>\n",
       "      <td>1.266639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEATHER_1</th>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.969150</td>\n",
       "      <td>1.031832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEATHER_2</th>\n",
       "      <td>0.058793</td>\n",
       "      <td>0.941207</td>\n",
       "      <td>1.062465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    r2  Tolerance       VIF\n",
       "DAY_WEEK_1    0.264405   0.735595  1.359444\n",
       "DAY_WEEK_2    0.274139   0.725861  1.377674\n",
       "DAY_WEEK_3    0.367973   0.632027  1.582211\n",
       "DAY_WEEK_4    0.291846   0.708154  1.412122\n",
       "HOUR_1        0.507268   0.492732  2.029500\n",
       "HOUR_2        0.517712   0.482288  2.073449\n",
       "HOUR_3        0.722258   0.277742  3.600467\n",
       "HOUR_4        0.868739   0.131261  7.618424\n",
       "HOUR_5        0.870694   0.129306  7.733611\n",
       "HOUR_6        0.788340   0.211660  4.724565\n",
       "MONTH_1       0.304607   0.695393  1.438036\n",
       "MONTH_2       0.321844   0.678156  1.474586\n",
       "MONTH_3       0.332401   0.667599  1.497906\n",
       "MONTH_4       0.336382   0.663618  1.506891\n",
       "MONTH_5       0.216730   0.783270  1.276699\n",
       "PJ_1          0.479834   0.520166  1.922463\n",
       "PJ_2          0.515377   0.484623  2.063460\n",
       "PJ_3          0.788083   0.211917  4.718827\n",
       "PJ_4          0.795694   0.204306  4.894628\n",
       "PJ_5          0.821904   0.178096  5.614964\n",
       "PJ_6          0.795035   0.204965  4.878877\n",
       "PJ_7          0.796742   0.203258  4.919845\n",
       "PJ_8          0.818443   0.181557  5.507913\n",
       "PJ_9          0.665595   0.334405  2.990387\n",
       "PSU_1         0.708226   0.291774  3.427306\n",
       "PSU_2         0.755966   0.244034  4.097793\n",
       "PSU_3         0.876574   0.123426  8.102049\n",
       "PSU_4         0.857701   0.142299  7.027440\n",
       "PSU_5         0.815576   0.184424  5.422301\n",
       "PSU_6         0.781098   0.218902  4.568252\n",
       "PSU_7         0.754047   0.245953  4.065821\n",
       "PSU_8         0.823526   0.176474  5.666563\n",
       "PSU_9         0.673789   0.326211  3.065504\n",
       "REGION_1      0.715585   0.284415  3.515989\n",
       "REGION_2      0.632264   0.367736  2.719345\n",
       "REGION_3      0.594590   0.405410  2.466640\n",
       "URBANICITY_1  0.210509   0.789491  1.266639\n",
       "WEATHER_1     0.030850   0.969150  1.031832\n",
       "WEATHER_2     0.058793   0.941207  1.062465"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSU_3\n",
      "\n",
      "(817623, 9)\n",
      "\n",
      "\n",
      "CRSS_Reduced_Easy_Features.csv Written\n",
      "\n",
      "Get_Data\n",
      "data.shape =  (817623, 67)\n",
      "Thin_to_Medium_Features\n",
      "Thin_to_Medium_Features()\n",
      "Removed Features\n",
      "ACC_TYPE\n",
      "AIR_BAG\n",
      "ALC_STATUS\n",
      "BODY_TYP\n",
      "CARGO_BT\n",
      "DEFORMED\n",
      "DR_ZIP\n",
      "EJECTION\n",
      "HARM_EV\n",
      "HIT_RUN\n",
      "IMPACT1\n",
      "INJ_SEV\n",
      "J_KNIFE\n",
      "LGT_COND\n",
      "MAKE\n",
      "MAK_MOD\n",
      "MAN_COLL\n",
      "MAX_SEV\n",
      "MAX_VSEV\n",
      "MODEL\n",
      "M_HARM\n",
      "NUMOCCS\n",
      "NUM_INJ\n",
      "NUM_INJV\n",
      "PCRASH4\n",
      "PCRASH5\n",
      "PERMVIT\n",
      "PER_TYP\n",
      "PVH_INVL\n",
      "P_CRASH1\n",
      "P_CRASH2\n",
      "RELJCT2\n",
      "REST_MIS\n",
      "REST_USE\n",
      "ROLINLOC\n",
      "ROLLOVER\n",
      "SEAT_POS\n",
      "SPEC_USE\n",
      "SPEEDREL\n",
      "TOWED\n",
      "TOW_VEH\n",
      "VEH_AGE\n",
      "VE_FORMS\n",
      "VE_TOTAL\n",
      "VSURCOND\n",
      "VTCONT_F\n",
      "WRK_ZONE\n",
      "\n",
      "data.shape:  (817623, 20)\n",
      "End Thin_to_Medium_Features()\n",
      "\n",
      "(817623, 20)\n",
      "Get_Dummies\n",
      "(817623, 20)\n",
      "(817623, 80)\n",
      "\n",
      "(817623, 20)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:60\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:33\u001b[0m, in \u001b[0;36mMain\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mReduce_Dimensionality\u001b[0;34m(data, target)\u001b[0m\n\u001b[1;32m      4\u001b[0m Target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpop(target)\n\u001b[1;32m      5\u001b[0m Features \u001b[38;5;241m=\u001b[39m [feature \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m----> 6\u001b[0m VIF, Tolerance_Dict, r2_Dict \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_vif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m Max_r2_Feature \u001b[38;5;241m=\u001b[39m VIF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax()\n\u001b[1;32m      8\u001b[0m display(VIF)\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mcalculate_vif\u001b[0;34m(df, features)\u001b[0m\n\u001b[1;32m      8\u001b[0m X, y \u001b[38;5;241m=\u001b[39m df[X], df[feature]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# extract r-squared from the fit\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m r2 \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscore(X, y)\n\u001b[1;32m     11\u001b[0m r2_Dict[feature] \u001b[38;5;241m=\u001b[39m r2\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# calculate tolerance\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/linear_model/_base.py:682\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outs])\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_ \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/scipy/linalg/_basic.py:1270\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m real_data:\n\u001b[1;32m   1269\u001b[0m     lwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n, nrhs, cond)\n\u001b[0;32m-> 1270\u001b[0m     x, s, rank, info \u001b[38;5;241m=\u001b[39m \u001b[43mlapack_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43miwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# complex data\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     lwork, rwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n,\n\u001b[1;32m   1274\u001b[0m                                          nrhs, cond)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Main():\n",
    "    target = 'HOSPITAL'\n",
    "\n",
    "    for i in range (3):\n",
    "        data = Get_Data(target)\n",
    "        data = data.astype('int64')\n",
    "        if i==2:\n",
    "            print ('Thin_to_Hard_Features()')\n",
    "            data = Thin_to_Hard_Features(data)\n",
    "        if i==1:\n",
    "            print ('Thin_to_Medium_Features')\n",
    "            data = Thin_to_Medium_Features(data)\n",
    "        if i==0:\n",
    "            print ('Thin_to_Easy_Features')\n",
    "            data = Thin_to_Easy_Features(data)\n",
    "            \n",
    "#    Features = [feature for feature in data]\n",
    "#    VIF, VIF_Dict = calculate_vif(data, Features)\n",
    "#    display(VIF)\n",
    "#    print ()\n",
    "\n",
    "        data = Reduce_Dimensionality(data, target)\n",
    "    \n",
    "        for feature in data:\n",
    "            data[feature] = pd.to_numeric(data[feature])\n",
    "        print (data.shape)\n",
    "\n",
    "        data_dummies = Get_Dummies(data, target)\n",
    "#        for feature in data_dummies:\n",
    "#            print (feature)\n",
    "        print (data.shape)\n",
    "        print ()\n",
    "        data_dummies = Reduce_Dimensionality(data_dummies, target)\n",
    "#        for feature in data_dummies:\n",
    "#            print (feature)\n",
    "        print (data.shape)\n",
    "        print ()\n",
    "        \n",
    "        \n",
    "        if i==2:\n",
    "            data.to_csv('../../Big_Files/CRSS_Reduced_Hard_Features.csv', index=False)\n",
    "            print ()\n",
    "            print ('CRSS_Reduced_Hard_Features.csv Written')\n",
    "            print ()\n",
    "        if i==1:\n",
    "            data.to_csv('../../Big_Files/CRSS_Reduced_Medium_Features.csv', index=False)\n",
    "            print ()\n",
    "            print ('CRSS_Reduced_Medium_Features.csv Written')\n",
    "            print ()\n",
    "        if i==0:\n",
    "            data.to_csv('../../Big_Files/CRSS_Reduced_Easy_Features.csv', index=False)\n",
    "            print ()\n",
    "            print ('CRSS_Reduced_Easy_Features.csv Written')\n",
    "            print ()\n",
    "    \n",
    "    print ()\n",
    "    print ('Finished!')\n",
    "    \n",
    "    \n",
    "Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d067ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d86e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
