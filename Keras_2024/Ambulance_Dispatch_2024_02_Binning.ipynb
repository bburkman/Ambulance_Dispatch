{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edb2407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57cfc2",
   "metadata": {},
   "source": [
    "# Ambulance_Dispatch_02_Binning_2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f533cd",
   "metadata": {},
   "source": [
    "## Goals of this Notebook\n",
    "- Bin the values of each feature into up to some number of different codes, plus 99 for the values representing Missing or Unknown.\n",
    "- We chose ten as the maximum number of codes for each feature, but that's easy to change if you want to explore different options.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286f65",
   "metadata": {},
   "source": [
    "## Background\n",
    "- Each feature has at least two and up to several hundred values.\n",
    "- Most features have one or more values signifying \"Missing\" or \"Unknown.\"  \n",
    "    - The previous notebook, Ambulance_Dispatch_01_Get_Data, creates a dictionary called Missing_Unknown_Dict that is saved in Big_Files as Missing_Unknown_Dict.json.\n",
    "- All of the values are categorical.  \n",
    "    - Some are ordered and linear, like age of person, number of lanes, speed limit,...\n",
    "    - Some are ordered and periodic, particularly hour, day of week, and month.  We binned these by hand.\n",
    "    - Most are unordered, like MAKE:  In which order would you put Buick, Renault, and Toyota?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0bde5",
   "metadata": {},
   "source": [
    "## Method\n",
    "For each feature:\n",
    "- Make a dataframe with just that feature and the target variable, HOSPITAL.\n",
    "- Delete all rows with missing or unknown values.\n",
    "- Make a list of the unique (known) values of the feature.\n",
    "- For each unique value, \n",
    "    - find the percentage of the set with that value, and\n",
    "    - find the correlation between that value and the target variable, HOSPITAL, the proportion of the samples with that value for which HOSPITAL == 1.\n",
    "- For unordered features, order the unique values by their correlation to HOSPITAL.\n",
    "- As an example, MAKE has 68 unique values representing vehicle makes and three values for Missing/Unknown:\n",
    "    - 97:  Not Reported\n",
    "    - 98:  Other Make\n",
    "    - 99:  Unknown Make\n",
    "- This table shows, for each of the 68 MAKE codes, the percentage of crash persons (in a vehicle of known make) in a vehicle of that make, and the correlation (percentage of those persons who went) to the hospital.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a01992",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [75]  |  0.0001  |  100.0  | \n",
    " |  [71]  |  0.025  |  70.3518  | \n",
    " |  [76]  |  0.3382  |  67.1625  | \n",
    " |  [74]  |  0.0023  |  66.6667  | \n",
    " |  [73]  |  0.2749  |  65.7365  | \n",
    " |  [72]  |  0.9013  |  65.7318  | \n",
    " |  [50]  |  0.0297  |  61.8644  | \n",
    " |  [77]  |  0.0201  |  56.875  | \n",
    " |  [53]  |  0.4319  |  49.5341  | \n",
    " |  [64]  |  0.0015  |  33.3333  | \n",
    " |  [43]  |  0.0005  |  25.0  | \n",
    " |  [65]  |  0.0122  |  23.7113  | \n",
    " |  [21]  |  0.1562  |  21.1755  | \n",
    " |  [9]  |  0.0376  |  21.0702  | \n",
    " |  [92]  |  0.0809  |  18.6625  | \n",
    " |  [22]  |  0.8055  |  17.9547  | \n",
    " |  [52]  |  0.8212  |  17.6417  | \n",
    " |  [14]  |  0.4968  |  17.2152  | \n",
    " |  [24]  |  0.4492  |  17.1613  | \n",
    " |  [37]  |  9.5237  |  17.0835  | \n",
    " |  [18]  |  1.4039  |  17.0116  | \n",
    " |  [63]  |  3.3311  |  16.7894  | \n",
    " |  [35]  |  8.247  |  16.4237  | \n",
    " |  [36]  |  0.0635  |  16.0396  | \n",
    " |  [55]  |  4.0438  |  15.8767  | \n",
    " |  [6]  |  1.9401  |  15.6609  | \n",
    " |  [69]  |  0.1693  |  15.3789  | \n",
    " |  [20]  |  12.6771  |  15.1059  | \n",
    " |  [13]  |  0.5906  |  14.9489  | \n",
    " |  [34]  |  1.3292  |  14.8358  | \n",
    " |  [39]  |  0.1026  |  14.8284  | \n",
    " |  [47]  |  0.0438  |  14.6552  | \n",
    " |  [67]  |  0.1239  |  14.5178  | \n",
    " |  [49]  |  11.9024  |  14.3657  | \n",
    " |  [25]  |  0.007  |  14.2857  | \n",
    " |  [19]  |  0.9208  |  14.2721  | \n",
    " |  [58]  |  0.9032  |  13.9933  | \n",
    " |  [12]  |  13.3348  |  13.9543  | \n",
    " |  [41]  |  1.488  |  13.9537  | \n",
    " |  [30]  |  1.5739  |  13.7195  | \n",
    " |  [2]  |  3.3535  |  13.6359  | \n",
    " |  [42]  |  1.3751  |  13.5449  | \n",
    " |  [7]  |  5.9545  |  13.2976  | \n",
    " |  [54]  |  0.9884  |  13.1314  | \n",
    " |  [23]  |  2.7177  |  12.7256  | \n",
    " |  [59]  |  1.4425  |  12.6242  | \n",
    " |  [93]  |  0.0101  |  12.5  | \n",
    " |  [31]  |  0.0174  |  12.3188  | \n",
    " |  [48]  |  1.3498  |  11.8979  | \n",
    " |  [3]  |  0.055  |  11.4416  | \n",
    " |  [62]  |  0.2026  |  10.6145  | \n",
    " |  [38]  |  0.1742  |  10.3249  | \n",
    " |  [45]  |  0.0917  |  10.1509  | \n",
    " |  [32]  |  0.5354  |  10.007  | \n",
    " |  [29]  |  0.1373  |  9.5238  | \n",
    " |  [51]  |  0.6131  |  9.4154  | \n",
    " |  [90]  |  0.0548  |  8.945  | \n",
    " |  [10]  |  0.0015  |  8.3333  | \n",
    " |  [94]  |  0.0218  |  5.7803  | \n",
    " |  [84]  |  0.5081  |  4.9505  | \n",
    " |  [89]  |  0.0136  |  4.6296  | \n",
    " |  [86]  |  0.1836  |  4.5205  | \n",
    " |  [82]  |  0.9525  |  4.1326  | \n",
    " |  [85]  |  0.2953  |  4.0886  | \n",
    " |  [87]  |  0.3452  |  3.4244  | \n",
    " |  [1]  |  0.0004  |  0.0  | \n",
    " |  [33]  |  0.0001  |  0.0  | \n",
    " |  [46]  |  0.0001  |  0.0  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df7442",
   "metadata": {},
   "source": [
    "- We want to bin these 68 codes into a smaller number of bins.  For an example, we'll use 10 bins as our goal.\n",
    "- First merge all of the codes with the same correlation to HOSPITAL, which is MAKE is only the codes with corr==0.0, meaning that of the crashes in that MAKE of vehicle, no one went to the hospital.  \n",
    "- Now we have 66 bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd48e5c0",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [75]  |  0.0001  |  100.0  | \n",
    " |  [71]  |  0.025  |  70.3518  | \n",
    " | ... | ... | ... |\n",
    " |  [82]  |  0.9525  |  4.1326  | \n",
    " |  [85]  |  0.2953  |  4.0886  | \n",
    " |  [87]  |  0.3452  |  3.4244  | \n",
    " |  [1, 33, 46]  |  0.0006  |  0.0  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813c80a",
   "metadata": {},
   "source": [
    "- Next find the smallest Per and merge it either into the bin above or below, whichever has the closest Corr.\n",
    "    - Repeat until the smallest Per is 2%.  (This choice is arbitrary, and refining it is an opportunity for future research)\n",
    "    - The smallest Per was 0.0001 with code [75], so merge it with [71].  \n",
    "    - The new Per is the sum of the two Per's, and the Corr is the weighted average of the two.\n",
    "    - The next smallest Per is 0.0005 with code [43], whose Corr of 25.0% is closer to the Corr of the bin below (23.7%) than that of the bin above (33.33%), so merge [43] with [65]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997389ac",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [75, 71]  |  0.0252  |  70.5  | \n",
    " |  [76]  |  0.3382  |  67.1625  | \n",
    " |  [74]  |  0.0023  |  66.6667  | \n",
    " |  [73]  |  0.2749  |  65.7365  | \n",
    " |  [72]  |  0.9013  |  65.7318  | \n",
    " |  [50]  |  0.0297  |  61.8644  | \n",
    " |  [77]  |  0.0201  |  56.875  | \n",
    " |  [53]  |  0.4319  |  49.5341  | \n",
    " |  [64]  |  0.0015  |  33.3333  | \n",
    " |  [43]  |  0.0005  |  25.0  | \n",
    " |  [65]  |  0.0122  |  23.7113  | \n",
    " |  [21]  |  0.1562  |  21.1755  | \n",
    " \n",
    " | Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [75, 71]  |  0.0252  |  70.5  | \n",
    " |  [76]  |  0.3382  |  67.1625  | \n",
    " |  [74]  |  0.0023  |  66.6667  | \n",
    " |  [73]  |  0.2749  |  65.7365  | \n",
    " |  [72]  |  0.9013  |  65.7318  | \n",
    " |  [50]  |  0.0297  |  61.8644  | \n",
    " |  [77]  |  0.0201  |  56.875  | \n",
    " |  [53]  |  0.4319  |  49.5341  | \n",
    " |  [64]  |  0.0015  |  33.3333  | \n",
    " |  [43, 65]  |  0.0127  |  23.7624  | \n",
    " |  [21]  |  0.1562  |  21.1755  | \n",
    " |  [9]  |  0.0376  |  21.0702  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec013a0",
   "metadata": {},
   "source": [
    "- Continue until the smallest Per is at least 2.0%.\n",
    "    - Now we're down to fifteen bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1589c0",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [53, 75, 71, 74, 76, 73, 77, 50, 72]  |  2.0235  |  62.4301  | \n",
    " |  [9, 64, 43, 65, 21, 92, 22, 52, 24, 14]  |  2.8616  |  17.8826  | \n",
    " |  [18, 37]  |  10.9276  |  17.0743  | \n",
    " |  [63]  |  3.3311  |  16.7894  | \n",
    " |  [35]  |  8.247  |  16.4237  | \n",
    " |  [6, 36, 55]  |  6.0474  |  15.8092  | \n",
    " |  [69, 20]  |  12.8464  |  15.1094  | \n",
    " |  [13, 39, 34]  |  2.0224  |  14.8685  | \n",
    " |  [25, 19, 47, 67, 49]  |  12.9979  |  14.3615  | \n",
    " |  [41, 58, 12]  |  15.7261  |  13.9565  | \n",
    " |  [30, 42, 2]  |  6.3025  |  13.6369  | \n",
    " |  [54, 7]  |  6.9428  |  13.2739  | \n",
    " |  [23]  |  2.7177  |  12.7256  | \n",
    " |  [29, 10, 90, 51, 38, 62, 45, 32, 3, 48, 31, 93, 59]  |  4.6853  |  11.3247  | \n",
    " |  [94, 84, 1, 33, 46, 87, 85, 89, 86, 82]  |  2.3207  |  4.2486  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690daaa",
   "metadata": {},
   "source": [
    "- Finally, find the smallest gaps in Corr and merge until we have at most ten bins.\n",
    "    - Here, the smallest gap is between [69,20] at Corr of 15.1094 and [13,39,34] with Corr of 14.8685.  Merge those two.\n",
    "- The choice of a maximum of ten bins was arbitrary, and searching for a better option is an opporunity for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7335f65",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [53, 75, 71, 74, 76, 73, 77, 50, 72]  |  2.0235  |  62.4301  | \n",
    " |  [9, 64, 43, 65, 21, 92, 22, 52, 24, 14]  |  2.8616  |  17.8826  | \n",
    " |  [18, 37]  |  10.9276  |  17.0743  | \n",
    " |  [63]  |  3.3311  |  16.7894  | \n",
    " |  [35]  |  8.247  |  16.4237  | \n",
    " |  [6, 36, 55]  |  6.0474  |  15.8092  | \n",
    " |  [69, 20, 13, 39, 34]  |  14.8688  |  15.0767  | \n",
    " |  [25, 19, 47, 67, 49]  |  12.9979  |  14.3615  | \n",
    " |  [41, 58, 12]  |  15.7261  |  13.9565  | \n",
    " |  [30, 42, 2]  |  6.3025  |  13.6369  | \n",
    " |  [54, 7]  |  6.9428  |  13.2739  | \n",
    " |  [23]  |  2.7177  |  12.7256  | \n",
    " |  [29, 10, 90, 51, 38, 62, 45, 32, 3, 48, 31, 93, 59]  |  4.6853  |  11.3247  | \n",
    " |  [94, 84, 1, 33, 46, 87, 85, 89, 86, 82]  |  2.3207  |  4.2486  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503c1cd",
   "metadata": {},
   "source": [
    "- Here is the final binning with ten bins.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89efe8c6",
   "metadata": {},
   "source": [
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [50, 53, 71, 72, 73, 74, 75, 76, 77]  |  2.0235  |  62.4301  | \n",
    " |  [9, 14, 21, 22, 24, 43, 52, 64, 65, 92]  |  2.8616  |  17.8826  | \n",
    " |  [18, 37, 63]  |  14.2587  |  17.0077  | \n",
    " |  [35]  |  8.247  |  16.4237  | \n",
    " |  [6, 36, 55]  |  6.0474  |  15.8092  | \n",
    " |  [13, 20, 34, 39, 69]  |  14.8688  |  15.0767  | \n",
    " |  [2, 12, 19, 25, 30, 41, 42, 47, 49, 58, 67]  |  35.0265  |  14.0493  | \n",
    " |  [7, 23, 54]  |  9.6606  |  13.1197  | \n",
    " |  [3, 10, 29, 31, 32, 38, 45, 48, 51, 59, 62, 90, 93]  |  4.6853  |  11.3247  | \n",
    " |  [1, 33, 46, 82, 84, 85, 86, 87, 89, 94]  |  2.3207  |  4.2486  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615121de",
   "metadata": {},
   "source": [
    "- Side Note:  Edge Cases.  We had to deal with two features, SPEC_USE and WRK_ZONE because the algorithm condensed them to just one bin. \n",
    "    - We earlier dropped features where one code was at least 99% of the dataset, but the most common value in SPEC_USE is only 98.84%.\n",
    "\n",
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [19]  |  0.0038  |  48.3871  | \n",
    " |  [1]  |  0.159  |  25.2147  | \n",
    " |  [4]  |  0.0026  |  23.8095  | \n",
    " |  [5]  |  0.2904  |  20.1709  | \n",
    " |  [20]  |  0.0247  |  19.598  | \n",
    " |  [10]  |  0.002  |  18.75  | \n",
    " |  [8]  |  0.0073  |  16.9492  | \n",
    " |  [3]  |  0.2186  |  15.7865  | \n",
    " |  [0]  |  98.8412  |  15.6171  | \n",
    " |  [21]  |  0.0262  |  14.6919  | \n",
    " |  [6]  |  0.0989  |  10.2886  | \n",
    " |  [22]  |  0.0122  |  9.1837  | \n",
    " |  [13]  |  0.0014  |  9.0909  | \n",
    " |  [2]  |  0.228  |  8.4377  | \n",
    " |  [7]  |  0.0726  |  2.906  | \n",
    " |  [23]  |  0.0081  |  1.5385  | \n",
    " |  [11]  |  0.0007  |  0.0  | \n",
    " |  [12]  |  0.0022  |  0.0  | \n",
    " \n",
    "-\n",
    "    - If we follow our algorithm, iteratively finding the smallest PER and merging it into the nearest Corr, we get to:\n",
    "\n",
    "| Code | Per | Corr |\n",
    "|---|---|---|\n",
    " |  [19, 4, 1, 10, 20, 5]  |  0.4826  |  22.0422  | \n",
    " |  [8, 3, 21, 0]  |  99.0933  |  15.6174  | \n",
    " |  [6, 11, 12, 23, 7, 13, 22, 2]  |  0.4241  |  7.7553  | \n",
    " \n",
    "-\n",
    "    - If we continued with the algorithm, merging until each bin accounted for at least 2% of the feature, then we would have reduced this feature to just one bin, which is silly.\n",
    "    - Instead, we modified the algorithm to stop binning if the feature has three or fewer bins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e81434",
   "metadata": {},
   "source": [
    "- Back to MAKE example:  Convert to a dictionary\n",
    "\n",
    "{50: 0, 53: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 9: 1, 14: 1, 21: 1, 22: 1, 24: 1, 43: 1, 52: 1, 64: 1, 65: 1, 92: 1, 18: 2, 37: 2, 63: 2, 35: 3, 6: 4, 36: 4, 55: 4, 13: 5, 20: 5, 34: 5, 39: 5, 69: 5, 2: 6, 12: 6, 19: 6, 25: 6, 30: 6, 41: 6, 42: 6, 47: 6, 49: 6, 58: 6, 67: 6, 7: 7, 23: 7, 54: 7, 3: 8, 10: 8, 29: 8, 31: 8, 32: 8, 38: 8, 45: 8, 48: 8, 51: 8, 59: 8, 62: 8, 90: 8, 93: 8, 1: 9, 33: 9, 46: 9, 82: 9, 84: 9, 85: 9, 86: 9, 87: 9, 89: 9, 94: 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fedb26",
   "metadata": {},
   "source": [
    "- Add the Missing/Unknown codes to the dictionary as bin 99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8ffce",
   "metadata": {},
   "source": [
    "{50: 0, 53: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 9: 1, 14: 1, 21: 1, 22: 1, 24: 1, 43: 1, 52: 1, 64: 1, 65: 1, 92: 1, 18: 2, 37: 2, 63: 2, 35: 3, 6: 4, 36: 4, 55: 4, 13: 5, 20: 5, 34: 5, 39: 5, 69: 5, 2: 6, 12: 6, 19: 6, 25: 6, 30: 6, 41: 6, 42: 6, 47: 6, 49: 6, 58: 6, 67: 6, 7: 7, 23: 7, 54: 7, 3: 8, 10: 8, 29: 8, 31: 8, 32: 8, 38: 8, 45: 8, 48: 8, 51: 8, 59: 8, 62: 8, 90: 8, 93: 8, 1: 9, 33: 9, 46: 9, 82: 9, 84: 9, 85: 9, 86: 9, 87: 9, 89: 9, 94: 9, 97: 99, 98: 99, 99: 99}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af2d2a",
   "metadata": {},
   "source": [
    "- Append this dictionary to the master Binning_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943f42c",
   "metadata": {},
   "source": [
    "Binning_Dict = {\n",
    "\n",
    "'ACC_TYPE': {6: 0, 50: 0, 51: 0, 52: 0, 53: 0, 55: 0, 58: 0, 59: 0, 60: 0, 61: 0, 1: 1, 4: 1, 10: 1, 14: 1, 2: 2, 5: 2, 7: 2, 16: 2, 54: 2, 0: 3, 3: 3, 8: 3, 9: 3, 41: 3, 62: 3, 64: 3, 69: 3, 89: 3, 66: 4, 83: 4, 87: 4, 90: 4, 91: 4, 30: 5, 34: 5, 65: 5, 68: 5, 82: 5, 86: 5, 88: 5, 11: 6, 12: 6, 22: 6, 24: 6, 25: 6, 26: 6, 31: 6, 32: 6, 35: 6, 38: 6, 39: 6, 42: 6, 73: 6, 77: 6, 79: 6, 85: 6, 15: 7, 21: 7, 27: 7, 29: 7, 33: 7, 40: 7, 43: 7, 48: 7, 71: 7, 72: 7, 75: 7, 80: 7, 81: 7, 13: 8, 20: 8, 23: 8, 28: 8, 44: 8, 45: 8, 47: 8, 49: 8, 67: 8, 74: 8, 76: 8, 78: 8, 84: 8, 36: 9, 37: 9, 46: 9, 56: 9, 57: 9, 63: 9, 70: 9, 92: 9, 93: 9, 98: 99, 99: 99}, \n",
    "\n",
    "'AGE': {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 3, 17: 3, 18: 4, 19: 5, 20: 5, 21: 6, 22: 6, 23: 6, 24: 6, 25: 6, 26: 6, 27: 6, 28: 6, 29: 6, 30: 6, 31: 6, 32: 6, 33: 6, 34: 6, 35: 6, 36: 6, 37: 6, 38: 6, 39: 6, 40: 6, 41: 6, 42: 6, 43: 6, 44: 6, 45: 6, 46: 6, 47: 6, 48: 6, 49: 6, 50: 7, 51: 7, 52: 7, 53: 7, 54: 7, 55: 7, 56: 7, 57: 7, 58: 7, 59: 7, 60: 7, 61: 7, 62: 7, 63: 7, 64: 8, 65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 9, 73: 9, 74: 9, 75: 9, 76: 9, 77: 9, 78: 9, 79: 9, 80: 9, 81: 9, 82: 9, 83: 9, 84: 9, 85: 9, 86: 9, 87: 9, 88: 9, 89: 9, 90: 9, 91: 9, 92: 9, 93: 9, 94: 9, 95: 9, 96: 9, 97: 9, 98: 9, 99: 9, 100: 9, 101: 9, 102: 9, 105: 9, 106: 9, 107: 9, 108: 9, 109: 9, 110: 9, 111: 9, 113: 9, 115: 9, 116: 9, 117: 9, 118: 9, 119: 9, 120: 9, 998: 99, 999: 99}, \n",
    "\n",
    "'AIR_BAG': {8: 0, 1: 1, 3: 2, 9: 2, 0: 3, 2: 3, 7: 3, 20: 4, 28: 4, 98: 99, 99: 99}, \n",
    "\n",
    "'ALC_STATUS': {1: 0, 2: 0, 0: 1, 8: 99, 9: 99},\n",
    "\n",
    "...\n",
    "\n",
    " 'MAKE': {50: 0, 53: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 9: 1, 14: 1, 21: 1, 22: 1, 24: 1, 43: 1, 52: 1, 64: 1, 65: 1, 92: 1, 18: 2, 37: 2, 63: 2, 35: 3, 6: 4, 36: 4, 55: 4, 13: 5, 20: 5, 34: 5, 39: 5, 69: 5, 2: 6, 12: 6, 19: 6, 25: 6, 30: 6, 41: 6, 42: 6, 47: 6, 49: 6, 58: 6, 67: 6, 7: 7, 23: 7, 54: 7, 3: 8, 10: 8, 29: 8, 31: 8, 32: 8, 38: 8, 45: 8, 48: 8, 51: 8, 59: 8, 62: 8, 90: 8, 93: 8, 1: 9, 33: 9, 46: 9, 82: 9, 84: 9, 85: 9, 86: 9, 87: 9, 89: 9, 94: 9, 97: 99, 98: 99, 99: 99}\n",
    " \n",
    "...\n",
    " \n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bea2c",
   "metadata": {},
   "source": [
    "- Write the Binning_Dict to file \"../../Big_Files/Binning_Dict.json\"\n",
    "    - Beware that in the original dictionary the keys are ints, but when you read in the .json dictionary, the keys are strings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8938e3b",
   "metadata": {},
   "source": [
    "- Use Pandas .replace() to bin the data\n",
    "- Reduce the dimensionality\n",
    "    - Drop features that are highly multicolinear with the other features\n",
    "    - Keeping these features would just gum up the works in the next notebook where we impute missing data, and turn out to not be features we will not want to use in building our models for predicting needing an ambulance.  If they were, we could have made a list of features that can be dropped.\n",
    "    - Remember that we are keeping many features that we will not use in the end to have more data for the imputation; then we will drop them.  We had thought VE_FORMS would be useful, but not if it's basically the same as VE_TOTAL. \n",
    "    - See Ambulance_Dispatch_2024_04_Reduce_Dimensionality for details on the method\n",
    "    - This method drops five features\n",
    "        - MAX_VSEV\n",
    "        - VE_FORMS\n",
    "        - VTCONT_F\n",
    "        - MAX_SEV\n",
    "        - NUM_INJV\n",
    "- Save as '../../Big_Files/CRSS_Binned_Data.csv'\n",
    "- Save a 10% sample and a n=1000 sample for future code development and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cc6ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fd6971",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9db3b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy version: 1.26.4\n",
      "SciPy version: 1.13.1\n",
      "sklearn version: 1.5.0\n",
      "Matplotlib version: 3.8.4\n",
      "Pandas version:  2.2.2\n",
      "JSON version:  2.0.9\n",
      "random_seed =  0\n",
      "Finished Importing Libraries\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import sys, copy, math, time\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from numpy import array, linspace\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version: {}'.format(sc.__version__))\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sklearn\n",
    "print ('sklearn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib\n",
    "print ('Matplotlib version: {}'.format(matplotlib.__version__))\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "import json # We will use json ('JavaScript Object Notation') to write and read dictionaries to/from files\n",
    "print ('JSON version:  {}'.format(json.__version__))\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "random_seed = 0\n",
    "print ('random_seed = ', random_seed)\n",
    "np.random.seed(random_seed) # NumPy\n",
    "random.seed(random_seed) # Python\n",
    "#tf.set_random_seed(0) # Tensorflow\n",
    "\n",
    "\n",
    "print ('Finished Importing Libraries')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f41d34",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "- Read the data file \n",
    "- Take out the NAME files and the IMputed files\n",
    "- Read in the dictionary of feature values signifying \"Missing\" or \"Unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475eed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Stuff(file_number):\n",
    "    print ('Import_Stuff()')\n",
    "    filename_dict = {\n",
    "        0: '../../Big_Files/CRSS_Merged_Raw_Data.csv',\n",
    "        1: '../../Big_Files/CRSS_Merged_Raw_Data_Sample_frac_01.csv',\n",
    "        2: '../../Big_Files/CRSS_Merged_Raw_Data_Sample_n_1000.csv'\n",
    "    }\n",
    "    filename = filename_dict[file_number]\n",
    "    print (filename)\n",
    "    data = pd.read_csv(filename, index_col=None, low_memory=False)\n",
    "    print ('data.shape: ', data.shape)\n",
    "\n",
    "    for feature in data:\n",
    "        if 'NAME' in feature or '_IM' in feature:\n",
    "            data.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print ('data.shape: ', data.shape)\n",
    "    print ()\n",
    "    \n",
    "    print ('Reading in Missing/Unknown Dictionary')\n",
    "    filename = '../../Big_Files/Missing_Unknown_Dict.json'\n",
    "    with open(filename) as json_file:\n",
    "        Missing_Unknown_Dict = json.load(json_file)\n",
    "    print ()\n",
    "\n",
    "    \n",
    "    return data, Missing_Unknown_Dict\n",
    "\n",
    "#Import_Data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842e2a6",
   "metadata": {},
   "source": [
    "## List of Ordered Features\n",
    "- Counts are ordered\n",
    "- Some time features (YEAR, VEH_AGE) are ordered.\n",
    "- Some time features (MONTH, HOUR, DAY_OF_WEEK) are ordered but periodic.\n",
    "- They aren't continuous, so we still can't use SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183260e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ordered_Unordered():\n",
    "    Ordered = [\n",
    "        'AGE',\n",
    "        'ALC_RES',\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'MONTH',\n",
    "        'NUMOCCS',\n",
    "        'NUM_INJ',\n",
    "        'NUM_INJV',\n",
    "        'PVH_INVL',\n",
    "        'VEH_AGE',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'VSPD_LIM',\n",
    "    ]\n",
    "    Unordered = [\n",
    "        'ACC_TYPE',\n",
    "        'AIR_BAG',\n",
    "        'ALC_STATUS',\n",
    "        'BODY_TYP',\n",
    "        'CARGO_BT',\n",
    "        'DEFORMED',\n",
    "        'DR_ZIP',\n",
    "        'HARM_EV',\n",
    "        'HIT_RUN',\n",
    "        'HOSPITAL',\n",
    "        'IMPACT1',\n",
    "        'INJ_SEV',\n",
    "        'INT_HWY',\n",
    "        'J_KNIFE',\n",
    "        'LGT_COND',\n",
    "        'MAKE',\n",
    "        'MAK_MOD',\n",
    "        'MAN_COLL',\n",
    "        'MAX_SEV',\n",
    "        'MAX_VSEV',\n",
    "        'MODEL',\n",
    "        'M_HARM',\n",
    "        'PCRASH4',\n",
    "        'PCRASH5',\n",
    "        'PERMVIT',\n",
    "        'PER_TYP',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'P_CRASH1',\n",
    "        'P_CRASH2',\n",
    "        'REGION',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'REL_ROAD',\n",
    "        'REST_MIS',\n",
    "        'REST_USE',\n",
    "        'ROLINLOC',\n",
    "        'ROLLOVER',\n",
    "        'SEAT_POS',\n",
    "        'SEX',\n",
    "        'SPEC_USE',\n",
    "        'SPEEDREL',\n",
    "        'TOWED',\n",
    "        'TOW_VEH',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VALIGN',\n",
    "        'VPROFILE',\n",
    "        'VSURCOND',\n",
    "        'VTCONT_F',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',        \n",
    "    ]\n",
    "    \n",
    "    return Ordered, Unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1f01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Unknowns_in_Feature(data, Missing_Unknown_Dict, feature):\n",
    "#    print ('Remove_Unknowns_in_Feature()')\n",
    "#    print (feature)\n",
    "\n",
    "    data.dropna(subset=[feature], inplace=True)\n",
    "\n",
    "    if feature in Missing_Unknown_Dict.keys():\n",
    "         data = data[~data[feature].isin(Missing_Unknown_Dict[feature])]\n",
    "#        print (data.shape)\n",
    "#        print (data[feature].unique())\n",
    "#        print ()\n",
    "#    print ()\n",
    "    return data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74bb77",
   "metadata": {},
   "source": [
    "## Bin 'HOUR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f31ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bin_HOUR():\n",
    "\n",
    "    a = \"\"\"\n",
    "# 0\n",
    "[[0], 1.3659, 24.2163]\n",
    "# 1\n",
    "[[1], 1.0729, 27.6615]\n",
    "[[2], 0.9663, 28.098]\n",
    "[[3], 0.7437, 27.4662]\n",
    "[[4], 0.7268, 26.2323]\n",
    "# 2\n",
    "[[5], 1.2066, 21.6472]\n",
    "[[6], 2.4182, 17.4277]\n",
    "# 3\n",
    "[[7], 4.7249, 13.2663]\n",
    "[[8], 4.5453, 13.5396]\n",
    "# 4\n",
    "[[9], 3.8083, 14.5844]\n",
    "[[10], 4.0647, 15.1554]\n",
    "[[11], 5.0718, 14.4391]\n",
    "[[12], 6.2467, 13.7563]\n",
    "[[13], 6.2725, 14.3218]\n",
    "[[14], 7.0865, 14.4616]\n",
    "# 5\n",
    "[[15], 8.594, 13.1969]\n",
    "[[16], 8.6698, 13.4154]\n",
    "[[17], 9.2544, 12.9285]\n",
    "[[18], 6.9822, 14.5054]\n",
    "# 6\n",
    "[[19], 4.7885, 16.7948]\n",
    "[[20], 3.8226, 18.1821]\n",
    "[[21], 3.2503, 19.1077]\n",
    "[[22], 2.49, 20.6258]\n",
    "# 0\n",
    "[[23], 1.8271, 23.1265]\n",
    "# 99 -> 99\n",
    "\"\"\"\n",
    "    HOUR_Dict = {\n",
    "        0:0, \n",
    "        1:1, 2:1, 3:1, 4:1, \n",
    "        5:2, 6:2, 7:3, 8:3, \n",
    "        9:4, 10:4, 11:4, 12:4, 13:4, 14:4, \n",
    "        15:5, 16:5, 17:5, 18:5,\n",
    "        19:6, 20:6, 21:6, 22:6, \n",
    "        23:0, \n",
    "        99:99\n",
    "    }\n",
    "    \n",
    "    return HOUR_Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd6c39",
   "metadata": {},
   "source": [
    "## Bin 'DAY_WEEK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc27845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bin_DAY_WEEK():\n",
    "    a = \"\"\"\n",
    "# 0\n",
    "[[1], 11.3057, 18.6612]\n",
    "# 1\n",
    "[[2], 13.807, 15.0023]\n",
    "# 2\n",
    "[[3], 14.3335, 14.4461]\n",
    "# 3\n",
    "[[4], 14.6499, 14.7937]\n",
    "[[5], 15.0209, 14.7084]\n",
    "# 4\n",
    "[[6], 17.0467, 14.3588]\n",
    "# 0\n",
    "[[7], 13.8363, 17.1742]\n",
    "9 -> 99\n",
    "\"\"\"\n",
    "    DAY_WEEK_Dict = {\n",
    "        1:0, \n",
    "        2:1, \n",
    "        3:2, \n",
    "        4:3, 5:3, \n",
    "        6:4, \n",
    "        7:0,\n",
    "        9:99\n",
    "    }\n",
    "    \n",
    "    return DAY_WEEK_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc407098",
   "metadata": {},
   "source": [
    "## Bin 'MONTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a6d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bin_MONTH():\n",
    "    a = \"\"\"\n",
    "# 0\n",
    "[[1], 7.399, 14.5299]\n",
    "[[2], 7.0764, 14.5805]\n",
    "[[3], 7.7018, 15.04]\n",
    "# 1\n",
    "[[4], 7.3039, 15.9165]\n",
    "[[5], 8.0828, 16.1499]\n",
    "# 2\n",
    "[[6], 8.3043, 16.8252]\n",
    "[[7], 8.4636, 17.0246]\n",
    "# 3\n",
    "[[8], 9.0931, 16.2912]\n",
    "[[9], 9.0098, 15.9137]\n",
    "# 4\n",
    "[[10], 9.8147, 14.9376]\n",
    "[[11], 8.9343, 14.4027]\n",
    "# 5\n",
    "[[12], 8.8164, 13.7601]\n",
    "    \"\"\"\n",
    "    MONTH_Dict = {\n",
    "        1:0, 2:0, 3:0, \n",
    "        4:1, 5:1, \n",
    "        6:2, 7:2, \n",
    "        8:3, 9:3, \n",
    "        10:4, 11:4, \n",
    "        12:5\n",
    "    }\n",
    "    \n",
    "    return MONTH_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e708fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bin_Feature(data, A, C, U, feature, Print):\n",
    "    if Print==1:\n",
    "        print ('Bin_Feature()')\n",
    "    \n",
    "    # C has a row for each unique value of the feature.\n",
    "    # C[0] is a list containing the value.\n",
    "    # C[1] is the percentage of the dataset for that value, and \n",
    "    # C[2] is the correlation of that value to HOSPITAL.\n",
    "    # The rows are sorted by decreasing D[1].\n",
    "    \n",
    "    if Print==1:\n",
    "        print (feature)\n",
    "        print (len(C))\n",
    "#        display (C)\n",
    "        \n",
    "    if feature in ['SPEC_USE', 'WRK_ZONE']:\n",
    "        print (len(C))\n",
    "        for c in C:\n",
    "            print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "        print ()\n",
    "\n",
    "    # Merge equal values of C[1]\n",
    "    for i in range (len(C)-1, 0, -1):\n",
    "        if C[i-1][2] == C[i][2]:\n",
    "            C[i-1][1] = C[i][1] + C[i-1][1]\n",
    "            C[i-1][0] = C[i-1][0] + C[i][0]\n",
    "            del(C[i])\n",
    "            \n",
    "#    if feature == 'MAKE':\n",
    "#        print (len(C))\n",
    "#        for c in C:\n",
    "#            print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "#        print ()\n",
    "\n",
    "#    display(C)\n",
    "            \n",
    "#    print ('Merge Small Per')\n",
    "        \n",
    "    # Merge values of C[1] less than 2% into the closest C[2]\n",
    "    E = [c[1] for c in C]\n",
    "    if len(E)>0:\n",
    "        m = min(E)\n",
    "        i = E.index(m)\n",
    "    else:\n",
    "        m = 1\n",
    "#        data.drop(columns=[feature], inplace=True)\n",
    "        print ('Drop ', feature, ' because it has devolved to just one value.')\n",
    "    \n",
    "    while m < 2.0 and len(C) > 3:        \n",
    "        # Into which row are we going to merge row i ?\n",
    "        if i==0:\n",
    "            j=1\n",
    "        elif i==len(C)-1:\n",
    "            j=len(C)-2\n",
    "        elif C[i][2] - C[i+1][2] > C[i-1][2] - C[i][2]: # We already took care of them being equal\n",
    "            j = i-1\n",
    "        else:\n",
    "            j = i+1\n",
    "        \n",
    "#        print (i, C[i])\n",
    "#        print (j, C[j])\n",
    "        C[j][2] = (C[i][1]*C[i][2] + C[j][1]*C[j][2])/(C[i][1] + C[j][1])\n",
    "        C[j][1] = C[i][1] + C[j][1]\n",
    "        C[j][0] = C[i][0] + C[j][0]\n",
    "#        print (C[j])\n",
    "        del(C[i])\n",
    "#        print (len(C))\n",
    "#        print ()\n",
    "        \n",
    "        E = [c[1] for c in C]\n",
    "        m = min(E)\n",
    "        i = E.index(m)\n",
    "\n",
    "        if feature in ['SPEC_USE', 'WRK_ZONE']:\n",
    "            print (len(C))\n",
    "            for c in C:\n",
    "                print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "            print ()\n",
    "\n",
    "\n",
    "#    if feature == 'MAKE':\n",
    "#        print (len(C))\n",
    "#        for c in C:\n",
    "#            print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "#        print ()\n",
    "\n",
    "#    print ()\n",
    "#    print (len(C))\n",
    "#    display(C)\n",
    "#    print ('Merge Gaps')\n",
    "    \n",
    "    # Merge gaps less than 0.1%\n",
    "    E = [abs(C[i][2] - C[i+1][2]) for i in range (0,len(C)-1)]\n",
    "    if len(E) > 0:\n",
    "        m = min(E)\n",
    "        i = E.index(m)\n",
    "    else:\n",
    "        m = 1\n",
    "        data.drop(columns=[feature], inplace=True)\n",
    "        print ('Drop ', feature, ' because it has devolved into just one value.')\n",
    "    \n",
    "#    while m < 0.1:        \n",
    "#    while len(C) > 10:        \n",
    "    while m < 0.1 or len(C) > 10:\n",
    "        j = i+1\n",
    "#        print (i, C[i])\n",
    "#        print (j, C[j])\n",
    "        C[j][2] = (C[i][1]*C[i][2] + C[j][1]*C[j][2])/(C[i][1] + C[j][1])\n",
    "        C[j][1] = C[i][1] + C[j][1]\n",
    "        C[j][0] = C[i][0] + C[j][0]\n",
    "#        print (C[j])\n",
    "        del(C[i])\n",
    "#        print (len(C))\n",
    "#        print ()\n",
    "        \n",
    "        E = [abs(C[i][2] - C[i+1][2]) for i in range (0,len(C)-1)]\n",
    "        m = min(E)\n",
    "        i = E.index(m)\n",
    "\n",
    "#        if feature == 'MAKE':\n",
    "#            print (len(C))\n",
    "#            for c in C:\n",
    "#                print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "#            print ()\n",
    "\n",
    "\n",
    "#    print ()\n",
    "#    print (len(C))\n",
    "#    display(C)\n",
    "\n",
    "    for c in C:\n",
    "        c[0].sort()\n",
    "        \n",
    "#    if feature == 'MAKE':\n",
    "#        print (len(C))\n",
    "#        for c in C:\n",
    "#            print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "#        print ()\n",
    "\n",
    "        \n",
    "    for i in range (len(C)-1):\n",
    "        C[i].append(abs(C[i][2] - C[i+1][2]))\n",
    "        C[i].append(i)\n",
    "    \n",
    "#    print (len(C))\n",
    "#    display ([[c[1],c[2], len(c[0]), sorted(c[0])] for c in C])\n",
    "        \n",
    "    Feature_Binning_Dict = {}\n",
    "    for i in range (len(C)):\n",
    "        for c in C[i][0]:\n",
    "            Feature_Binning_Dict[int(c)] = int(i)\n",
    "            \n",
    "#    if feature=='MAKE':\n",
    "#        print (Feature_Binning_Dict)\n",
    "            \n",
    "        \n",
    "    return Feature_Binning_Dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511c11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binning(data, target, Missing_Unknown_Dict, Ordered, Unordered, Print):\n",
    "    \n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n",
    "    print (\"Binning\")\n",
    "    \n",
    "    Binning_Dict = {}\n",
    "    \n",
    "    for feature in data:\n",
    "#    for feature in ['ACC_TYPE']:\n",
    "        print (feature, len(data[feature].unique()))\n",
    "#        display (data[feature].value_counts())\n",
    "        if feature != target:\n",
    "            A = Remove_Unknowns_in_Feature(data[[target, feature]], Missing_Unknown_Dict, feature)\n",
    "            U = sorted(A[feature].unique())\n",
    "            C = []\n",
    "            for u in U:\n",
    "                B = A[A[feature]==u]\n",
    "                TP = B[target].sum()\n",
    "                PP = len(B)\n",
    "                corr = TP/PP*100\n",
    "                per = len(B)/len(A)*100\n",
    "\n",
    "                C.append([[u], per, corr])\n",
    "\n",
    "            if feature in Unordered:\n",
    "                if Print==1:\n",
    "                    print ('Sort Unordered Feature')\n",
    "                C.sort(key=lambda x:x[2], reverse=True)\n",
    "\n",
    "            if feature == 'SPEC_USE':\n",
    "                print (feature)\n",
    "                print (len(C))\n",
    "                for c in C:\n",
    "                    print (\" | \", c[0], \" | \", round(c[1],4), \" | \", round(c[2],4), \" | \")\n",
    "                print ()\n",
    "\n",
    "            \"\"\"\n",
    "            data[feature] is a Pandas series of the values of the feature for each the 800,000+ samples\n",
    "            A is the a Pandas dataframe with two columns:\n",
    "                    HOSPITAL (the target feature)\n",
    "                    feature (the current feature)\n",
    "                with the samples that have \"Missing\" and \"Unknown\" values for the current feature removed\n",
    "            U is a list of the unique values in A\n",
    "            for each unique value u in U:\n",
    "                B is the Pandas series A filtered to just have the samples with that unique value u\n",
    "            C is a list, for each unique value u in U, of:\n",
    "                u, the unique value, \n",
    "                per, the percentage of the samples in list A that has that value\n",
    "                corr (correlation), the percent of the samples that have this value that are hospitalized\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            Feature_Binning_Dict = Bin_Feature(data, A, C, U, feature, Print)\n",
    "                \n",
    "            if Print==1:\n",
    "                print (feature, len(U))\n",
    "#                print (Feature_Binning_Dict)\n",
    "                print (Missing_Unknown_Dict[feature])\n",
    "#                print ()\n",
    "\n",
    "            # Put the missing and unknown values back in as 99.\n",
    "            for mu in Missing_Unknown_Dict[feature]:\n",
    "                Feature_Binning_Dict[mu] = 99\n",
    "\n",
    "            if feature == 'HOUR':\n",
    "                Feature_Binning_Dict = Bin_HOUR()\n",
    "            if feature == 'DAY_WEEK':\n",
    "                Feature_Binning_Dict = Bin_DAY_WEEK()\n",
    "            if feature == 'MONTH':\n",
    "                Feature_Binning_Dict = Bin_MONTH()\n",
    "\n",
    "            Binning_Dict[feature] = Feature_Binning_Dict\n",
    "\n",
    "            if Print==1 or feature == 'SPEC_USE':\n",
    "                print (list(set(list(Feature_Binning_Dict.values()))))\n",
    "                print ()\n",
    "                print(Feature_Binning_Dict)\n",
    "                    \n",
    "\n",
    "            \n",
    "    return Binning_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e9a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_List_of_Features(data, Missing_Unknown_Dict, target):\n",
    "    print ('Features and number of unique values in feature')\n",
    "    \n",
    "    B = []\n",
    "    for feature in data:\n",
    "        if feature != target:\n",
    "            A = Remove_Unknowns_in_Feature(data[[target, feature]], Missing_Unknown_Dict, feature)\n",
    "            U = sorted(A[feature].unique())\n",
    "            B.append([feature, len(U)])\n",
    "    B.sort(key = lambda x:x[0])\n",
    "    B.sort(key = lambda x:x[1])\n",
    "    for b in B:\n",
    "        print ('    %s, # %d' % (b[0], b[1]))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a3982",
   "metadata": {},
   "source": [
    "## Reduce Dimensionality by Multicolinearlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1802db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f\n",
    "def calculate_vif(df, features):    \n",
    "    r2_Dict, tolerance, vif = {}, {}, {}\n",
    "    # all the features that you want to examine\n",
    "    for feature in features:\n",
    "        # extract all the other features you will regress against\n",
    "        X = [f for f in features if f != feature]        \n",
    "        X, y = df[X], df[feature]\n",
    "        # extract r-squared from the fit\n",
    "        r2 = LinearRegression().fit(X, y).score(X, y)\n",
    "        r2_Dict[feature] = r2\n",
    "        # calculate tolerance\n",
    "        tolerance[feature] = 1 - r2\n",
    "        # calculate VIF\n",
    "        if tolerance[feature] !=0:\n",
    "            vif[feature] = 1/(tolerance[feature])\n",
    "        else:\n",
    "            vif[feature] = 10000\n",
    "\n",
    "    return pd.DataFrame({'r2': r2_Dict, 'Tolerance': tolerance, 'VIF': vif}), tolerance, r2_Dict\n",
    "\n",
    "# Iteratively remove the feature with the largest VIF ('Variance Inflaction Factor')\n",
    "# until the largest VIF is 10, or smallest Tolerance is 0.1, or largest R^2 is 0.9\n",
    "def Reduce_Dimensionality(data, target):\n",
    "    print ()\n",
    "    print ('Reduce_Dimensionality()')\n",
    "    print ('data.shape: ', data.shape)\n",
    "    Target = data.pop(target)\n",
    "    Features = [feature for feature in data]\n",
    "    VIF, Tolerance_Dict, r2_Dict = calculate_vif(data, Features)\n",
    "    Max_r2_Feature = VIF['r2'].idxmax()\n",
    "    display(VIF)\n",
    "    print (Max_r2_Feature)\n",
    "    if r2_Dict[Max_r2_Feature] > 0.9:\n",
    "        data.drop(columns = [Max_r2_Feature], inplace=True)\n",
    "        print ('Drop ', Max_r2_Feature)\n",
    "    print ()\n",
    "    while r2_Dict[Max_r2_Feature] > 0.9:\n",
    "        Features = [feature for feature in data]\n",
    "        VIF, Tolerance_Dict, r2_Dict = calculate_vif(data, Features)\n",
    "        Max_r2_Feature = VIF['r2'].idxmax()\n",
    "        display(VIF)\n",
    "        print (Max_r2_Feature)\n",
    "        if r2_Dict[Max_r2_Feature] > 0.9:\n",
    "            data.drop(columns = [Max_r2_Feature], inplace=True)\n",
    "            print ('Drop ', Max_r2_Feature)\n",
    "        print ()\n",
    "\n",
    "    data = data.join(Target)\n",
    "    print ('data.shape: ', data.shape)\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483837df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fec66590",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import_Stuff()\n",
      "../../Big_Files/CRSS_Merged_Raw_Data.csv\n",
      "data.shape:  (817623, 86)\n",
      "data.shape:  (817623, 67)\n",
      "\n",
      "Reading in Missing/Unknown Dictionary\n",
      "\n",
      "Binning\n",
      "HOSPITAL 2\n",
      "ACC_TYPE 93\n",
      "AGE 119\n",
      "AIR_BAG 11\n",
      "ALC_STATUS 5\n",
      "BODY_TYP 73\n",
      "CARGO_BT 18\n",
      "DAY_WEEK 7\n",
      "DEFORMED 7\n",
      "DR_ZIP 19297\n",
      "EJECTION 7\n",
      "HARM_EV 53\n",
      "HIT_RUN 3\n",
      "HOUR 25\n",
      "IMPACT1 26\n",
      "INJ_SEV 8\n",
      "INT_HWY 3\n",
      "J_KNIFE 4\n",
      "LGT_COND 9\n",
      "MAKE 71\n",
      "MAK_MOD 1219\n",
      "MAN_COLL 11\n",
      "MAX_SEV 8\n",
      "MAX_VSEV 8\n",
      "MODEL 141\n",
      "MONTH 12\n",
      "M_HARM 53\n",
      "NUMOCCS 65\n",
      "NUM_INJ 19\n",
      "NUM_INJV 17\n",
      "PCRASH4 8\n",
      "PCRASH5 9\n",
      "PERMVIT 25\n",
      "PER_TYP 3\n",
      "PJ 425\n",
      "PSU 60\n",
      "PVH_INVL 12\n",
      "P_CRASH1 20\n",
      "P_CRASH2 57\n",
      "REGION 4\n",
      "RELJCT1 4\n",
      "RELJCT2 15\n",
      "REL_ROAD 13\n",
      "REST_MIS 3\n",
      "REST_USE 20\n",
      "ROLINLOC 10\n",
      "ROLLOVER 6\n",
      "SEAT_POS 29\n",
      "SEX 5\n",
      "SPEC_USE 20\n",
      "SPEC_USE\n",
      "18\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [1]  |  0.159  |  25.2147  | \n",
      " |  [4]  |  0.0026  |  23.8095  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [20]  |  0.0247  |  19.598  | \n",
      " |  [10]  |  0.002  |  18.75  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [22]  |  0.0122  |  9.1837  | \n",
      " |  [13]  |  0.0014  |  9.0909  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [23]  |  0.0081  |  1.5385  | \n",
      " |  [11]  |  0.0007  |  0.0  | \n",
      " |  [12]  |  0.0022  |  0.0  | \n",
      "\n",
      "18\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [1]  |  0.159  |  25.2147  | \n",
      " |  [4]  |  0.0026  |  23.8095  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [20]  |  0.0247  |  19.598  | \n",
      " |  [10]  |  0.002  |  18.75  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [22]  |  0.0122  |  9.1837  | \n",
      " |  [13]  |  0.0014  |  9.0909  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [23]  |  0.0081  |  1.5385  | \n",
      " |  [11]  |  0.0007  |  0.0  | \n",
      " |  [12]  |  0.0022  |  0.0  | \n",
      "\n",
      "16\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [1]  |  0.159  |  25.2147  | \n",
      " |  [4]  |  0.0026  |  23.8095  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [20]  |  0.0247  |  19.598  | \n",
      " |  [10]  |  0.002  |  18.75  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [23]  |  0.0081  |  1.5385  | \n",
      " |  [11, 12]  |  0.003  |  0.0  | \n",
      "\n",
      "15\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [1]  |  0.159  |  25.2147  | \n",
      " |  [4]  |  0.0026  |  23.8095  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [23]  |  0.0081  |  1.5385  | \n",
      " |  [11, 12]  |  0.003  |  0.0  | \n",
      "\n",
      "14\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [4, 1]  |  0.1616  |  25.192  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [23]  |  0.0081  |  1.5385  | \n",
      " |  [11, 12]  |  0.003  |  0.0  | \n",
      "\n",
      "13\n",
      " |  [19]  |  0.0038  |  48.3871  | \n",
      " |  [4, 1]  |  0.1616  |  25.192  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [11, 12, 23]  |  0.011  |  1.1236  | \n",
      "\n",
      "12\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8]  |  0.0073  |  16.9492  | \n",
      " |  [3]  |  0.2186  |  15.7865  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [11, 12, 23]  |  0.011  |  1.1236  | \n",
      "\n",
      "11\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [7]  |  0.0726  |  2.906  | \n",
      " |  [11, 12, 23]  |  0.011  |  1.1236  | \n",
      "\n",
      "10\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22]  |  0.0135  |  9.1743  | \n",
      " |  [2]  |  0.228  |  8.4377  | \n",
      " |  [11, 12, 23, 7]  |  0.0837  |  2.6706  | \n",
      "\n",
      "9\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [0]  |  98.8412  |  15.6171  | \n",
      " |  [21]  |  0.0262  |  14.6919  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22, 2]  |  0.2415  |  8.4789  | \n",
      " |  [11, 12, 23, 7]  |  0.0837  |  2.6706  | \n",
      "\n",
      "8\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [5]  |  0.2904  |  20.1709  | \n",
      " |  [10, 20]  |  0.0267  |  19.5349  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [21, 0]  |  98.8674  |  15.6169  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22, 2]  |  0.2415  |  8.4789  | \n",
      " |  [11, 12, 23, 7]  |  0.0837  |  2.6706  | \n",
      "\n",
      "7\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [10, 20, 5]  |  0.3171  |  20.1174  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [21, 0]  |  98.8674  |  15.6169  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [13, 22, 2]  |  0.2415  |  8.4789  | \n",
      " |  [11, 12, 23, 7]  |  0.0837  |  2.6706  | \n",
      "\n",
      "6\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [10, 20, 5]  |  0.3171  |  20.1174  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [21, 0]  |  98.8674  |  15.6169  | \n",
      " |  [6]  |  0.0989  |  10.2886  | \n",
      " |  [11, 12, 23, 7, 13, 22, 2]  |  0.3252  |  6.9847  | \n",
      "\n",
      "5\n",
      " |  [19, 4, 1]  |  0.1655  |  25.7314  | \n",
      " |  [10, 20, 5]  |  0.3171  |  20.1174  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [21, 0]  |  98.8674  |  15.6169  | \n",
      " |  [6, 11, 12, 23, 7, 13, 22, 2]  |  0.4241  |  7.7553  | \n",
      "\n",
      "4\n",
      " |  [19, 4, 1, 10, 20, 5]  |  0.4826  |  22.0422  | \n",
      " |  [8, 3]  |  0.2259  |  15.8242  | \n",
      " |  [21, 0]  |  98.8674  |  15.6169  | \n",
      " |  [6, 11, 12, 23, 7, 13, 22, 2]  |  0.4241  |  7.7553  | \n",
      "\n",
      "3\n",
      " |  [19, 4, 1, 10, 20, 5]  |  0.4826  |  22.0422  | \n",
      " |  [8, 3, 21, 0]  |  99.0933  |  15.6174  | \n",
      " |  [6, 11, 12, 23, 7, 13, 22, 2]  |  0.4241  |  7.7553  | \n",
      "\n",
      "[0, 1, 2, 99]\n",
      "\n",
      "{1: 0, 4: 0, 5: 0, 10: 0, 19: 0, 20: 0, 0: 1, 3: 1, 8: 1, 21: 1, 2: 2, 6: 2, 7: 2, 11: 2, 12: 2, 13: 2, 22: 2, 23: 2, 98: 99, 99: 99}\n",
      "SPEEDREL 7\n",
      "TOWED 7\n",
      "TOW_VEH 9\n",
      "TYP_INT 11\n",
      "URBANICITY 2\n",
      "VALIGN 7\n",
      "VEH_AGE 88\n",
      "VE_FORMS 14\n",
      "VE_TOTAL 14\n",
      "VPROFILE 9\n",
      "VSPD_LIM 20\n",
      "VSURCOND 13\n",
      "VTCONT_F 7\n",
      "VTRAFCON 19\n",
      "VTRAFWAY 10\n",
      "WEATHER 13\n",
      "WRK_ZONE 5\n",
      "5\n",
      " |  [3]  |  0.024  |  19.3878  | \n",
      " |  [0]  |  98.0438  |  15.5123  | \n",
      " |  [2]  |  0.1082  |  14.3503  | \n",
      " |  [1]  |  0.9893  |  12.6715  | \n",
      " |  [4]  |  0.8346  |  11.738  | \n",
      "\n",
      "4\n",
      " |  [3, 0]  |  98.0678  |  15.5132  | \n",
      " |  [2]  |  0.1082  |  14.3503  | \n",
      " |  [1]  |  0.9893  |  12.6715  | \n",
      " |  [4]  |  0.8346  |  11.738  | \n",
      "\n",
      "3\n",
      " |  [2, 3, 0]  |  98.1761  |  15.512  | \n",
      " |  [1]  |  0.9893  |  12.6715  | \n",
      " |  [4]  |  0.8346  |  11.738  | \n",
      "\n",
      "Reading in Binning Dict\n",
      "HOSPITAL\n",
      "HOSPITAL  not in Binning_Dict\n",
      "\n",
      "ACC_TYPE\n",
      "93  unique features in original data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "AGE\n",
      "119  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "AIR_BAG\n",
      "11  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "ALC_STATUS\n",
      "5  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "BODY_TYP\n",
      "73  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "CARGO_BT\n",
      "18  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "DAY_WEEK\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 3, 4]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "DEFORMED\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "DR_ZIP\n",
      "19297  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "EJECTION\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "HARM_EV\n",
      "53  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "HIT_RUN\n",
      "3  unique features in original data\n",
      "[0, 1, 99]  values in binned feature\n",
      "[0, 1, 99]  values in dictionary\n",
      "\n",
      "HOUR\n",
      "25  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 99]  values in dictionary\n",
      "\n",
      "IMPACT1\n",
      "26  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "INJ_SEV\n",
      "8  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "INT_HWY\n",
      "3  unique features in original data\n",
      "[0, 1, 99]  values in binned feature\n",
      "[0, 1, 99]  values in dictionary\n",
      "\n",
      "J_KNIFE\n",
      "4  unique features in original data\n",
      "[0, 1, 2]  values in binned feature\n",
      "[0, 1, 2]  values in dictionary\n",
      "\n",
      "LGT_COND\n",
      "9  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "MAKE\n",
      "71  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "MAK_MOD\n",
      "1219  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "MAN_COLL\n",
      "11  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "MAX_SEV\n",
      "8  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "MAX_VSEV\n",
      "8  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "MODEL\n",
      "141  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "MONTH\n",
      "12  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5]  values in dictionary\n",
      "\n",
      "M_HARM\n",
      "53  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "NUMOCCS\n",
      "65  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "NUM_INJ\n",
      "19  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 99]  values in dictionary\n",
      "\n",
      "NUM_INJV\n",
      "17  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "PCRASH4\n",
      "8  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "PCRASH5\n",
      "9  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "PERMVIT\n",
      "25  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5]  values in dictionary\n",
      "\n",
      "PER_TYP\n",
      "3  unique features in original data\n",
      "[0, 1, 99]  values in binned feature\n",
      "[0, 1, 99]  values in dictionary\n",
      "\n",
      "PJ\n",
      "425  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  values in dictionary\n",
      "\n",
      "PSU\n",
      "60  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  values in dictionary\n",
      "\n",
      "PVH_INVL\n",
      "12  unique features in original data\n",
      "[0, 1, 2]  values in binned feature\n",
      "[0, 1, 2]  values in dictionary\n",
      "\n",
      "P_CRASH1\n",
      "20  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 99]  values in dictionary\n",
      "\n",
      "P_CRASH2\n",
      "57  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "REGION\n",
      "4  unique features in original data\n",
      "[0, 1, 2, 3]  values in binned feature\n",
      "[0, 1, 2, 3]  values in dictionary\n",
      "\n",
      "RELJCT1\n",
      "4  unique features in original data\n",
      "[0, 1, 99]  values in binned feature\n",
      "[0, 1, 99]  values in dictionary\n",
      "\n",
      "RELJCT2\n",
      "15  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "REL_ROAD\n",
      "13  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "REST_MIS\n",
      "3  unique features in original data\n",
      "[0, 1, 2]  values in binned feature\n",
      "[0, 1, 2]  values in dictionary\n",
      "\n",
      "REST_USE\n",
      "20  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "ROLINLOC\n",
      "10  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "ROLLOVER\n",
      "6  unique features in original data\n",
      "[0, 1, 2]  values in binned feature\n",
      "[0, 1, 2]  values in dictionary\n",
      "\n",
      "SEAT_POS\n",
      "29  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "SEX\n",
      "5  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "SPEC_USE\n",
      "20  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "SPEEDREL\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "TOWED\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "TOW_VEH\n",
      "9  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "TYP_INT\n",
      "11  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "URBANICITY\n",
      "2  unique features in original data\n",
      "[0, 1]  values in binned feature\n",
      "[0, 1]  values in dictionary\n",
      "\n",
      "VALIGN\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 3, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 99]  values in dictionary\n",
      "\n",
      "VEH_AGE\n",
      "88  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "VE_FORMS\n",
      "14  unique features in original data\n",
      "[0, 1, 2, 3]  values in binned feature\n",
      "[0, 1, 2, 3]  values in dictionary\n",
      "\n",
      "VE_TOTAL\n",
      "14  unique features in original data\n",
      "[0, 1, 2, 3]  values in binned feature\n",
      "[0, 1, 2, 3]  values in dictionary\n",
      "\n",
      "VPROFILE\n",
      "9  unique features in original data\n",
      "[0, 1, 2, 3, 4, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 99]  values in dictionary\n",
      "\n",
      "VSPD_LIM\n",
      "20  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 99]  values in dictionary\n",
      "\n",
      "VSURCOND\n",
      "13  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "VTCONT_F\n",
      "7  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "VTRAFCON\n",
      "19  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "VTRAFWAY\n",
      "10  unique features in original data\n",
      "[0, 1, 2, 3, 4, 5, 99]  values in binned feature\n",
      "[0, 1, 2, 3, 4, 5, 99]  values in dictionary\n",
      "\n",
      "WEATHER\n",
      "13  unique features in original data\n",
      "[0, 1, 2, 99]  values in binned feature\n",
      "[0, 1, 2, 99]  values in dictionary\n",
      "\n",
      "WRK_ZONE\n",
      "5  unique features in original data\n",
      "[0, 1, 2]  values in binned feature\n",
      "[0, 1, 2]  values in dictionary\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop rows with more than 20% of features missing\n",
      "(817623, 67)\n",
      "(802700, 67)\n",
      "data.shape:  (802700, 67)\n",
      "Finished Binning Data\n",
      "CPU times: user 34.1 s, sys: 5.65 s, total: 39.8 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Main():\n",
    "    target = 'HOSPITAL'\n",
    "    data, Missing_Unknown_Dict = Import_Stuff(0)\n",
    "    data = data.reindex(sorted(data.columns), axis=1)\n",
    "    data = data[ [target] + [col for col in data.columns if col != target]]\n",
    "    \n",
    "    Ordered, Unordered = Ordered_Unordered()\n",
    "    \n",
    "#    print ('Missing_Unknown_Dict')\n",
    "#    print (Missing_Unknown_Dict)\n",
    "#    print ()\n",
    "    \n",
    "#    Make_List_of_Features(data, Missing_Unknown_Dict, target)\n",
    "    \n",
    "    Print = 0 # Change to Print=1 for Verbose\n",
    "    Binning_Dict = Binning(data, target, Missing_Unknown_Dict, Ordered, Unordered, Print)\n",
    "\n",
    "#    for key in Binning_Dict.keys():\n",
    "#        print (key)\n",
    "#        for item in Binning_Dict[key]:\n",
    "#            print (item, type(item))\n",
    "#    print ()\n",
    "\n",
    "    with open(\"../../Big_Files/Binning_Dict.json\", \"w\") as outfile: \n",
    "        json.dump(Binning_Dict, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    print ('Reading in Binning Dict')\n",
    "    with open('../../Big_Files/Binning_Dict.json') as json_file:\n",
    "        D = json.load(json_file)\n",
    "    json_file.close()\n",
    "    \n",
    "    for feature in data:\n",
    "        print (feature)\n",
    "        if feature in Binning_Dict.keys():\n",
    "            nU = len(data[feature].unique())\n",
    "            print (nU, ' unique features in original data')\n",
    "            data[feature].replace(Binning_Dict[feature], inplace=True)\n",
    "            U = sorted(list(data[feature].unique()))\n",
    "            V = list(set(Binning_Dict[feature].values()))\n",
    "            print (U, ' values in binned feature')\n",
    "            print (V, ' values in dictionary')\n",
    "#            print (Binning_Dict[feature])\n",
    "        else:\n",
    "            print (feature, ' not in Binning_Dict')\n",
    "        print ()\n",
    "        \n",
    "    for feature in data:\n",
    "        data[feature] = pd.to_numeric(data[feature])\n",
    "    data = data.astype('int64')\n",
    "    \n",
    "    print ('Drop rows with more than 20% of features missing')\n",
    "    print (data.shape)\n",
    "    data['new'] = data.isin({99}).sum(1)\n",
    "    \n",
    "    data.drop(data[data['new'] > 0.2 * data.shape[1]].index, inplace=True)\n",
    "    data.drop(columns=['new'], inplace=True)\n",
    "    print (data.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    data.to_csv('../../Big_Files/CRSS_Binned_Data.csv', index=False)\n",
    "#    data.to_csv('../../Big_Files/CRSS_Binned_Data_Seed_0.csv', index=False)\n",
    "#    data.to_csv('../../Big_Files/CRSS_Binned_Data_Seed_42.csv', index=False)\n",
    "\n",
    "    # Make a sample of the dataset for testing while writing code\n",
    "    data.sample(frac=0.1).to_csv('../../Big_Files/CRSS_Binned_Data_Sample_frac_01.csv', index=False)\n",
    "\n",
    "    # Make a really small sample of the dataset for testing while writing code\n",
    "    data.sample(n=1000).to_csv('../../Big_Files/CRSS_Binned_Data_Sample_n_1000.csv', index=False)\n",
    "\n",
    "    \"\"\"\n",
    "    data = Reduce_Dimensionality(data, target)\n",
    "    print ('data.shape: ', data.shape)\n",
    "        \n",
    "    data.to_csv('../../Big_Files/CRSS_Binned_Reduced_Dimensionality_Data.csv', index=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('Finished Binning Data')\n",
    "        \n",
    "Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9ec78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
