{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edb2407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c5248",
   "metadata": {},
   "source": [
    "# Ambulance_Dispatch_2024_03_Impute_Missing_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa289b",
   "metadata": {},
   "source": [
    "# readme\n",
    "- Most of our other Jupyter Notebooks have a main() function at the bottom that runs everything.  \n",
    "- This notebook is structured differently, with several functions that run in sequence.  \n",
    "- The reason for the difference is that part of the work has to be done outside this notebook.  \n",
    "- The IVEware imputation software is available in several languages, but not Python.  We ran it in R using scrlib.  \n",
    "- This notebook prepares the data for the Mode, Random Forest, and IVEware imputations, and does the first two.  Then the user must separately run the IVEware software.  Finally, this notebook pulls in those results and compares the three methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b7cd9",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484970b",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- We have about 3% of the values in the dataset missing.  \n",
    "- CRSS used IVEware to impute missing values in some, but not most, of the features.  \n",
    "- We can use IVEware to impute the rest of the features, but we should compare to other methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433aa44",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- We have the discretized CRSS dataset in '../../Big_Files/CRSS_Binned_Data.csv'.\n",
    "- The dataset is 802,700 samples with 67 features.\n",
    "- In that dataset, each feature has values in {0,1,2,3,4,5,6,7,8,9,99}, with most features having fewer values and 99 signifying \"Missing\" or \"Unknown.\"\n",
    "- Overall, about 3% of the values are 99, \n",
    "    - In the features, thirteen features have no missing values, and six features have more than ten percent missing, the highest being RELJCT1 with 18% missing.  \n",
    "    - In the rows, 29% have no missing values, 25% one missing value, 16% two, ..., 1% eight, ..., and 0.3% thirteen missing values.  \n",
    "    - See results of the Analyze_Data() function for full details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eada507",
   "metadata": {},
   "source": [
    "## Imputation Methods\n",
    "### MissForest\n",
    "- MissForest is a round-robin imputation method most commonly implemented in R, generally considered one of the best imputation methods.  It has several Python implementations. The Python implementation we found most current and referenced is https://pypi.org/project/MissForest/, and that's the one we used, version 2.5.5.  There are other current implementations that we did not try.  \n",
    "- MissForest doesn't work with some datasets, because it imputes each feature by first separating the samples into those with a missing value in that feature and those with a known value, build a model with the set with known values, and applying the model to the set with unknown values.  The problem comes when it builds the model, that it drops all samples with a missing value in any feature. If all samples have at least two missing values, then the learning algorithm gets an empty dataframe on which to build the model, and it (rightly) throws an error.  Our original test for comparing imputation methods took out 15% of the values in each feature, which meant that each model-building round, if it had any clean data, had very little, so the models were poor, giving MissForest poor results.  \n",
    "- MissForest takes an enormous amount of memory, sometimes enough to crash the process on the computer we were using.  Imputing the ground-truth set we will use to compare imputation methods, 232,333 samples with 67 features, it used 40+ GB of memory and crashed in the sixth iteration.  Imputing the full set on which we want to use the best imputation method, 802,700 samples with 67 features, it crashed in the second iteration.\n",
    "- Interesting bug (feature?) in the code:  It starts counting iterations at zero and stops at the end of an iteration `` if n_iter > self.max_iter ``, so it will go through one more iteration than ``max_iter``.\n",
    "- We couldn't find any detailed documentation for MissForest, and ended up reading the code itself.  \n",
    "    - It does use mode imputation for categorical features (line 192 of missforest.py, version 2.5.5)\n",
    "- We added one line to the transform() function in missforest.py, a print statement to let us know which interation it was on and which function it was imputing.  \n",
    "````{verbtim}\n",
    "```456        while True:\n",
    "```457            for c in missing_rows:\n",
    "```457                print ('Iteration ', n_iter, ' of ', self.max_iter,  ', feature ', c)\n",
    "````\n",
    "\n",
    "\n",
    "- \n",
    "- We compare here four methods:\n",
    "    - Round-Robin Random Forest \n",
    "        - Our own implementation of Round Robin, using scikit-learn's random forest\n",
    "        - Using imputation by mode as the starting point\n",
    "    - Imputation by mode\n",
    "    - Random Imputation\n",
    "    - IVEware, using the hyperparameters in the CRSS Imputation report\n",
    "- To compare, we followed the example for MissForest.\n",
    "    - We dropped all samples with a missing value, so we would have ground truth, going from 817,623 samples to 232,333 samples to make a Pandas dataframe data_Ground_Truth\n",
    "    - We erased ~15% of the values in each sample to make data_NaN\n",
    "    - We used each imputation method to impute the missing values.\n",
    "    - To compare methods, we counted:\n",
    "        - For each method, what percentage of imputed values did not match ground truth (28-44%)\n",
    "        - For each pair of methods, which method did a better job on how many features\n",
    "        - For each pair of methods, how many values are different\n",
    "- Our round-robin method\n",
    "    - In data_NaN, change all of the 'Unknown' to np.NaN.\n",
    "    - In each feature, count the number of unknown samples.\n",
    "    - In another copy, data_Mode, impute by mode in all of the features.\n",
    "    - Starting with the feature with the least (nonzero) number of missing samples:\n",
    "        - Copy that feature from data_NaN into data_Mode, so that only that feature has missing values.\n",
    "        - Separate the dataframe into two, one with known values in the target variable (X) and one with unknown values (Z).\n",
    "        - From the dataframe with known values (X), separate out the target variable (call it 'y')\n",
    "        - Using Random Forest, build a model that maps X to y.  \n",
    "        - Use the model to impute the missing values\n",
    "    - At each iteration we replace the mode-imputed values with RF-imputed values.\n",
    "- Our Random Imputation method\n",
    "    - We did not choose randomly from the unique values in the feature, because some values may be much more common than others.  We wanted (approximately) the same distribution of values.\n",
    "    - We started with 232,333 samples with 67 features.\n",
    "    - We erased values with a probability of 15%, but that doesn't mean that exactly 34,849.95 values are missing from each feature, but we did erase *about* 35,000 values from each feature.  The exact number erased from each feature is printed out when the code runs.\n",
    "    - For each feature:\n",
    "        - Create a temporary copy of the feature, which will have 232,333 samples, about 35,000 of which are NaN.\n",
    "        - Drop the NaN samples in the temp feature, leaving about 200,000 samples.\n",
    "        - Resample the temp feature to have 232,333 samples.  The resampling will change the order of the values but keep about the same distribution.\n",
    "        - In the original feature, replace the NaN values with the non-NaN corresponding values in the temporary feature.\n",
    "- The IVEware implementation is available in several platforms, but Python is not one of them.  We run it in R outside this notebook.  Be aware that the random selection of values to erase is different for each run, so the IVEware imputation must be run anew. \n",
    "\n",
    "- Once we had analyzed the results and decided that the Random Forest method is best for our work, we implemented it and saved the results to CRSS_Imputed_Data.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ee00c",
   "metadata": {},
   "source": [
    "## What is going on with IVEware using \"seed 0;\" ?\n",
    "- When we set the random seed to 0, the accuracy of IVEware jumps from about 70% to about 80%, from slightly worse than Random Forest to MUCH better.  WHAT ???\n",
    "\n",
    "- These runs have the same random seed for Python and NumPy, have the five multicollinear features used in the imputation but dropped for the evaluation.  \n",
    "\n",
    "- Having the same Python and NumPy random seed means that the input datasets for the IVEware imputation have the same samples have the same missing feature values.  \n",
    "\n",
    "- \"seed 0;\" in IVEware_CRSS_Imputation.xml\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,006,143  | 100% | \n",
    "    | RF |  558,626  |  27.85 % | \n",
    "    | Mode |  681,514  |  33.97 % | \n",
    "    | Random |  888,663  |  44.3 % | \n",
    "    | IVEware |  438,072  |  21.84 % | \n",
    "\n",
    "- \"seed 1;\"\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,006,143  | 100% | \n",
    "    | RF |  558,626  |  27.85 % | \n",
    "    | Mode |  681,514  |  33.97 % | \n",
    "    | Random |  888,663  |  44.3 % | \n",
    "    | IVEware |  592,313  |  29.52 % | \n",
    "\n",
    "- \"seed 2;\"\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,006,143  | 100% | \n",
    "    | RF |  558,626  |  27.85 % | \n",
    "    | Mode |  681,514  |  33.97 % | \n",
    "    | Random |  888,663  |  44.3 % | \n",
    "    | IVEware |  568,719  |  28.35 % | \n",
    "    \n",
    "    \n",
    "<br><br>\n",
    "- Found what was going on. \"seed 1;\" in IVEware is setting the random seed in R, but \"seed 0;\" is something different.\n",
    "- Cite IVEware_User_Guide, page 17\n",
    "\n",
    "\"SEED number;\n",
    "\n",
    "Specifies a seed for the random draws from the posterior predictive distribution. Number should be greater than zero. A zero seed will result in no perturbations of the predicted values or the regression coefficients. If the SEED keyword is missing from the setup file then the seed will be determined by your computer’s internal clock.\"\n",
    "\n",
    "- set.seed(int) in R does not have this behavior at int=0.  I tried set.seed(0) in R and it worked just fine.  \n",
    "- SAS requires that the random seed be a positive integer, and SAS is one of the implementations of IVEware, so that may be why the IVEware authors thought to implement this functionality for their seed.\n",
    "\n",
    "- According to this ~2017 scraping of GitHub Python code to count the choices of random seeds,\n",
    "    - https://www.kaggle.com/code/residentmario/kernel16e284dcb7\n",
    "    - 0 is the most common (19%)\n",
    "    - 1 and 42 are next(9% and 4%, respectively)\n",
    "    \n",
    "- According to this 2014 scraping of 100 top R repositories owned by 27 people, \n",
    "    - https://www.r-bloggers.com/2014/03/what-are-the-most-common-rng-seeds-used-in-r-scripts-on-github/\n",
    "    - 1 is by far the most common (60 examples)\n",
    "    - 123 is next (about 25)\n",
    "    - 0 is not on the list\n",
    "    \n",
    "### Is this just an anomaly, or might \"seed 0;\" be useful?\n",
    "\n",
    "- Test Method\n",
    "    - Test with all 67 features, not dropping five multicollinear features\n",
    "    - We have results with seeds 1 and 42\n",
    "    - Test with seed 0 in IVEware, Python, and NumPy\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,167,826  | 100% | \n",
    "    | RF |  591,364  |  27.28 % | \n",
    "    | Mode |  739,696  |  34.12 % | \n",
    "    | Random |  971,759  |  44.83 % | \n",
    "    | IVEware |  447,881  |  20.66 % | \n",
    "    \n",
    "    <br><br>\n",
    "    - Test with seed 0 in IVEware but seed 42 in Python and NumPy in the Binning and Imputation\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,168,989  | 100% | \n",
    "    | RF |  587,221  |  27.07 % | \n",
    "    | Mode |  739,903  |  34.11 % | \n",
    "    | Random |  971,670  |  44.8 % | \n",
    "    | IVEware |  445,195  |  20.53 % | \n",
    "    \n",
    "- Another test method\n",
    "    - Randomly sample from 67 to 40 features and test again\n",
    "    - Note that dropping features will increase the number of samples that have no missing values, so data_Ground_Truth and data_NaN will have fewer features but more samples, so having about the same number of total missing values over the 40 features is not a problem.\n",
    "    - Do it twice with two random seeds.  \n",
    "    - The same random seed for Python and NumPy will preserve, but different random seeds will change:\n",
    "        - Which features get dropped\n",
    "        - Which 15% of the samples will get dropped to make data_NaN for testing the imputation\n",
    "    - Seed 0 in Python and Numpy, seed 0 in IVEware:\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,071,755  | 100% | \n",
    "    | RF |  652,983  |  31.52 % | \n",
    "    | Mode |  774,049  |  37.36 % | \n",
    "    | Random |  997,759  |  48.16 % | \n",
    "    | IVEware |  556,618  |  26.87 % | \n",
    "\n",
    "    - Seed 0 in Python and NumPy, seed 1 in IVEware:\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,071,755  | 100% | \n",
    "    | RF |  652,983  |  31.52 % | \n",
    "    | Mode |  774,049  |  37.36 % | \n",
    "    | Random |  997,759  |  48.16 % | \n",
    "    | IVEware |  738,201  |  35.63 % | \n",
    "    \n",
    "    \n",
    "    - Seed 1 in Python and Numpy, seed 0 in IVEware:\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  1,861,972  | 100% | \n",
    "    | RF |  449,626  |  24.15 % | \n",
    "    | Mode |  547,876  |  29.42 % | \n",
    "    | Random |  737,527  |  39.61 % | \n",
    "    | IVEware |  370,546  |  19.9 % | \n",
    "\n",
    "    - Seed 1 in Python and Numpy, seed 1 in IVEware:\n",
    "    \n",
    "   | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  1,861,972  | 100% | \n",
    "    | RF |  449,626  |  24.15 % | \n",
    "    | Mode |  547,876  |  29.42 % | \n",
    "    | Random |  737,527  |  39.61 % | \n",
    "    | IVEware |  486,820  |  26.15 % | \n",
    "    \n",
    "    - Analysis\n",
    "        - Seed 1 (compared with seed 0) for Python and NumPy appears to have chosen features that are easier to impute\n",
    "        - Within each seed for Python and Numpy, choosing seed 0 for IVEware gave much better results.  \n",
    "    \n",
    "### Conclusion\n",
    "- Setting the IVEware seed to zero is not recommended in the manual, and we think it shouldn't work well, but it works dramatically well with our test methods.  \n",
    "- Use two sets of data from here on, one imputed with Random Forest and another imputed with IVEware with random seed zero.  See which gives best results at the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ec94a",
   "metadata": {},
   "source": [
    "# Results of Comparison of Six Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614afad2",
   "metadata": {},
   "source": [
    "- We start with the binned (discretized) data, CRSS_Binned_Data.csv, with 817,623 samples in 67 features.\n",
    "<br><br>\n",
    "- Dropping any sample with a missing value, we have 232,333 samples of Ground Truth.\n",
    "\n",
    "- Replacing 15% of the sample values in each feature with NaN, we have $232,333  \\times 67 \\times 0.15 = 1,866,090$ missing values to be imputed.  \n",
    "\n",
    "<br><br>\n",
    "- First run with random seed  42 in Python and NumPy, and both 0 and 1 as random seeds for IVEware\n",
    "    <br><br>\n",
    "    - Samples Incorrectly Imputed\n",
    "   \n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  417,246  | 100% | \n",
    "    | RF |  112,645  |  27.0 % | \n",
    "    | MF |  85,624  |  20.52 % | \n",
    "    | Mode |  148,831  |  35.67 % | \n",
    "    | Random |  194,305  |  46.57 % | \n",
    "    | IVEware_seed_0 |  90,130  |  21.6 % | \n",
    "    | IVEware_seed_1 |  123,928  |  29.7 % | \n",
    "\n",
    "    <br><br>\n",
    "    - Comparison of number of errors in the 67 features.  For instance, comparing my Random Forest Round-Robin method to MissForest, in 1 feature RF had fewer errors than MissForest, in 18 features the two methods had the same number of errors, and in 48 features RF had more errors than MissForest.  \n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  1  |  18  |  48  | 67  |\n",
    "    | Compare RF to Mode |  40  |  27  |  0  | 67  |\n",
    "    | Compare RF to Random |  54  |  13  |  0  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  4  |  19  |  44  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  30  |  14  |  23  | 67  |\n",
    "    | Compare MF to Mode |  50  |  17  |  0  | 67  |\n",
    "    | Compare MF to Random |  54  |  13  |  0  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  31  |  22  |  14  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  53  |  12  |  2  | 67  |\n",
    "    | Compare Mode to Random |  52  |  15  |  0  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  1  |  17  |  49  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  22  |  12  |  33  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  1  |  11  |  55  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  7  |  13  |  47  | 67  |\n",
    "\n",
    "\n",
    "    <br><br>\n",
    "     - Number of NaN Imputed Differently by Different Methods\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  417,246  | 100% |\n",
    "    | RF Different from MF |  53,566  |  12.84 % |\n",
    "    | RF Different from Mode |  57,378  |  13.75 % |\n",
    "    | RF Different from Random |  161,299  |  38.66 % |\n",
    "    | RF Different from IVEware_seed_0 |  48,866  |  11.71 % |\n",
    "    | RF Different from IVEware_seed_1 |  120,019  |  28.76 % |\n",
    "    | MF Different from Mode |  99,730  |  23.9 % |\n",
    "    | MF Different from Random |  174,445  |  41.81 % |\n",
    "    | MF Different from IVEware_seed_0 |  38,763  |  9.29 % |\n",
    "    | MF Different from IVEware_seed_1 |  103,645  |  24.84 % |\n",
    "    | Mode Different from Random |  148,772  |  35.66 % |\n",
    "    | Mode Different from IVEware_seed_0 |  96,820  |  23.2 % |\n",
    "    | Mode Different from IVEware_seed_1 |  153,020  |  36.67 % |\n",
    "    | Random Different from IVEware_seed_0 |  175,503  |  42.06 % |\n",
    "    | Random Different from IVEware_seed_1 |  196,586  |  47.12 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  97,299  |  23.32 % |\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "- Second Run, Same random seed (42) to make sure the random seed is implemented correctly.  Same results. \n",
    "\n",
    "    <br><br>\n",
    "     - Percentage of Samples Incorrectly Imputed\n",
    "     \n",
    "\n",
    "\n",
    "   | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  417,246  | 100% | \n",
    "    | RF |  112,645  |  27.0 % | \n",
    "    | MF |  85,624  |  20.52 % | \n",
    "    | Mode |  148,831  |  35.67 % | \n",
    "    | Random |  194,305  |  46.57 % | \n",
    "    | IVEware_seed_0 |  90,130  |  21.6 % | \n",
    "    | IVEware_seed_1 |  123,928  |  29.7 % | \n",
    "\n",
    "    <br><br>\n",
    "     - Comparison of number of errors in the 67 features:\n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  1  |  18  |  48  | 67  |\n",
    "    | Compare RF to Mode |  40  |  27  |  0  | 67  |\n",
    "    | Compare RF to Random |  54  |  13  |  0  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  4  |  19  |  44  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  30  |  14  |  23  | 67  |\n",
    "    | Compare MF to Mode |  50  |  17  |  0  | 67  |\n",
    "    | Compare MF to Random |  54  |  13  |  0  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  31  |  22  |  14  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  53  |  12  |  2  | 67  |\n",
    "    | Compare Mode to Random |  52  |  15  |  0  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  1  |  17  |  49  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  22  |  12  |  33  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  1  |  11  |  55  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  7  |  13  |  47  | 67  |\n",
    "\n",
    "    <br><br>\n",
    "     - Number of NaN Imputed Differently by Different Methods\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  417,246  | 100% |\n",
    "    | RF Different from MF |  53,566  |  12.84 % |\n",
    "    | RF Different from Mode |  57,378  |  13.75 % |\n",
    "    | RF Different from Random |  161,299  |  38.66 % |\n",
    "    | RF Different from IVEware_seed_0 |  48,866  |  11.71 % |\n",
    "    | RF Different from IVEware_seed_1 |  120,019  |  28.76 % |\n",
    "    | MF Different from Mode |  99,730  |  23.9 % |\n",
    "    | MF Different from Random |  174,445  |  41.81 % |\n",
    "    | MF Different from IVEware_seed_0 |  38,763  |  9.29 % |\n",
    "    | MF Different from IVEware_seed_1 |  103,645  |  24.84 % |\n",
    "    | Mode Different from Random |  148,772  |  35.66 % |\n",
    "    | Mode Different from IVEware_seed_0 |  96,820  |  23.2 % |\n",
    "    | Mode Different from IVEware_seed_1 |  153,020  |  36.67 % |\n",
    "    | Random Different from IVEware_seed_0 |  175,503  |  42.06 % |\n",
    "    | Random Different from IVEware_seed_1 |  196,586  |  47.12 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  97,299  |  23.32 % |\n",
    "\n",
    "<br><br>\n",
    "- Third run, with random seed 0 in Python and Numpy, with both 0 and 1 as random seeds for IVEware.\n",
    "    - Note that the IVEware results are different in this run than in the previous runs, even though we used the same random seeds in IVEware.  The reason for the change is the different seed for Python and NumPy, which changed which values were missing in the dataset that we fed into IVEware.  \n",
    "\n",
    "    <br><br>\n",
    "    - Samples Incorrectly Imputed by Method\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  417,237  | 100% | \n",
    "    | RF |  112,416  |  26.94 % | \n",
    "    | MF |  85,273  |  20.44 % | \n",
    "    | Mode |  149,065  |  35.73 % | \n",
    "    | Random |  194,724  |  46.67 % | \n",
    "    | IVEware_seed_0 |  89,391  |  21.42 % | \n",
    "    | IVEware_seed_1 |  122,642  |  29.39 % | \n",
    "\n",
    "\n",
    "    <br><br>\n",
    "- Comparison of number of errors in the 67 features:\n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  2  |  14  |  51  | 67  |\n",
    "    | Compare RF to Mode |  39  |  28  |  0  | 67  |\n",
    "    | Compare RF to Random |  53  |  14  |  0  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  3  |  20  |  44  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  33  |  11  |  23  | 67  |\n",
    "    | Compare MF to Mode |  53  |  13  |  1  | 67  |\n",
    "    | Compare MF to Random |  56  |  10  |  1  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  31  |  18  |  18  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  54  |  11  |  2  | 67  |\n",
    "    | Compare Mode to Random |  51  |  16  |  0  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  0  |  21  |  46  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  21  |  10  |  36  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  0  |  13  |  54  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  8  |  8  |  51  | 67  |\n",
    "\n",
    "\n",
    "\n",
    "    <br><br>\n",
    "- Number of NaN Imputed Differently by Pairs of Methods\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  417,237  | 100% |\n",
    "    | RF Different from MF |  54,162  |  12.98 % |\n",
    "    | RF Different from Mode |  58,329  |  13.98 % |\n",
    "    | RF Different from Random |  161,621  |  38.74 % |\n",
    "    | RF Different from IVEware_seed_0 |  57,464  |  13.77 % |\n",
    "    | RF Different from IVEware_seed_1 |  118,643  |  28.44 % |\n",
    "    | MF Different from Mode |  101,716  |  24.38 % |\n",
    "    | MF Different from Random |  174,740  |  41.88 % |\n",
    "    | MF Different from IVEware_seed_0 |  38,365  |  9.2 % |\n",
    "    | MF Different from IVEware_seed_1 |  102,005  |  24.45 % |\n",
    "    | Mode Different from Random |  148,709  |  35.64 % |\n",
    "    | Mode Different from IVEware_seed_0 |  104,535  |  25.05 % |\n",
    "    | Mode Different from IVEware_seed_1 |  151,758  |  36.37 % |\n",
    "    | Random Different from IVEware_seed_0 |  176,490  |  42.3 % |\n",
    "    | Random Different from IVEware_seed_1 |  195,705  |  46.9 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  96,334  |  23.09 % |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e79629",
   "metadata": {},
   "source": [
    "## Drop Multicollinear Features before Imputing?  Compare two methods\n",
    "- First Method\n",
    "    - After Binning, reduce dimensionality\n",
    "        - Removes MAX_VSEV, VE_FORMS, VTCONT_F, MAX_SEV, NUM_INJV\n",
    "        - Reduces from 67 to 62 features\n",
    "    - Impute\n",
    "- Second Method\n",
    "    - Impute with all 67 features\n",
    "    - Before evaluating the imputation, remove the five features and only evaluate the results on the 62 features used in the comparison above\n",
    "- We used random seed 42 for both methods\n",
    "- First Method Results\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,007,463  | 100% | \n",
    "    | RF |  569,509  |  28.37 % | \n",
    "    | Mode |  681,753  |  33.96 % | \n",
    "    | Random |  889,794  |  44.32 % | \n",
    "    | IVEware |  606,632  |  30.22 % | \n",
    "    \n",
    "- Second Method Results\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,007,235  | 100% | \n",
    "    | RF |  558,936  |  27.85 % | \n",
    "    | Mode |  681,996  |  33.98 % | \n",
    "    | Random |  888,845  |  44.28 % | \n",
    "    | IVEware |  606,062  |  30.19 % | \n",
    "\n",
    "\n",
    "### Analysis\n",
    "- Mode was the same, as it should be.\n",
    "- Random was slightly different, perhaps because the features were in a different order?\n",
    "- IVEware was not significantly different in the two methods.\n",
    "- Random Forest was slightly but significantly better (0.52%) with the second method, not removing the multicollinear features before imputing, which is surprising.  \n",
    "\n",
    "### Conclusion\n",
    "- Run again with different random seed = 1\n",
    "\n",
    "### Second Round Results\n",
    "- First Method\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,006,643  | 100% | \n",
    "    | RF |  568,909  |  28.35 % | \n",
    "    | Mode |  681,061  |  33.94 % | \n",
    "    | Random |  889,048  |  44.31 % | \n",
    "    | IVEware |  592,233  |  29.51 % | \n",
    "\n",
    "\n",
    "- Second Method\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,005,955  | 100% | \n",
    "    | RF |  558,742  |  27.85 % | \n",
    "    | Mode |  680,715  |  33.93 % | \n",
    "    | Random |  887,944  |  44.27 % | \n",
    "    | IVEware |  564,254  |  28.13 % | \n",
    "    \n",
    "### Analysis\n",
    "\n",
    "- Again, the second method, leaving in multicollinear features, is better for both Random Forest and IVEware\n",
    "\n",
    "### Conclusion\n",
    "- When we impute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32425867",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed07de",
   "metadata": {},
   "source": [
    "- Random imputation is clearly worse than Mode and RF on every feature.\n",
    "- Random is overall worse than IVEware, but on one of our runs there are five features on which Random is better than IVEware.\n",
    "- Random Forest is as good or better than Mode on every feature, which is not surprising, as RF starts at Mode and improves on it.  \n",
    "- Random Forest is as good or better than IVEware on more than half of the features, but not overwhelmingly, and slightly better in the count of missing samples correctly imputed.\n",
    "- IVEware and Mode are comparable in the number of features, but IVEware is much better in the count of missing samples correctly imputed.\n",
    "- Random Forest and Mode make the same mistakes.  \n",
    "- IVEware makes different mistakes from Random Forest and Mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d8b76",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e27f7d",
   "metadata": {},
   "source": [
    "- Use Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6fe8c",
   "metadata": {},
   "source": [
    "## Opportunities for Future Research\n",
    "(or, \"Things we didn't do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bb784",
   "metadata": {},
   "source": [
    "- Which features are better imputed by Random imputation than by IVEware, and why?\n",
    "- Which features are better imputed by IVEware than by Random Forest, and why?\n",
    "- Would a different mix of features make IVEware perform better than Random Forest?\n",
    "- Is it okay to use one imputation method for some features and another method for other features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd6971",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9db3b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy version: 1.26.4\n",
      "Pandas version:  2.2.2\n",
      "SciKit-Learn version: 1.5.0\n",
      "MissForest version:  2.5.5\n",
      "Random seed for Python and NumPy set to  0\n",
      "Finished Importing Libraries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import missforest.missforest\n",
    "from missforest.missforest import MissForest\n",
    "print ('MissForest version:  {}'.format(missforest.missforest.__version__))\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "random_seed = 0\n",
    "print ('Random seed for Python and NumPy set to ', random_seed)\n",
    "np.random.seed(random_seed) # NumPy\n",
    "random.seed(random_seed) # Python\n",
    "#tf.set_random_seed(random_seed) # Tensorflow\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './beep.wav'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Importing Libraries')\n",
    "print ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d05f7",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "This notebook pulls in the saved output of Ambulance_Dispatch_2024_02_Binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9ba441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data')\n",
    "    data = pd.read_csv('../../Big_Files/CRSS_Binned_Data.csv', low_memory=False)\n",
    "#    data = pd.read_csv('../../Big_Files/CRSS_Binned_Data_Seed_42.csv', low_memory=False)\n",
    "#    data = pd.read_csv('../../Big_Files/CRSS_Binned_Reduced_Dimensionality_Data.csv', low_memory=False)\n",
    "    print ('data.shape = ', data.shape)\n",
    "    print ()\n",
    "\n",
    "    # We already dropped the imputed columns in the Binning stage\n",
    "    print ('Drop Imputed Columns')\n",
    "    for feature in data:\n",
    "        if '_IM' in feature:\n",
    "            print (feature)\n",
    "            data.drop(columns=feature, inplace=True)\n",
    " \n",
    "\n",
    "    # Method for dropping from 67 to 40 features \n",
    "    # to test whether it was just this particular mix of features \n",
    "    # that made the IVEware behave strangely well with random seed of zero.\n",
    "#    print ('data.shape = ', data.shape)\n",
    "#    data = data.sample(n=40, axis='columns')\n",
    "    \n",
    "    print ('data.shape = ', data.shape)\n",
    "    print ()\n",
    "    \n",
    "    print ('Total number of NaN')\n",
    "    print (data.replace({99:np.nan}).isnull().sum().sum())\n",
    "    print ()\n",
    "    \n",
    "#    print (\"Remaining Features:\")\n",
    "#    Features = sorted(list(data.columns))\n",
    "#    for feature in Features:\n",
    "#        print (\"    \",feature)\n",
    "    print ('Finished Get_Data()')\n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d026ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = Get_Data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe14831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Analyze_Data():\n",
    "    data = Get_Data()\n",
    "    print ('Analyze_Data')\n",
    "    data = data.replace({99:np.nan})\n",
    "    \n",
    "    print ('Total NaN')\n",
    "    s = data.isna().sum().sum()\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    print (s, \" \", round(s/(rows*cols)*100,2))\n",
    "    print ()\n",
    "\n",
    "    print ('Number of NaN in each feature')\n",
    "    Feature_NaN_Counts = []\n",
    "    for feature in data:\n",
    "        s = data[feature].isna().sum()\n",
    "        n = len(data)\n",
    "#        print (feature, s, round(s/n*100,2))\n",
    "        Feature_NaN_Counts.append([feature, round(s/n*100,6)])\n",
    "    for row in Feature_NaN_Counts:\n",
    "        print (row)\n",
    "    print ()\n",
    "    print ('Distribution of number of NaN in each sample')\n",
    "    A = data.isna().sum(axis=1)\n",
    "    Row_NaN_Counts = A.value_counts(normalize=True)\n",
    "    display(Row_NaN_Counts)\n",
    "    Row_NaN_Counts = Row_NaN_Counts.to_list()\n",
    "    \n",
    "    print ('Finished Analyze_Data()')\n",
    "    print ()\n",
    "    \n",
    "    return Feature_NaN_Counts, Row_NaN_Counts\n",
    "    \n",
    "#Feature_NaN_Counts, Row_NaN_Counts = Analyze_Data()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a664ab1",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3627270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_MissForest(data):\n",
    "    print('Impute_MissForest()')\n",
    "\n",
    "    print (data.shape)\n",
    "    display(data.head(20))\n",
    "#    data.replace({np.nan: ''}, inplace=True)\n",
    "#    display(data.head(20))\n",
    "\n",
    "    categorical = list(data)\n",
    "    print ('categorical features: ', categorical)\n",
    "    \n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "#        verbose=2,\n",
    "#        max_features=0.5\n",
    "    )\n",
    "    rgr = RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "#        verbose=2,\n",
    "#        max_features=0.5\n",
    "    )\n",
    "\n",
    "    data_MF = MissForest(clf, rgr, max_iter = 4).fit_transform(\n",
    "        x = data,\n",
    "        categorical=categorical,\n",
    "    )\n",
    "    display(data_MF.head(20))\n",
    "    print ('Finished Impute_MissForest()')\n",
    "    print ()\n",
    "    \n",
    "    return data_MF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30637144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data\n",
      "data.shape =  (802700, 67)\n",
      "\n",
      "Drop Imputed Columns\n",
      "data.shape =  (802700, 67)\n",
      "\n",
      "Total number of NaN\n",
      "1674506\n",
      "\n",
      "Finished Get_Data()\n",
      "\n",
      "(802700, 67)\n",
      "data_Ground_Truth.shape\n",
      "(232333, 67)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>...</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTCONT_F</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HOSPITAL  ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  \\\n",
       "0         0       4.0  7.0      3.0         2.0       7.0       0.0         4   \n",
       "1         0       2.0  6.0      1.0         2.0       4.0       0.0         3   \n",
       "2         1       2.0  5.0      1.0         2.0       4.0       0.0         3   \n",
       "3         0       2.0  5.0      3.0         2.0       4.0       0.0         3   \n",
       "4         0       7.0  6.0      4.0         2.0       2.0       0.0         3   \n",
       "\n",
       "   DEFORMED  DR_ZIP  ...  VE_FORMS  VE_TOTAL  VPROFILE  VSPD_LIM  VSURCOND  \\\n",
       "0       0.0     5.0  ...         1         1       3.0       5.0       0.0   \n",
       "1       0.0     5.0  ...         0         0       3.0       3.0       1.0   \n",
       "2       0.0     5.0  ...         0         0       3.0       3.0       1.0   \n",
       "3       0.0     5.0  ...         0         0       3.0       3.0       1.0   \n",
       "4       4.0     5.0  ...         1         1       3.0       3.0       0.0   \n",
       "\n",
       "   VTCONT_F  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  \n",
       "0       1.0       0.0       0.0      0.0         0  \n",
       "1       1.0       0.0       2.0      1.0         0  \n",
       "2       1.0       0.0       2.0      1.0         0  \n",
       "3       1.0       0.0       2.0      1.0         0  \n",
       "4       1.0       0.0       1.0      2.0         0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Create_data_NaN_Method_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Test_Impute_MissForest()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m ()\n\u001b[0;32m---> 42\u001b[0m \u001b[43mTest_Impute_MissForest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mTest_Impute_MissForest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m (data_Ground_Truth\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m display(data_Ground_Truth\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 17\u001b[0m data_NaN \u001b[38;5;241m=\u001b[39m \u001b[43mCreate_data_NaN_Method_3\u001b[49m(data_Ground_Truth)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_NaN.shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m (data_NaN\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Create_data_NaN_Method_3' is not defined"
     ]
    }
   ],
   "source": [
    "def Test_Impute_MissForest():\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int64')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "\n",
    "    data_NaN = Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "\n",
    "\n",
    "    print ('data_NaN.shape')\n",
    "    print (data_NaN.shape)\n",
    "    display(data_NaN.head())\n",
    "    \n",
    "#    data_NaN = data_NaN.astype('Int8')\n",
    "    \n",
    "#    data_NaN = data_NaN.sample(n=200000)\n",
    "    print (data_NaN.shape)\n",
    "    print (data_NaN.head(20))\n",
    "\n",
    "    \n",
    "    # Perform MissForest imputation\n",
    "    print ('Start Imputation')\n",
    "    data_MF = Impute_MissForest(data_NaN)\n",
    "    data_MF.sort_index(inplace=True)\n",
    "    data_MF = data_MF[data.columns]  \n",
    "#    data_MF = data_MF.astype('Int64')\n",
    "    print (data_MF.head(20))\n",
    "    \n",
    "    print ('Finished Test_Impute_MissForest()')\n",
    "    print ()\n",
    "    \n",
    "Test_Impute_MissForest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f489df19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Impute_Miss_Forest_2()\n",
      "Get_Data\n",
      "data.shape =  (802700, 67)\n",
      "\n",
      "Drop Imputed Columns\n",
      "data.shape =  (802700, 67)\n",
      "\n",
      "Total number of NaN\n",
      "1674506\n",
      "\n",
      "Finished Get_Data()\n",
      "\n",
      "(802700, 67)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>...</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTCONT_F</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HOSPITAL  ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  \\\n",
       "0         0         9    9        4          99         2         0         1   \n",
       "1         0         8    6        4          99         3         0         1   \n",
       "2         0         5    3        1          99         7         0         1   \n",
       "3         0         3    4        1          99         2         0         1   \n",
       "4         0         3    4        3           2         2         0         1   \n",
       "5         0         3    3        3           2         2         0         1   \n",
       "6         1         2    6        4          99         2         0         2   \n",
       "7         0         0    5        4           2         2         0         4   \n",
       "8         0         8    7       99           2         7         0         3   \n",
       "9         0         7    6       99           2         2         0         3   \n",
       "\n",
       "   DEFORMED  DR_ZIP  ...  VE_FORMS  VE_TOTAL  VPROFILE  VSPD_LIM  VSURCOND  \\\n",
       "0         2       8  ...         1         1         3         2         0   \n",
       "1         2       8  ...         1         1         3         2         0   \n",
       "2         0       8  ...         1         1         2         2         0   \n",
       "3         0       8  ...         1         1         2         2         0   \n",
       "4         0       8  ...         1         1         2         2         0   \n",
       "5         0       8  ...         1         1         2         2         0   \n",
       "6         0       8  ...         0         0         3         5         0   \n",
       "7         4       8  ...         1         1         2         3         1   \n",
       "8         3       5  ...         1         1         0         3         0   \n",
       "9         4       5  ...         1         1         0         3         0   \n",
       "\n",
       "   VTCONT_F  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  \n",
       "0        99        99        99        0         0  \n",
       "1        99        99        99        0         0  \n",
       "2        99        99        99        0         0  \n",
       "3        99        99        99        0         0  \n",
       "4        99        99        99        0         0  \n",
       "5        99        99        99        0         0  \n",
       "6        99        99         1        0         0  \n",
       "7        99        99        99        0         0  \n",
       "8         1         0         1        0         0  \n",
       "9         1         0         1        0         0  \n",
       "\n",
       "[10 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802700, 67)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>...</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTCONT_F</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HOSPITAL  ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  \\\n",
       "0         0       9.0  9.0      4.0         NaN       2.0       0.0         1   \n",
       "1         0       8.0  6.0      4.0         NaN       3.0       0.0         1   \n",
       "2         0       5.0  3.0      1.0         NaN       7.0       0.0         1   \n",
       "3         0       3.0  4.0      1.0         NaN       2.0       0.0         1   \n",
       "4         0       3.0  4.0      3.0         2.0       2.0       0.0         1   \n",
       "5         0       3.0  3.0      3.0         2.0       2.0       0.0         1   \n",
       "6         1       2.0  6.0      4.0         NaN       2.0       0.0         2   \n",
       "7         0       0.0  5.0      4.0         2.0       2.0       0.0         4   \n",
       "8         0       8.0  7.0      NaN         2.0       7.0       0.0         3   \n",
       "9         0       7.0  6.0      NaN         2.0       2.0       0.0         3   \n",
       "\n",
       "   DEFORMED  DR_ZIP  ...  VE_FORMS  VE_TOTAL  VPROFILE  VSPD_LIM  VSURCOND  \\\n",
       "0       2.0     8.0  ...         1         1       3.0       2.0       0.0   \n",
       "1       2.0     8.0  ...         1         1       3.0       2.0       0.0   \n",
       "2       0.0     8.0  ...         1         1       2.0       2.0       0.0   \n",
       "3       0.0     8.0  ...         1         1       2.0       2.0       0.0   \n",
       "4       0.0     8.0  ...         1         1       2.0       2.0       0.0   \n",
       "5       0.0     8.0  ...         1         1       2.0       2.0       0.0   \n",
       "6       0.0     8.0  ...         0         0       3.0       5.0       0.0   \n",
       "7       4.0     8.0  ...         1         1       2.0       3.0       1.0   \n",
       "8       3.0     5.0  ...         1         1       0.0       3.0       0.0   \n",
       "9       4.0     5.0  ...         1         1       0.0       3.0       0.0   \n",
       "\n",
       "   VTCONT_F  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  \n",
       "0       NaN       NaN       NaN      0.0         0  \n",
       "1       NaN       NaN       NaN      0.0         0  \n",
       "2       NaN       NaN       NaN      0.0         0  \n",
       "3       NaN       NaN       NaN      0.0         0  \n",
       "4       NaN       NaN       NaN      0.0         0  \n",
       "5       NaN       NaN       NaN      0.0         0  \n",
       "6       NaN       NaN       1.0      0.0         0  \n",
       "7       NaN       NaN       NaN      0.0         0  \n",
       "8       1.0       0.0       1.0      0.0         0  \n",
       "9       1.0       0.0       1.0      0.0         0  \n",
       "\n",
       "[10 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute_MissForest()\n",
      "(802700, 67)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>...</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTCONT_F</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    HOSPITAL  ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  \\\n",
       "0          0       9.0  9.0      4.0         NaN       2.0       0.0   \n",
       "1          0       8.0  6.0      4.0         NaN       3.0       0.0   \n",
       "2          0       5.0  3.0      1.0         NaN       7.0       0.0   \n",
       "3          0       3.0  4.0      1.0         NaN       2.0       0.0   \n",
       "4          0       3.0  4.0      3.0         2.0       2.0       0.0   \n",
       "5          0       3.0  3.0      3.0         2.0       2.0       0.0   \n",
       "6          1       2.0  6.0      4.0         NaN       2.0       0.0   \n",
       "7          0       0.0  5.0      4.0         2.0       2.0       0.0   \n",
       "8          0       8.0  7.0      NaN         2.0       7.0       0.0   \n",
       "9          0       7.0  6.0      NaN         2.0       2.0       0.0   \n",
       "10         0       7.0  1.0      NaN         2.0       2.0       0.0   \n",
       "11         0       7.0  2.0      NaN         2.0       2.0       0.0   \n",
       "12         0       7.0  7.0      NaN         2.0       2.0       0.0   \n",
       "13         0       8.0  6.0      NaN         2.0       7.0       0.0   \n",
       "14         1       7.0  9.0      NaN         2.0       7.0       0.0   \n",
       "15         0       NaN  7.0      NaN         2.0       4.0       0.0   \n",
       "16         0       NaN  7.0      NaN         2.0       4.0       0.0   \n",
       "17         0       8.0  6.0      NaN         2.0       7.0       0.0   \n",
       "18         0       5.0  9.0      NaN         2.0       7.0       0.0   \n",
       "19         0       3.0  7.0      NaN         2.0       7.0       0.0   \n",
       "\n",
       "    DAY_WEEK  DEFORMED  DR_ZIP  ...  VE_FORMS  VE_TOTAL  VPROFILE  VSPD_LIM  \\\n",
       "0          1       2.0     8.0  ...         1         1       3.0       2.0   \n",
       "1          1       2.0     8.0  ...         1         1       3.0       2.0   \n",
       "2          1       0.0     8.0  ...         1         1       2.0       2.0   \n",
       "3          1       0.0     8.0  ...         1         1       2.0       2.0   \n",
       "4          1       0.0     8.0  ...         1         1       2.0       2.0   \n",
       "5          1       0.0     8.0  ...         1         1       2.0       2.0   \n",
       "6          2       0.0     8.0  ...         0         0       3.0       5.0   \n",
       "7          4       4.0     8.0  ...         1         1       2.0       3.0   \n",
       "8          3       3.0     5.0  ...         1         1       0.0       3.0   \n",
       "9          3       4.0     5.0  ...         1         1       0.0       3.0   \n",
       "10         3       4.0     5.0  ...         1         1       0.0       3.0   \n",
       "11         3       4.0     5.0  ...         1         1       0.0       3.0   \n",
       "12         3       4.0     5.0  ...         1         1       0.0       3.0   \n",
       "13         4       0.0     5.0  ...         3         3       3.0       4.0   \n",
       "14         4       0.0     5.0  ...         3         3       3.0       4.0   \n",
       "15         4       2.0     5.0  ...         3         3       3.0       4.0   \n",
       "16         4       4.0     4.0  ...         3         3       3.0       4.0   \n",
       "17         1       3.0     5.0  ...         1         1       3.0       6.0   \n",
       "18         3       0.0     4.0  ...         1         1       3.0       3.0   \n",
       "19         3       0.0     9.0  ...         1         1       3.0       3.0   \n",
       "\n",
       "    VSURCOND  VTCONT_F  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  \n",
       "0        0.0       NaN       NaN       NaN      0.0         0  \n",
       "1        0.0       NaN       NaN       NaN      0.0         0  \n",
       "2        0.0       NaN       NaN       NaN      0.0         0  \n",
       "3        0.0       NaN       NaN       NaN      0.0         0  \n",
       "4        0.0       NaN       NaN       NaN      0.0         0  \n",
       "5        0.0       NaN       NaN       NaN      0.0         0  \n",
       "6        0.0       NaN       NaN       1.0      0.0         0  \n",
       "7        1.0       NaN       NaN       NaN      0.0         0  \n",
       "8        0.0       1.0       0.0       1.0      0.0         0  \n",
       "9        0.0       1.0       0.0       1.0      0.0         0  \n",
       "10       0.0       1.0       0.0       1.0      0.0         0  \n",
       "11       0.0       1.0       0.0       1.0      0.0         0  \n",
       "12       0.0       1.0       0.0       1.0      0.0         0  \n",
       "13       0.0       1.0       0.0       1.0      0.0         0  \n",
       "14       0.0       1.0       0.0       1.0      0.0         0  \n",
       "15       0.0       1.0       0.0       1.0      0.0         0  \n",
       "16       0.0       1.0       0.0       1.0      0.0         0  \n",
       "17       0.0       1.0       0.0       1.0      0.0         0  \n",
       "18       0.0       2.0       1.0       0.0      0.0         0  \n",
       "19       0.0       2.0       1.0       0.0      0.0         0  \n",
       "\n",
       "[20 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:  ['HOSPITAL', 'ACC_TYPE', 'AGE', 'AIR_BAG', 'ALC_STATUS', 'BODY_TYP', 'CARGO_BT', 'DAY_WEEK', 'DEFORMED', 'DR_ZIP', 'EJECTION', 'HARM_EV', 'HIT_RUN', 'HOUR', 'IMPACT1', 'INJ_SEV', 'INT_HWY', 'J_KNIFE', 'LGT_COND', 'MAKE', 'MAK_MOD', 'MAN_COLL', 'MAX_SEV', 'MAX_VSEV', 'MODEL', 'MONTH', 'M_HARM', 'NUMOCCS', 'NUM_INJ', 'NUM_INJV', 'PCRASH4', 'PCRASH5', 'PERMVIT', 'PER_TYP', 'PJ', 'PSU', 'PVH_INVL', 'P_CRASH1', 'P_CRASH2', 'REGION', 'RELJCT1', 'RELJCT2', 'REL_ROAD', 'REST_MIS', 'REST_USE', 'ROLINLOC', 'ROLLOVER', 'SEAT_POS', 'SEX', 'SPEC_USE', 'SPEEDREL', 'TOWED', 'TOW_VEH', 'TYP_INT', 'URBANICITY', 'VALIGN', 'VEH_AGE', 'VE_FORMS', 'VE_TOTAL', 'VPROFILE', 'VSPD_LIM', 'VSURCOND', 'VTCONT_F', 'VTRAFCON', 'VTRAFWAY', 'WEATHER', 'WRK_ZONE']\n",
      "Iteration  0  of  4 , feature  ACC_TYPE\n",
      "Iteration  0  of  4 , feature  AGE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Test_Impute_Miss_Forest_2()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m ()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mTest_Impute_MissForest_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mTest_Impute_MissForest_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;241m99\u001b[39m:np\u001b[38;5;241m.\u001b[39mnan}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m display(data\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m data_MF \u001b[38;5;241m=\u001b[39m \u001b[43mImpute_MissForest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Test_Impute_Miss_Forest_2()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m ()\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mImpute_MissForest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m     clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     13\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m     14\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#        verbose=2,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#        max_features=0.5\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     rgr \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[1;32m     19\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m     20\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#        verbose=2,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#        max_features=0.5\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     data_MF \u001b[38;5;241m=\u001b[39m \u001b[43mMissForest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     display(data_MF\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Impute_MissForest()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/missforest/missforest.py:533\u001b[0m, in \u001b[0;36mMissForest.fit_transform\u001b[0;34m(self, x, categorical)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03mCalls class method 'fit' and 'transform' on 'x'.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Imputed dataset (features only).\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(x, categorical)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/missforest/missforest.py:467\u001b[0m, in \u001b[0;36mMissForest.transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m x_obs \u001b[38;5;241m=\u001b[39m x_imp\u001b[38;5;241m.\u001b[39mdrop(c, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[obs_rows]\n\u001b[1;32m    466\u001b[0m y_obs \u001b[38;5;241m=\u001b[39m x_imp[c]\u001b[38;5;241m.\u001b[39mloc[obs_rows]\n\u001b[0;32m--> 467\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Predict the missing column with the trained estimator\u001b[39;00m\n\u001b[1;32m    470\u001b[0m miss_index \u001b[38;5;241m=\u001b[39m missing_rows[c]\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def Test_Impute_MissForest_2():\n",
    "    print ('Test_Impute_Miss_Forest_2()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    display(data.head(10))\n",
    "#    data = data.sample(n=1000)\n",
    "    print (data.shape)\n",
    "    data.replace({99:np.nan}, inplace=True)\n",
    "    display(data.head(10))\n",
    "    data_MF = Impute_MissForest(data)\n",
    "    \n",
    "    print ('Finished Test_Impute_Miss_Forest_2()')\n",
    "    print ()\n",
    "\n",
    "Test_Impute_MissForest_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defec34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Round_Robin(data):\n",
    "    print ('Impute_Round_Robin()')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "    # Replace 'Unknown' with np.NaN\n",
    "#    data.replace({'Unknown': np.nan}, inplace=True)\n",
    "    data.replace({99: np.nan}, inplace=True)\n",
    "    display(data.head(20))\n",
    "    print ()\n",
    "    \n",
    "    # Make a list of features with missing samples, \n",
    "    #     ordered by the number of missing samples, \n",
    "    #     from least to most.  \n",
    "    Missing = []\n",
    "    Complete = []\n",
    "    for feature in data:\n",
    "        s = data[feature].isna().sum()\n",
    "        if s==0:\n",
    "            Complete.append([feature, s])\n",
    "        if s>0:\n",
    "            Missing.append([feature, s])\n",
    "    Missing = sorted (Missing, key=lambda x:x[1], reverse=False)\n",
    "#    print ()\n",
    "#    print ('Complete[]')\n",
    "#    display(Complete)\n",
    "#    print ()\n",
    "#    print ('Missing[]')\n",
    "#    display(Missing)\n",
    "#    print ()\n",
    "    \n",
    "#    print ('Make data_Mode')\n",
    "#    print ()\n",
    "    data_Mode = pd.DataFrame()\n",
    "    for X in Complete:\n",
    "        feature = X[0]\n",
    "        data_Mode[feature] = data[feature]\n",
    "    for M in Missing:\n",
    "        feature = M[0]\n",
    "        m = data[feature].mode()[0]\n",
    "#        print (feature, M[1], m)\n",
    "        data_Mode[feature] = data[feature].fillna(m)\n",
    "#    print ('data_Mode')\n",
    "    display(data_Mode.head(20))\n",
    "\n",
    "#    print ()\n",
    "#    print ('Make starting point for data_Imputed')\n",
    "    data_Imputed = pd.DataFrame()\n",
    "    for X in Complete:\n",
    "        feature = X[0]\n",
    "        data_Imputed[feature] = data[feature]\n",
    "    for X in Missing:\n",
    "        feature = X[0]\n",
    "        data_Imputed[feature] = data_Mode[feature]\n",
    "#    print ('data_Imputed')\n",
    "#    display(data_Imputed.head(20))\n",
    "#    print ()\n",
    "\n",
    "    print ('Start Loop')\n",
    "    print ()\n",
    "    n = 0\n",
    "    for M in Missing:\n",
    "        n += 1\n",
    "        print (M)\n",
    "        feature = M[0]\n",
    "        data_Imputed[feature] = data[feature]\n",
    "#        print ()\n",
    "#        print ('data[feature].isna().sum()')\n",
    "#        print (data[feature].isna().sum())\n",
    "#        print ('data_Imputed[feature].isna().sum()')\n",
    "#        print (data_Imputed[feature].isna().sum())\n",
    "#        print ()\n",
    "        W = data_Imputed.dropna(subset=[feature])\n",
    "        X = data_Imputed.dropna(subset=[feature])\n",
    "        y = X[feature]\n",
    "        X.drop(columns=feature, inplace=True)\n",
    "        Z = data_Imputed[data_Imputed[feature].isna()]\n",
    "        Z.drop(columns=feature, inplace=True)\n",
    "#        Z.reset_index(drop=True, inplace=True)\n",
    "#        print (data.shape)\n",
    "#        print (X.shape)\n",
    "#        display(X.head(40))\n",
    "#        display(y.head(40))\n",
    "#        print (Z.shape)\n",
    "#        display(Z)\n",
    "        clf = RandomForestClassifier(max_depth=2, random_state=random_seed)\n",
    "        clf.fit(X,y)\n",
    "#        print ('clf.predict(Z)')\n",
    "        z = clf.predict(Z)\n",
    "#        print (len(z))\n",
    "#        display(z)\n",
    "        Z[feature] = z\n",
    "#        display(Z)\n",
    "        data_Imputed = pd.concat([Z, W])\n",
    "#        display(data_Imputed.head(60))\n",
    "#        print (data_Imputed.shape)\n",
    "#        print ()\n",
    "#        data_Imputed.sort_values(\n",
    "#            by = ['CASENUM', 'VEH_NO', 'PER_NO'], \n",
    "#            ascending = [True, True, True], \n",
    "#            inplace=True\n",
    "#        )\n",
    "#        print ()\n",
    "#        print ('data.PER_NO.equals(data_Imputed.PER_NO)')\n",
    "#        print (data.PER_NO.equals(data_Imputed.PER_NO))\n",
    "#        print ()\n",
    "               \n",
    "        Check_Feature(data, data_Imputed, feature)\n",
    "#        if n==10:\n",
    "#            return data_Imputed\n",
    "    \n",
    "    \n",
    "    display(data_Imputed.head(20))\n",
    "\n",
    "    print ('Finished Impute_Round_Robin()')\n",
    "    print ()\n",
    "    return data_Imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a34ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(data, data_Imputed):\n",
    "    print ('Check()')\n",
    "    Features = data.columns\n",
    "    print (Features)\n",
    "    for feature in Features:\n",
    "        U = pd.unique(data[feature]).tolist()\n",
    "        print (U)\n",
    "        A = []\n",
    "        for u in U:\n",
    "            a = len(data[data[feature]==u])\n",
    "            b = len(data_Imputed[data_Imputed[feature]==u])\n",
    "            A.append([u, a, b])\n",
    "        display(A)\n",
    "        print ()\n",
    "    print ('Finished Check()')\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Feature(data, data_Imputed, feature):\n",
    "    print ('Check_Feature()')\n",
    "    U = pd.unique(data[feature]).tolist()\n",
    "    U = [x for x in U if x == x]\n",
    "    print (U)\n",
    "    A = []\n",
    "    for u in U:\n",
    "        a = len(data[data[feature]==u])\n",
    "        b = len(data_Imputed[data_Imputed[feature]==u])\n",
    "        A.append([u, a, b, b-a])\n",
    "    a = data[feature].isna().sum()\n",
    "    b = data_Imputed[feature].isna().sum()\n",
    "    A.append(['NaN', a, b, 0])\n",
    "    A = pd.DataFrame(A, columns=['Value', 'Original', 'Imputed', 'Difference'])\n",
    "    display(A)\n",
    "    \n",
    "    print ('Finished Check_Feature()')\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Randomly(data):\n",
    "    print ('Impute_Randomly()')\n",
    "    print ()\n",
    "    \n",
    "    data.sample(frac=1, replace=True) # Randomly shuffle the rows of the dataset\n",
    "    for feature in data:\n",
    "        print (feature)\n",
    "#        print ('display(data[feature].head())')\n",
    "#        display(data[feature].head())\n",
    "        dfA = data[feature]\n",
    "#        print ('display(dfA.head())')\n",
    "#        display(dfA.head())\n",
    "        dfA.dropna(inplace=True)\n",
    "#        print ('display(dfA.head()) after dfA.dropna(inplace=True)')\n",
    "#        display(dfA.head())\n",
    "#        print ('Original Value Counts')\n",
    "#        print (dfA.value_counts(normalize=True))\n",
    "        dfA = dfA.sample(n = len(data), replace=True)\n",
    "#        print ('display(dfA.head()) after dfA.sample(n = len(data), replace=True)')\n",
    "#        display(dfA.head())\n",
    "#        print ('Value Counts after Sampling')\n",
    "#        print (dfA.value_counts(normalize=True))\n",
    "        dfA.reset_index(drop=True, inplace=True)\n",
    "#        print ('display(dfA.head()) after dfA.reset_index(drop=True)')\n",
    "#        display(dfA.head())\n",
    "        data[feature].fillna(dfA, inplace=True)\n",
    "#        print ('display(data[feature].head())')\n",
    "#        display(data[feature].head())        \n",
    "#        print ()\n",
    "        \n",
    "    print ('Finished Impute_Randomly()')\n",
    "    print ()\n",
    "    return data\n",
    "        \n",
    "def Test_Impute_Randomly():\n",
    "    Dict = {\n",
    "        'A':[0,0,0,1,np.nan],\n",
    "        'B':[1,2,3,4,np.nan]\n",
    "    }\n",
    "    \n",
    "    data = pd.DataFrame(Dict)\n",
    "    display(data)\n",
    "    data = Impute_Randomly(data)\n",
    "    display(data)\n",
    "    \n",
    "#Test_Impute_Randomly()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83dc402d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN_Method_3(data_Ground_Truth):\n",
    "    print ('Create_data_NaN_Method_3()')\n",
    "    nRows = data_Ground_Truth.shape[0]\n",
    "    nCols = data_Ground_Truth.shape[1]\n",
    "    print ('nRows = ', nRows, ' nCols = ', nCols)\n",
    "    \n",
    "    Feature_NaN_Counts, Row_NaN_Counts = Analyze_Data()\n",
    "    data_NaN_list = []\n",
    "    for i in range (len(Row_NaN_Counts)):\n",
    "        Ones = i\n",
    "        Zeros = nCols - i\n",
    "        Number = int(Row_NaN_Counts[i] * nRows + 1)\n",
    "        for j in range (Number):\n",
    "            New_row = [1]*Ones + [0]*Zeros\n",
    "            random.shuffle(New_row)\n",
    "            data_NaN_list.append(New_row.copy())\n",
    "    \n",
    "    data_NaN = pd.DataFrame(data_NaN_list, columns = data_Ground_Truth.columns)\n",
    "#    print (data_NaN.shape)\n",
    "    data_NaN = data_NaN.sample(n = nRows)\n",
    "#    print (data_NaN.shape)\n",
    "#    display(data_NaN.head(20))\n",
    "#    display(data_NaN.tail(20))\n",
    "    \n",
    "#    print ('Distribution of number of NaN in each sample')\n",
    "    A = data_NaN.sum(axis=1)\n",
    "    Row_NaN_Counts = A.value_counts(normalize=True)\n",
    "#    display(Row_NaN_Counts)\n",
    "#    Row_NaN_Counts = Row_NaN_Counts.to_list()\n",
    "\n",
    "    Feature_NaN_Counts = [[x[0], x[1], 0, 0] for x in Feature_NaN_Counts]\n",
    "    Feature_NaN_Counts = Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN)\n",
    "#    for row in Feature_NaN_Counts:\n",
    "#        print (row)\n",
    "#    print ()\n",
    "    \n",
    "    old_give = ''\n",
    "    old_take = ''\n",
    "    \n",
    "    stop = False\n",
    "    while stop == False:\n",
    "        if Feature_NaN_Counts[0][3] > -0.001 or Feature_NaN_Counts[-1][3] < 0.001:\n",
    "            stop = True\n",
    "\n",
    "        give_feature = Feature_NaN_Counts[0][0]\n",
    "        take_i = -1\n",
    "        take_feature = Feature_NaN_Counts[take_i][0]\n",
    "        nGive = int(round(-1/100 * Feature_NaN_Counts[0][3] * nRows,0))\n",
    "        nTake = int(round(1/100 * Feature_NaN_Counts[take_i][3] * nRows,0))\n",
    "        \n",
    "        mask = ((data_NaN[give_feature]==1) & (data_NaN[take_feature] == 0))\n",
    "        Swap = data_NaN[mask]\n",
    "        nSample = min([nGive, nTake, Swap.shape[0]])\n",
    "        print ('give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample')\n",
    "        print (give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample)\n",
    "        while nSample==0:\n",
    "            take_i = take_i - 1\n",
    "            take_feature = Feature_NaN_Counts[take_i][0]\n",
    "            nGive = int(round(-1/100 * Feature_NaN_Counts[0][3] * nRows,0))\n",
    "            nTake = int(round(1/100 * Feature_NaN_Counts[take_i][3] * nRows,0))\n",
    "            mask = ((data_NaN[give_feature]==1) & (data_NaN[take_feature] == 0))\n",
    "            Swap = data_NaN[mask]\n",
    "            nSample = min([nGive, nTake, Swap.shape[0]])\n",
    "            print ('give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample')\n",
    "            print (give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample)\n",
    "            print ()\n",
    "            if nSample==0 and take_i < -10:\n",
    "                break\n",
    "                stop = True\n",
    "        \n",
    "#        if nSample == 0:\n",
    "#            stop = True\n",
    "        Swap = Swap.sample(n=nSample)\n",
    "#        display(Swap[[give_feature, take_feature]])\n",
    "        mask = Swap.index.values.tolist()\n",
    "        for m in mask:\n",
    "            data_NaN.loc[[m], give_feature] = 0\n",
    "            data_NaN.loc[[m], take_feature] = 1\n",
    "            \n",
    "#        print ()\n",
    "#        data_NaN[give_feature],data_NaN[take_feature]=np.where(mask,(data_NaN[take_feature],data_NaN[give_feature]),(data_NaN[give_feature],data_NaN[take_feature]))\n",
    "        Feature_NaN_Counts = Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN)\n",
    "\n",
    "#    for row in Feature_NaN_Counts:\n",
    "#        print (row)\n",
    "#    print ()\n",
    "\n",
    "    data_NaN = data_NaN.sample(frac=1)\n",
    "#    print ('data_NaN')\n",
    "#    display(data_NaN.head(10))\n",
    "#    print ('data_NaN reindexed')\n",
    "    data_NaN.reset_index(inplace=True, drop=True)\n",
    " #   display(data_NaN.head(10))\n",
    " #   print ('data_Ground_Truth')\n",
    " #   display(data_Ground_Truth.head(10))\n",
    " #   print ('data_NaN')\n",
    " #   display(data_NaN.head(10))\n",
    "    data_NaN = data_Ground_Truth.where(data_NaN==0)\n",
    "#    print ('data_NaN')\n",
    "#    display(data_NaN.head(20))\n",
    "#    display(data_NaN.tail(20))\n",
    "    \n",
    "    display(data_NaN.isna().sum())\n",
    "#    print ('Finished Create_data_NaN_Method_3')\n",
    "\n",
    "    print ('Finished Create_data_NaN_Method_3()')\n",
    "    print ()\n",
    "    \n",
    "    return data_NaN\n",
    "    \n",
    "\n",
    "def Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN):\n",
    "    for row in Feature_NaN_Counts:\n",
    "        feature = row[0]\n",
    "        s = data_NaN[feature].sum()\n",
    "        n = len(data_NaN)\n",
    "#        print (feature, s, round(s/n*100,2))\n",
    "        row[2] = round(s/n*100,6)\n",
    "        row[3] = round(row[1] - row[2], 6)\n",
    "    Feature_NaN_Counts = sorted(Feature_NaN_Counts, key=lambda x:x[3])\n",
    "    \n",
    "    return Feature_NaN_Counts\n",
    "\n",
    "    \n",
    "def Test_Create_data_NaN_Method_3():\n",
    "    print ('Test_Create_data_NaN_Method_3()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "#    data_Ground_Truth.astype('Int64')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head(10))\n",
    "    Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "    print ('Finished Test_Create_data_NaN_Method_3')\n",
    "    print ()\n",
    "\n",
    "    \n",
    "#Test_Create_data_NaN_Method_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef777e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN(data_Ground_Truth):\n",
    "    print ('Create_data_NaN()')\n",
    "    \n",
    "    # Create a 2d list of the same shape as \"data\"\n",
    "    # Each row has \"1\" in 15% of the columns, \"0\" otherwise\n",
    "        # By \"15%,\" we mean that \"(# columns)*0.15\" rounded to the nearest integer.\n",
    "    # The first row is a random shuffle of such a row.\n",
    "    # The next (columns-1) rows are rotations of that row\n",
    "    # Each \"column\" number of rows, shuffle and repeat.\n",
    "    # Each row will have 15% of the samples 1, and each column will have 15% of the samples 1.\n",
    "    # Shuffle the rows.\n",
    "    # Then shuffling the columns would be redundant.\n",
    "    \n",
    "    rows = data_Ground_Truth.shape[0]\n",
    "    columns = data_Ground_Truth.shape[1]\n",
    "    drops_in_row = int(round(columns*0.15,0))\n",
    "    print ('drops_in_row = ', drops_in_row)\n",
    "    \n",
    "    Rand_Drop = []\n",
    "    Row = [1]*(drops_in_row) + [0]*(columns - drops_in_row)\n",
    "\n",
    "    for i in range (rows):\n",
    "        if i%3==0:\n",
    "            Row = [1]*(drops_in_row) + [0]*(columns - drops_in_row)\n",
    "        elif i%3==1:\n",
    "            Row = [1]*(drops_in_row + 1) + [0]*(columns - drops_in_row - 1)\n",
    "        else:\n",
    "            Row = [1]*(drops_in_row - 1) + [0]*(columns - drops_in_row + 1)\n",
    "        random.shuffle(Row)\n",
    "        Rand_Drop.append(Row.copy())\n",
    "\n",
    "#    for i in range (rows):\n",
    "#        if i%columns==0:\n",
    "#            random.shuffle(Row)\n",
    "#        Row.append(Row.pop(0))\n",
    "#        Rand_Drop.append(Row.copy())\n",
    "\n",
    "    random.shuffle(Rand_Drop)\n",
    "\n",
    "#    for i in range (columns):\n",
    "#        print (i, sum([x[i] for x in Rand_Drop]))\n",
    "\n",
    "    # Turn the 2d list into a dataframe\n",
    "        \n",
    "    Rand_Drop_df = pd.DataFrame(Rand_Drop, columns=data_Ground_Truth.columns)\n",
    "    display(Rand_Drop_df)\n",
    "    \n",
    "#    for feature in Rand_Drop_df:\n",
    "#        print (feature, Rand_Drop_df[feature].sum())\n",
    "\n",
    "    # Change the Ground Truth values to NaN where the corresponding value in Rand_Drop_df is 1\n",
    "    data_NaN = data_Ground_Truth.where(Rand_Drop_df==0)\n",
    "#    data_NaN = data_NaN.astype('Int')\n",
    "    \n",
    "    print ('data_NaN')\n",
    "    display(data_NaN)\n",
    "    \n",
    "    print ('data_NaN.isna().sum()')\n",
    "    display(data_NaN.isna().sum())\n",
    "    \n",
    "    print ('data_NaN.dropna().shape')\n",
    "    print (data_NaN.dropna().shape)\n",
    "    \n",
    "    print ('Finished Create_data_NaN()')\n",
    "    print ()\n",
    "\n",
    "    return data_NaN\n",
    "    \n",
    "    \n",
    "def Test_Create_data_NaN():\n",
    "    print ('Test_Create_data_NaN()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int32')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    Create_data_NaN(data_Ground_Truth)\n",
    "    \n",
    "    print ('Finished Test_Create_data_NaN()')\n",
    "    print ()\n",
    "\n",
    "#Test_Create_data_NaN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959fc06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN_Old(data_Ground_Truth):\n",
    "    \"\"\"\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('int64')\n",
    "\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth = data_Ground_Truth.astype('int64')\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly pick 15% of the values from each row\n",
    "    # and set them to be missing\n",
    "    print ('Remove 15% of values from each row')\n",
    "    frac = .15\n",
    "    data_NaN = data_Ground_Truth.copy(deep=True)\n",
    "    N = data_NaN.shape[0] * frac # Number of NaN in each feature\n",
    "    for c in data_NaN.columns:\n",
    "        idx = np.random.choice(a=data_NaN.index, size=int(len(data_NaN) * frac))\n",
    "        data_NaN.loc[idx, c] = np.NaN\n",
    "#    for feature in data_NaN:\n",
    "#        data_NaN[feature] = pd.to_numeric(data_NaN[feature])\n",
    "#    data_NaN.astype('int64')\n",
    "\n",
    "\n",
    "#    print ('data_NaN.shape')\n",
    "#    print (data_NaN.shape)\n",
    "#    display(data_NaN.head())\n",
    "    \n",
    "    data_NaN = data_NaN.astype('Int8')\n",
    "    \n",
    "#    data_NaN = data_NaN.sample(n=200000)\n",
    "#    print (data_NaN.shape)\n",
    "#    print (data_NaN.head(20))\n",
    "    \n",
    "    return data_NaN\n",
    "\n",
    "#Create_data_NaN_Old()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c72a63",
   "metadata": {},
   "source": [
    "# Compare Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25a5b4",
   "metadata": {},
   "source": [
    "## Mode Imputation\n",
    "## Random Forest Imputation\n",
    "## Prepare Data for IVEware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_Imputation_Methods_Part_1():\n",
    "    print ('Compare_Imputation_Methods_Part_1()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int32')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    data_Ground_Truth = data_Ground_Truth.sample(n=200000)\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print ('data_Ground_Truth.shape after resampling')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "\n",
    "    print ('Remove values from each row')\n",
    "    data_NaN = Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "    \n",
    "    print ('data_NaN.shape')\n",
    "    print (data_NaN.shape)\n",
    "    display(data_NaN.head(10))\n",
    "    display(data_NaN.tail(10))\n",
    "    \n",
    "    # Perform MissForest imputation\n",
    "    data_MF = data_NaN.copy(deep=True)\n",
    "#    data_MF = data_NaN_Old.copy(deep=True)\n",
    "#    data_MF = data_MF.astype('Int8')\n",
    "    print ('data_MF')\n",
    "    display(data_MF.head(20))\n",
    "    data_MF = Impute_MissForest(data_MF)\n",
    "    data_MF.sort_index(inplace=True)\n",
    "    data_MF = data_MF[data.columns]  \n",
    "#    data_MF = data_MF.astype('Int32')\n",
    "    \n",
    "    print ('data_MF.shape')\n",
    "    print (data_MF.shape)\n",
    "    display(data_MF.head())\n",
    "#    print ()\n",
    "\n",
    "    \n",
    "    # Create .txt file to feed into IVEware imputation\n",
    "    data_IVEware = data_NaN.copy(deep=True)\n",
    "#    data_IVEware = data_IVEware.astype('str')\n",
    "    data_IVEware = data_IVEware.fillna('')\n",
    "    data_IVEware.to_csv('../../Big_Files/data_IVEware.txt', sep='\\t', index=False)\n",
    "    \n",
    "    data_Mode = pd.DataFrame()\n",
    "    for feature in data_NaN:\n",
    "        data_Mode[feature] = data_NaN[feature].fillna(data_NaN[feature].mode()[0])\n",
    "    data_Mode = data_Mode.astype('Int32')\n",
    "    print ('data_Mode.shape')\n",
    "    print (data_Mode.shape)\n",
    "    display(data_Mode.head())\n",
    "    \n",
    "    # Perform Round Robin imputation using Random Forest Classifier\n",
    "    data_RF = Impute_Round_Robin(data_NaN)\n",
    "    data_RF.sort_index(inplace=True)\n",
    "    data_RF = data_RF[data.columns]  \n",
    "    data_RF = data_RF.astype('Int32')\n",
    "    \n",
    "    print ('data_RF.shape')\n",
    "    print (data_RF.shape)\n",
    "    display(data_RF.head())\n",
    "#    print ()\n",
    "\n",
    "    # Impute randomly\n",
    "    data_Random = data_NaN.copy(deep=True)\n",
    "    data_Random = Impute_Randomly(data_Random)\n",
    "    data_Random = data_Random.astype('Int32')\n",
    "    \n",
    "    print ('data_Random.shape')\n",
    "    print (data_Random.shape)\n",
    "    display(data_Random.head())\n",
    "#    print ()\n",
    "\n",
    "    print ('Finished Compare_Imputation_Methods_Part_1()')\n",
    "    print ()\n",
    "\n",
    "    return data_Ground_Truth, data_NaN, data_RF, data_MF, data_Mode, data_Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0ab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "data_Ground_Truth, data_NaN, data_RF, data_MF, data_Mode, data_Random = Compare_Imputation_Methods_Part_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c93a1",
   "metadata": {},
   "source": [
    "## Do IVEware Imputation (Outside this Jupyter Notebook)\n",
    "- Go to the IVEware folder and run (at the command line) IVE_12_22_22.bat\n",
    "- Requires scrlib and R.  You may need to, in the batch file, change the path to your scrlib installation.\n",
    "- Notes to self:\n",
    "    - Open srcshell\n",
    "    - From srcshell, open IVEware_CRSS_Imputation.xml\n",
    "    - Run\n",
    "- Run time: ./IVEware_CRSS_Imputation.bat  1069.08s user 12.92s system 98% cpu 18:23.92 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_IVEware_seed_0 = pd.read_csv('../../Big_Files/data_IVEware_Compare_seed_0.csv')\n",
    "data_IVEware_seed_0.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "data_IVEware_seed_1 = pd.read_csv('../../Big_Files/data_IVEware_Compare_seed_1.csv')\n",
    "data_IVEware_seed_1.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "print ('data_Ground_Truth', data_Ground_Truth.shape)\n",
    "display(data_Ground_Truth.head(10))\n",
    "print ('data_NaN', data_NaN.shape)\n",
    "display(data_NaN.head(10))\n",
    "print ('data_RF', data_RF.shape)\n",
    "display(data_RF.head(10))\n",
    "print ('data_MF', data_MF.shape)\n",
    "display(data_MF.head(10))\n",
    "print ('data_Mode', data_Mode.shape)\n",
    "display(data_Mode.head(10))\n",
    "print ('data_IVEware_seed_0', data_IVEware_seed_0.shape)\n",
    "display(data_IVEware_seed_0.head(10))\n",
    "print ('data_IVEware_seed_1', data_IVEware_seed_1.shape)\n",
    "display(data_IVEware_seed_1.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7745a",
   "metadata": {},
   "source": [
    "## Compare Six Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf238635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_Imputation_Methods_Part_2(\n",
    "    data_Ground_Truth, \n",
    "    data_NaN, \n",
    "    data_RF, \n",
    "    data_MF, \n",
    "    data_Mode, \n",
    "    data_Random, \n",
    "    data_IVEware_seed_0, \n",
    "    data_IVEware_seed_1\n",
    "):\n",
    "    print ('Compare_Imputation_Methods_Part_2')\n",
    "    \n",
    "    \"\"\"\n",
    "    print ('Drop Multicollinear Features')\n",
    "    Drop = ['MAX_VSEV', 'VE_FORMS', 'VTCONT_F', 'MAX_SEV', 'NUM_INJV']\n",
    "    DF = [data_Ground_Truth, data_NaN, data_RF, data_Mode, data_Random, data_IVEware]\n",
    "    \n",
    "    for df in DF:\n",
    "        for feature in Drop:\n",
    "            if feature in df:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "                print ('Drop ', feature)\n",
    "    print ()\n",
    "    \"\"\"\n",
    "    \n",
    "    Datasets = [\n",
    "        ['data_Ground_Truth', data_Ground_Truth],\n",
    "        ['data_NaN', data_NaN],\n",
    "        ['data_RF', data_RF],\n",
    "        ['data_MF', data_MF],\n",
    "        ['data_Mode', data_Mode],\n",
    "        ['data_Random', data_Random],\n",
    "        ['data_IVEware_seed_0', data_IVEware_seed_0],\n",
    "        ['data_IVEware_seed_1', data_IVEware_seed_1],\n",
    "    ]\n",
    "    \n",
    "    for dataset in Datasets:\n",
    "        name = dataset[0]\n",
    "        data = dataset[1]\n",
    "        print (name, '.shape: ', data.shape)\n",
    "    print ()\n",
    "    \n",
    "    for dataset in Datasets:\n",
    "        name = dataset[0]\n",
    "        data = dataset[1]\n",
    "        print (name)\n",
    "        display(data.head())\n",
    "    print ()\n",
    "    \n",
    "    Datasets = [\n",
    "        ['data_Ground_Truth', data_Ground_Truth],\n",
    "#        ['data_NaN', data_NaN],\n",
    "        ['data_RF', data_RF],\n",
    "        ['data_MF', data_MF],\n",
    "        ['data_Mode', data_Mode],\n",
    "        ['data_Random', data_Random],\n",
    "        ['data_IVEware_seed_0', data_IVEware_seed_0],\n",
    "        ['data_IVEware_seed_1', data_IVEware_seed_1],\n",
    "    ]\n",
    "    \n",
    "    A = []\n",
    "    for feature in data_NaN:\n",
    "        B = []\n",
    "        B.append(feature)\n",
    "        B.append(data_NaN[feature].isna().sum())\n",
    "        for i in range (len(Datasets)-1):\n",
    "            for j in range (i+1, len(Datasets)):\n",
    "                C = (Datasets[i][1][feature] != Datasets[j][1][feature]).sum()\n",
    "                D = round(C/B[1]*100,4)\n",
    "                B.append(C)\n",
    "#                print ('B[', len(B)-1, '] counts differences between ', Datasets[i][0], ' and ', Datasets[j][0], '.')\n",
    "                B.append(D)\n",
    "#                print ('B[', len(B)-1, '] gives the count as a percentage.')\n",
    "#        print ()\n",
    "        \n",
    "#        print (B)\n",
    "        A.append(B)\n",
    "    print ()\n",
    "    \n",
    "    A = sorted(A, key=lambda x:x[3])\n",
    "    B = pd.DataFrame(\n",
    "        A, \n",
    "        columns=[\n",
    "            'Feature', # 0\n",
    "            'nNaN',  # 1\n",
    "            'nRF Incorrect', 'pRF Incorrect', # 2, 3\n",
    "            'nMF Incorrect', 'pMF Incorrect', # 4, 5\n",
    "            'nMode Incorrect', 'pMode Incorrect', # 6, 7\n",
    "            'nRandom Incorrect', 'pRandom Incorrect', # 8, 9\n",
    "            'nIVEware_seed_0 Incorrect', 'pIVEware_seed_0 Incorrect', # 10, 11\n",
    "            'nIVEware_seed_1 Incorrect', 'pIVEware_seed_1 Incorrect', # 12, 13\n",
    "            'RF and MF Different', 'RF v/s MF %', # 14, 15\n",
    "            'RF and Mode Different', 'RF v/s Mode %', # 16, 17\n",
    "            'RF and Random Different', 'RF v/s Random %', # 18, 19\n",
    "            'RF and IVEware_seed_0 Different', 'RF v/s IVEware_seed_0 %', # 20, 21\n",
    "            'RF and IVEware_seed_1 Different', 'RF v/s IVEware_seed_1 %', # 22, 23\n",
    "            'MF and Mode Different', 'MF v/s Mode %', # 24, 25\n",
    "            'MF and Random Different', 'MF v/s Random %', # 26, 27\n",
    "            'MF and IVEware_seed_0 Different', 'MF v/s IVEware_seed_0 %', # 28, 29\n",
    "            'MF and IVEware_seed_1 Different', 'MF v/s IVEware_seed_1 %', # 30, 31\n",
    "            'Mode and Random Different', 'Mode v/s Random %', # 32, 33\n",
    "            'Mode and IVEware_seed_0 Different', 'Mode v/s IVEware_seed_0 %', #, 34, 35\n",
    "            'Mode and IVEware_seed_1 Different', 'Mode v/s IVEware_seed_1 %', #, 36, 37\n",
    "            'Random and IVEware_seed_0 Different', 'Random v/s IVEware_seed_0 %', # 38, 39\n",
    "            'Random and IVEware_seed_1 Different', 'Random v/s IVEware_seed_1 %', # 40, 41\n",
    "            'IVEware_seed_0 and IVEware_seed_1 Different', 'IVEware_seed_0 v/s IVEware_seed_1 %', # 42, 43\n",
    "        ]\n",
    "    )\n",
    "    display(B)\n",
    "    a = sum([x[1] for x in A]) # nNaN\n",
    "    b = sum([x[2] for x in A]) # nRF Incorrect\n",
    "    c = sum([x[4] for x in A]) # nMF Incorrect\n",
    "    d = sum([x[6] for x in A]) # nMode INcorrect\n",
    "    e = sum([x[8] for x in A]) # nRandom Incorrect\n",
    "    f = sum([x[10] for x in A]) # nIVEware_seed_0 Incorrect\n",
    "    g = sum([x[12] for x in A]) # nIVEware_seed_1 Incorrect\n",
    "    h = round(b/a*100,2)\n",
    "    i = round(c/a*100,2)\n",
    "    j = round(d/a*100,2)\n",
    "    k = round(e/a*100,2)\n",
    "    l = round(f/a*100,2)\n",
    "    m = round(g/a*100,2)\n",
    "\n",
    "    RF_less_MF = sum([x[2] < x[4] for x in A])\n",
    "    RF_equal_MF = sum([x[2] == x[4] for x in A])\n",
    "    RF_greater_MF = sum([x[2] > x[4] for x in A])\n",
    "    \n",
    "    RF_less_Mode = sum([x[2] < x[6] for x in A])\n",
    "    RF_equal_Mode = sum([x[2] == x[6] for x in A])\n",
    "    RF_greater_Mode = sum([x[2] > x[6] for x in A])\n",
    "\n",
    "    RF_less_Random = sum([x[2] < x[8] for x in A])\n",
    "    RF_equal_Random = sum([x[2] == x[8] for x in A])\n",
    "    RF_greater_Random = sum([x[2] > x[8] for x in A])\n",
    "\n",
    "    RF_less_IVEware_seed_0 = sum([x[2] < x[10] for x in A])\n",
    "    RF_equal_IVEware_seed_0 = sum([x[2] == x[10] for x in A])\n",
    "    RF_greater_IVEware_seed_0 = sum([x[2] > x[10] for x in A])\n",
    "\n",
    "    RF_less_IVEware_seed_1 = sum([x[2] < x[12] for x in A])\n",
    "    RF_equal_IVEware_seed_1 = sum([x[2] == x[12] for x in A])\n",
    "    RF_greater_IVEware_seed_1 = sum([x[2] > x[12] for x in A])\n",
    "\n",
    "    MF_less_Mode = sum([x[4] < x[6] for x in A])\n",
    "    MF_equal_Mode = sum([x[4] == x[6] for x in A])\n",
    "    MF_greater_Mode = sum([x[4] > x[6] for x in A])\n",
    "\n",
    "    MF_less_Random = sum([x[4] < x[8] for x in A])\n",
    "    MF_equal_Random = sum([x[4] == x[8] for x in A])\n",
    "    MF_greater_Random = sum([x[4] > x[8] for x in A])\n",
    "\n",
    "    MF_less_IVEware_seed_0 = sum([x[4] < x[10] for x in A])\n",
    "    MF_equal_IVEware_seed_0 = sum([x[4] == x[10] for x in A])\n",
    "    MF_greater_IVEware_seed_0 = sum([x[4] > x[10] for x in A])\n",
    "\n",
    "    MF_less_IVEware_seed_1 = sum([x[4] < x[12] for x in A])\n",
    "    MF_equal_IVEware_seed_1 = sum([x[4] == x[12] for x in A])\n",
    "    MF_greater_IVEware_seed_1 = sum([x[4] > x[12] for x in A])\n",
    "\n",
    "    Mode_less_Random = sum([x[6] < x[8] for x in A])\n",
    "    Mode_equal_Random = sum([x[6] == x[8] for x in A])\n",
    "    Mode_greater_Random = sum([x[6] > x[8] for x in A])\n",
    "\n",
    "    Mode_less_IVEware_seed_0 = sum([x[6] < x[10] for x in A])\n",
    "    Mode_equal_IVEware_seed_0 = sum([x[6] == x[10] for x in A])\n",
    "    Mode_greater_IVEware_seed_0 = sum([x[6] > x[10] for x in A])\n",
    "\n",
    "    Mode_less_IVEware_seed_1 = sum([x[6] < x[12] for x in A])\n",
    "    Mode_equal_IVEware_seed_1 = sum([x[6] == x[12] for x in A])\n",
    "    Mode_greater_IVEware_seed_1 = sum([x[6] > x[12] for x in A])\n",
    "\n",
    "    Random_less_IVEware_seed_0 = sum([x[8] < x[10] for x in A])\n",
    "    Random_equal_IVEware_seed_0 = sum([x[8] == x[10] for x in A])\n",
    "    Random_greater_IVEware_seed_0 = sum([x[8] > x[10] for x in A])\n",
    "\n",
    "    Random_less_IVEware_seed_1 = sum([x[8] < x[12] for x in A])\n",
    "    Random_equal_IVEware_seed_1 = sum([x[8] == x[12] for x in A])\n",
    "    Random_greater_IVEware_seed_1 = sum([x[8] > x[12] for x in A])\n",
    "\n",
    "    IVEware_seed_0_less_IVEware_seed_1 = sum([x[10] < x[12] for x in A])\n",
    "    IVEware_seed_0_equal_IVEware_seed_1 = sum([x[10] == x[12] for x in A])\n",
    "    IVEware_seed_0_greater_IVEware_seed_1 = sum([x[10] > x[12] for x in A])\n",
    "\n",
    "    print ()\n",
    "    print ('    | | Number | Percentage |')\n",
    "    print ('    | --- | --- | --- | ')    \n",
    "    print ('    | Total NaN | ', f'{a:,d}', ' | 100% | ')\n",
    "    print ('    | RF | ', f'{b:,d}', ' | ', h, '% | ')\n",
    "    print ('    | MF | ', f'{c:,d}', ' | ', i, '% | ')\n",
    "    print ('    | Mode | ', f'{d:,d}', ' | ', j, '% | ')\n",
    "    print ('    | Random | ', f'{e:,d}', ' | ', k, '% | ')\n",
    "    print ('    | IVEware_seed_0 | ', f'{f:,d}', ' | ', l, '% | ')\n",
    "    print ('    | IVEware_seed_1 | ', f'{g:,d}', ' | ', m, '% | ')\n",
    "    print ()\n",
    "    print ('    |  | Fewer | Equal | More | Total | ')\n",
    "    print ('    | --- | --- | --- | --- | --- | ')\n",
    "    print ('    | Compare RF to MF | ', RF_less_MF, ' | ', RF_equal_MF,  ' | ' ,RF_greater_MF,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to Mode | ', RF_less_Mode, ' | ', RF_equal_Mode,  ' | ' ,RF_greater_Mode,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to Random | ', RF_less_Random, ' | ' , RF_equal_Random,  ' | ' , RF_greater_Random,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to IVEware_seed_0 | ', RF_less_IVEware_seed_0, ' | ' , RF_equal_IVEware_seed_0, ' | ' , RF_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to IVEware_seed_0 | ', RF_less_IVEware_seed_1, ' | ' , RF_equal_IVEware_seed_1, ' | ' , RF_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to Mode | ', MF_less_Mode, ' | ', MF_equal_Mode,  ' | ' ,MF_greater_Mode,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to Random | ', MF_less_Random, ' | ' , MF_equal_Random,  ' | ' , MF_greater_Random,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to IVEware_seed_0 | ', MF_less_IVEware_seed_0, ' | ' , MF_equal_IVEware_seed_0, ' | ' , MF_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to IVEware_seed_1 | ', MF_less_IVEware_seed_1, ' | ' , MF_equal_IVEware_seed_1, ' | ' , MF_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to Random | ', Mode_less_Random, ' | ' , Mode_equal_Random, ' | ' , Mode_greater_Random, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to IVEware_seed_0 | ', Mode_less_IVEware_seed_0, ' | ' , Mode_equal_IVEware_seed_0, ' | ' , Mode_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to IVEware_seed_1 | ', Mode_less_IVEware_seed_1, ' | ' , Mode_equal_IVEware_seed_1, ' | ' , Mode_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Random to IVEware_seed_0 | ', Random_less_IVEware_seed_0, ' | ' , Random_equal_IVEware_seed_0, ' | ' , Random_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Random to IVEware_seed_1 | ', Random_less_IVEware_seed_1, ' | ' , Random_equal_IVEware_seed_1, ' | ' , Random_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ()\n",
    "    \n",
    "    b = sum([x[14] for x in A])\n",
    "    c = sum([x[16] for x in A])\n",
    "    d = sum([x[18] for x in A])\n",
    "    e = sum([x[20] for x in A])\n",
    "    f = sum([x[22] for x in A])\n",
    "    g = sum([x[24] for x in A])\n",
    "    h = sum([x[26] for x in A])\n",
    "    i = sum([x[28] for x in A])\n",
    "    j = sum([x[30] for x in A])\n",
    "    k = sum([x[32] for x in A])\n",
    "    l = sum([x[34] for x in A])\n",
    "    m = sum([x[36] for x in A])\n",
    "    n = sum([x[38] for x in A])\n",
    "    o = sum([x[40] for x in A])\n",
    "    p = sum([x[42] for x in A])\n",
    "    \n",
    "    print ('    |  | Number |  Percentage |')\n",
    "    print ('    | --- | --- | -- |')\n",
    "    print ('    | Total NaN | ', f'{a:,d}', ' | 100% |' )\n",
    "    print ('    | RF Different from MF | ', f'{b:,d}', ' | ', round(b/a*100,2), '% |')\n",
    "    print ('    | RF Different from Mode | ', f'{c:,d}', ' | ', round(c/a*100,2), '% |')\n",
    "    print ('    | RF Different from Random | ', f'{d:,d}', ' | ', round(d/a*100,2), '% |')\n",
    "    print ('    | RF Different from IVEware_seed_0 | ', f'{e:,d}', ' | ', round(e/a*100,2), '% |')\n",
    "    print ('    | RF Different from IVEware_seed_1 | ', f'{f:,d}', ' | ', round(f/a*100,2), '% |')\n",
    "    print ('    | MF Different from Mode | ', f'{g:,d}', ' | ', round(g/a*100,2), '% |')\n",
    "    print ('    | MF Different from Random | ', f'{h:,d}', ' | ',  round(h/a*100,2), '% |')\n",
    "    print ('    | MF Different from IVEware_seed_0 | ', f'{i:,d}', ' | ',  round(i/a*100,2), '% |')\n",
    "    print ('    | MF Different from IVEware_seed_1 | ', f'{j:,d}', ' | ', round(j/a*100,2), '% |')\n",
    "    print ('    | Mode Different from Random | ', f'{k:,d}', ' | ', round(k/a*100,2), '% |')\n",
    "    print ('    | Mode Different from IVEware_seed_0 | ', f'{l:,d}', ' | ', round(l/a*100,2), '% |')\n",
    "    print ('    | Mode Different from IVEware_seed_1 | ', f'{m:,d}', ' | ', round(m/a*100,2), '% |')\n",
    "    print ('    | Random Different from IVEware_seed_0 | ', f'{n:,d}', ' | ', round(n/a*100,2), '% |')\n",
    "    print ('    | Random Different from IVEware_seed_1 | ', f'{o:,d}', ' | ', round(o/a*100,2), '% |')\n",
    "    print ('    | IVEware_seed_0 Different from IVEware_seed_1 | ', f'{p:,d}', ' | ', round(p/a*100,2), '% |')\n",
    "    print ()\n",
    "        \n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "    \n",
    "    print ('Finished Compare_Imputation_Methods_Part_2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fa807",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Compare_Imputation_Methods_Part_2(\n",
    "    data_Ground_Truth, data_NaN, \n",
    "    data_RF, data_MF, \n",
    "    data_Mode, data_Random, \n",
    "    data_IVEware_seed_0, data_IVEware_seed_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03ad4a",
   "metadata": {},
   "source": [
    "# Impute using Random Forest and Save for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Impute_Using_Random_Forest():\n",
    "    print ('Impute_Using_Random_Forest()')\n",
    "    data = Get_Data()\n",
    "    \n",
    "#    data_Imputed = Impute_Full(data)\n",
    "    data_Imputed = Impute_Round_Robin(data)\n",
    "    data_Imputed.to_csv('../../Big_Files/CRSS_Imputed_by_RF_Data.csv', index=False)\n",
    "#    display(data_Imputed.head(50))\n",
    "    \n",
    "    Check(data, data_Imputed)\n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "\n",
    "    print ('Finished Impute_Using_Random_Forest()')\n",
    "    print ()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "Impute_Using_Random_Forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18739d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Using_MissForest():\n",
    "    print ('Impute_Using_MissForest()')\n",
    "    data = Get_Data()\n",
    "    \n",
    "#    data_Imputed = Impute_Full(data)\n",
    "    data_Imputed = Impute_MissForest(data)\n",
    "    data_Imputed.to_csv('../../Big_Files/CRSS_Imputed_by_MF_Data.csv', index=False)\n",
    "#    display(data_Imputed.head(50))\n",
    "    \n",
    "    Check(data, data_Imputed)\n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "\n",
    "    print ('Finished Impute_Using_MissForest()')\n",
    "    print ()\n",
    "\n",
    "    return 0\n",
    "\n",
    "Impute_Using_Random_Forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af31f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Using_IVEware():\n",
    "    print ('Impute_Using_IVEware()')\n",
    "    data = Get_Data()\n",
    "    \n",
    "    # Create .txt file to feed into IVEware imputation\n",
    "    data_IVEware = data.copy(deep=True)\n",
    "    print (data_IVEware.shape)\n",
    "    display(data_IVEware.head(10))\n",
    "    \n",
    "    data_IVEware = data_IVEware.replace(99,'')\n",
    "    display(data_IVEware.head(10))\n",
    "    data_IVEware.to_csv('../../Big_Files/data_IVEware.txt', sep='\\t', index=False)\n",
    "    \n",
    "    print ('Finished Impute_Using_IVEware()')\n",
    "    print ()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "Impute_Using_IVEware()\n",
    "# About one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
