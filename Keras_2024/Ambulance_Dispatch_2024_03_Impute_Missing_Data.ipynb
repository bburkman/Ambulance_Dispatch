{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edb2407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c5248",
   "metadata": {},
   "source": [
    "# Ambulance_Dispatch_2024_03_Impute_Missing_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa289b",
   "metadata": {},
   "source": [
    "# readme\n",
    "- Most of our other Jupyter Notebooks have a main() function at the bottom that runs everything.  \n",
    "- This notebook is structured differently, with several functions that run in sequence.  \n",
    "- The reason for the difference is that part of the work has to be done outside this notebook.  \n",
    "- The IVEware imputation software is available in several languages, but not Python.  We ran it in R using scrlib.  \n",
    "- This notebook prepares the data for the Mode, Random Forest, and IVEware imputations, and does the first two.  Then the user must separately run the IVEware software.  Finally, this notebook pulls in those results and compares the three methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b7cd9",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484970b",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- We have about 3% of the values in the dataset missing.  \n",
    "- CRSS used IVEware to impute missing values in some, but not most, of the features.  \n",
    "- We can use IVEware to impute the rest of the features, but we should compare to other methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433aa44",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- We have the discretized CRSS dataset in '../../Big_Files/CRSS_Binned_Data.csv'.\n",
    "- The dataset is 802,700 samples with 67 features.\n",
    "- In that dataset, each feature has values in {0,1,2,3,4,5,6,7,8,9,99}, with most features having fewer values and 99 signifying \"Missing\" or \"Unknown.\"\n",
    "- Overall, about 3% of the values are 99, \n",
    "    - In the features, thirteen features have no missing values, and six features have more than ten percent missing, the highest being RELJCT1 with 18% missing.  \n",
    "    - In the rows, 29% have no missing values, 25% one missing value, 16% two, ..., 1% eight, ..., and 0.3% thirteen missing values.  \n",
    "    - See results of the Analyze_Data() function for full details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191d5fd",
   "metadata": {},
   "source": [
    "## Test Method\n",
    "- Start with the binned dataset with 802,700 samples in 67 features, and analyze the distribution of NaN.\n",
    "    - In the binned dataset, \"Missing\" is represented as 99, but for clarity we will call it NaN.  Also, many things we want to do are Pandas functions for NaN, so we often convert the 99 to NaN.  \n",
    "    - For each feature, find the proportion of samples missing.  For the full list, run Analyze_Data() below.\n",
    "    - Note that we previously dropped all features with more than 20% of samples missing.\n",
    "\n",
    "    | Feature | NaN % |\n",
    "    |---|---|\n",
    "    | HOSPITAL | 0.00% |\n",
    "    | ACC_TYPE | 9.82% |\n",
    "    | AGE | 4.11% |\n",
    "    | AIR_BAG | 6.47% |\n",
    "    | ALC_STATUS | 16.49% |\n",
    "    | BODY_TYP | 2.65% |\n",
    "    | $\\vdots$ | $\\vdots$ |\n",
    "    | VTRAFCON | 8.56% |\n",
    "    | VTRAFWAY | 15.53% |\n",
    "    | WEATHER | 3.33% |\n",
    "    | WRK_ZONE | 0.00% |\n",
    "    \n",
    "    - For each sample, find the number of samples missing.\n",
    "    \n",
    "\n",
    "    | Number of NaN in Sample | % of Dataset |\n",
    "    |---|---|\n",
    "    | 0 | 28.94% |\n",
    "    | 1 | 24.66% |\n",
    "    | 2 | 16.04% |\n",
    "    | 3 | 9.92% |\n",
    "    | 4 | 6.46% |\n",
    "    | 5 | 4.56% |\n",
    "    | 6 | 3.18% |\n",
    "    | 7 | 2.18% |\n",
    "    | 8 | 1.40% |\n",
    "    | 9 | 0.89% |\n",
    "    | 10 | 0.59% |\n",
    "    | 11 | 0.45% |\n",
    "    | 12 | 0.38% |\n",
    "    | 13 | 0.35% |\n",
    "\n",
    "- Drop all of the samples with any NaN, leaving 28.94% of the dataset, 232,333 samples.\n",
    "    - The reason to test with a dataset with known values is to know whether the imputation method has imputed the missing values correctly.  \n",
    "    - Call this dataframe data_Ground_Truth.\n",
    "- Make the test dataset, data_NaN, by dropping values from data_Ground_Truth in proportion as the original dataset. \n",
    "    - Each feature in data_NaN has the same proportion of NaN as in that feature in the binned data.\n",
    "    - The same proportion of rows have no NaN (28.94%), one NaN (24.66), ..., thirteen NaN (0.35%).\n",
    "    - Details below with the Make_data_NaN_Method_3() function.\n",
    "- For each of the six methods:\n",
    "    - Impute missing values on data_NaN.\n",
    "    - By comparing with data_Ground_Truth, count the number of NaN imputed incorrectly.\n",
    "- For each pair of methods:\n",
    "    - Compare by feature which method did better.\n",
    "    - Count the number of differently imputed values, to see which methods give similar results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eada507",
   "metadata": {},
   "source": [
    "## Imputation Methods\n",
    "- We compare here six methods:\n",
    "    - MissForest, implemented in Python\n",
    "        -  https://pypi.org/project/MissForest/, version 2.5.5\n",
    "        - We made significant modifications to optimize memory usage and correct what we think is a mistake in the logic\n",
    "        - Using scikit-learn's random forest classifier and regressor with the default options\n",
    "    - Our Round-Robin Random Forest \n",
    "        - Our own implementation of the MissForest method\n",
    "        - We wrote it to help us understand the MissForest method, so we could understand how to get the above Python implementation to work\n",
    "        - Written just for categorical features\n",
    "        - Only does one iteration\n",
    "    - IVEware with random seed 1\n",
    "        - Using the hyperparameters given in the CRSS Imputation report, \n",
    "            - minrsqd 0.01;\n",
    "            - maxpred 15;\n",
    "            - \"The minimum marginal r-squared required for a predictor to be included in the model is set to 0.01. The maximum number of predictors in a model 15.\"\n",
    "            - Footnote #6 on page 8\n",
    "            - Herbert, G. C. (2019, September). Crash Report Sampling System: Imputation (Report No. DOT HS 812 795). Washington, DC: National Highway Traffic Safety Administration.\n",
    "    - IVEware with random seed 0\n",
    "        - \"A zero seed will result in no perturbations of the predicted values or the regression coefficients,\" according to the IVEware documentation.\n",
    "        - We discovered by accident that a random seed of 0 makes a huge difference, in the case of our dataset a difference for the better.  \n",
    "        - In Python and R, zero is a perfectly acceptable random seed, and in Python code is the most common choice of random seed, but in another language in which IVEware can be implemented, SAS, random seeds have to be positive, which may explain why the IVEware authors chose ``seed 0;`` as the switch to turn off perturbations.\n",
    "    - Imputation by mode\n",
    "        - We include this as a control.  \n",
    "        - Both MissForest and our Round-Robin Random Forest start with imputation by mode and are supposed to improve on it.\n",
    "        - If the results of either MissForest or our Round-Robin Random Forest are not significantly better than those of imputation by mode, something has gone wrong.\n",
    "    - Random Imputation\n",
    "        - With the distribution of the values in each feature matching the values in the original dataset\n",
    "        - We include this as another control.  Anything should be better than this.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ec94a",
   "metadata": {},
   "source": [
    "# Results of Comparison of Six Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614afad2",
   "metadata": {},
   "source": [
    "- We start with the binned (discretized) data, CRSS_Binned_Data.csv, with 817,623 samples in 67 features.\n",
    "<br><br>\n",
    "- Dropping any sample with a missing value, we have 232,333 samples of Ground Truth.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "- First run with random seed  0 in Python and NumPy, and both 0 and 1 as random seeds for IVEware, 7/3/24\n",
    "    <br><br>\n",
    "    - Samples Incorrectly Imputed\n",
    "    \n",
    "   \n",
    "\n",
    "   | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  484,710  | 100% | \n",
    "    | RF |  130,045  |  26.83 % | \n",
    "    | MF |  80,995  |  16.71 % | \n",
    "    | Mode |  173,102  |  35.71 % | \n",
    "    | Random |  226,013  |  46.63 % | \n",
    "    | IVEware_seed_0 |  105,212  |  21.71 % | \n",
    "    | IVEware_seed_1 |  158,163  |  32.63 % | \n",
    "\n",
    "   \n",
    "    <br><br>\n",
    "    - Comparison of number of errors in the 67 features.  For instance, comparing my Random Forest Round-Robin method to MissForest, in 1 feature RF had fewer errors than MissForest, in 18 features the two methods had the same number of errors, and in 48 features RF had more errors than MissForest.  \n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  2  |  13  |  52  | 67  |\n",
    "    | Compare RF to Mode |  41  |  26  |  0  | 67  |\n",
    "    | Compare RF to Random |  56  |  8  |  3  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  5  |  19  |  43  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  29  |  15  |  23  | 67  |\n",
    "    | Compare MF to Mode |  55  |  10  |  2  | 67  |\n",
    "    | Compare MF to Random |  57  |  10  |  0  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  44  |  17  |  6  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  52  |  15  |  0  | 67  |\n",
    "    | Compare Mode to Random |  52  |  10  |  5  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  1  |  19  |  47  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  19  |  13  |  35  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  2  |  9  |  56  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  6  |  8  |  53  | 67  |\n",
    "\n",
    "    <br><br>\n",
    "     - Number of NaN Imputed Differently by Different Methods\n",
    "         - Smaller numbers mean the two methods give similar results\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  484,710  | 100% |\n",
    "    | RF Different from MF |  86,478  |  17.84 % |\n",
    "    | RF Different from Mode |  68,474  |  14.13 % |\n",
    "    | RF Different from Random |  188,373  |  38.86 % |\n",
    "    | RF Different from IVEware_seed_0 |  58,420  |  12.05 % |\n",
    "    | RF Different from IVEware_seed_1 |  154,275  |  31.83 % |\n",
    "    | MF Different from Mode |  136,683  |  28.2 % |\n",
    "    | MF Different from Random |  209,244  |  43.17 % |\n",
    "    | MF Different from IVEware_seed_0 |  63,811  |  13.16 % |\n",
    "    | MF Different from IVEware_seed_1 |  139,804  |  28.84 % |\n",
    "    | Mode Different from Random |  173,284  |  35.75 % |\n",
    "    | Mode Different from IVEware_seed_0 |  113,233  |  23.36 % |\n",
    "    | Mode Different from IVEware_seed_1 |  191,622  |  39.53 % |\n",
    "    | Random Different from IVEware_seed_0 |  203,925  |  42.07 % |\n",
    "    | Random Different from IVEware_seed_1 |  232,558  |  47.98 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  128,807  |  26.57 % |\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "- Second Run, Same random seed (0) to make sure the random seed is implemented correctly.  Same results. \n",
    "\n",
    "    <br><br>\n",
    "     - Percentage of Samples Incorrectly Imputed\n",
    "     \n",
    "   | | Number | NaN Imputed Incorrectly |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  484,710  | 100% | \n",
    "    | RF |  130,045  |  26.83 % | \n",
    "    | MF |  80,995  |  16.71 % | \n",
    "    | Mode |  173,102  |  35.71 % | \n",
    "    | Random |  226,013  |  46.63 % | \n",
    "    | IVEware_seed_0 |  105,212  |  21.71 % | \n",
    "    | IVEware_seed_1 |  158,163  |  32.63 % | \n",
    "\n",
    "\n",
    "    <br><br>\n",
    "     - Percentage of Samples Correctly Imputed\n",
    "     \n",
    "    | | Number | NaN Imputed Correctly |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  484,710  |  | \n",
    "    | RF |  130,045  |  73.17 % | \n",
    "    | MF |  80,995  |  83.29 % | \n",
    "    | Mode |  173,102  |  64.29 % | \n",
    "    | Random |  226,013  |  53.37 % | \n",
    "    | IVEware_seed_0 |  105,212  |  78.29 % | \n",
    "    | IVEware_seed_1 |  158,163  |  67.37 % | \n",
    "\n",
    "\n",
    "    <br><br>\n",
    "     - Comparison of number of errors in the 67 features:\n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  2  |  13  |  52  | 67  |\n",
    "    | Compare RF to Mode |  41  |  26  |  0  | 67  |\n",
    "    | Compare RF to Random |  56  |  8  |  3  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  5  |  19  |  43  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  29  |  15  |  23  | 67  |\n",
    "    | Compare MF to Mode |  55  |  10  |  2  | 67  |\n",
    "    | Compare MF to Random |  57  |  10  |  0  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  44  |  17  |  6  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  52  |  15  |  0  | 67  |\n",
    "    | Compare Mode to Random |  52  |  10  |  5  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  1  |  19  |  47  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  19  |  13  |  35  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  2  |  9  |  56  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  6  |  8  |  53  | 67  |\n",
    "\n",
    "\n",
    "    <br><br>\n",
    "     - Number of NaN Imputed Differently by Different Methods\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  484,710  | 100% |\n",
    "    | RF Different from MF |  86,478  |  17.84 % |\n",
    "    | RF Different from Mode |  68,474  |  14.13 % |\n",
    "    | RF Different from Random |  188,373  |  38.86 % |\n",
    "    | RF Different from IVEware_seed_0 |  58,420  |  12.05 % |\n",
    "    | RF Different from IVEware_seed_1 |  154,275  |  31.83 % |\n",
    "    | MF Different from Mode |  136,683  |  28.2 % |\n",
    "    | MF Different from Random |  209,244  |  43.17 % |\n",
    "    | MF Different from IVEware_seed_0 |  63,811  |  13.16 % |\n",
    "    | MF Different from IVEware_seed_1 |  139,804  |  28.84 % |\n",
    "    | Mode Different from Random |  173,284  |  35.75 % |\n",
    "    | Mode Different from IVEware_seed_0 |  113,233  |  23.36 % |\n",
    "    | Mode Different from IVEware_seed_1 |  191,622  |  39.53 % |\n",
    "    | Random Different from IVEware_seed_0 |  203,925  |  42.07 % |\n",
    "    | Random Different from IVEware_seed_1 |  232,558  |  47.98 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  128,807  |  26.57 % |\n",
    "\n",
    "<br><br>\n",
    "- Third run, with random seed 42 in Python and Numpy, with both 0 and 1 as random seeds for IVEware.\n",
    "    - Note that the IVEware results are different in this run than in the previous runs, even though we used the same random seeds in IVEware.  The reason for the change is the different seed for Python and NumPy, which changed which values were missing in the dataset that we fed into IVEware.  \n",
    "\n",
    "    <br><br>\n",
    "    - Samples Incorrectly Imputed by Method\n",
    "    \n",
    "    | | Number | NaN Imputed Incorrectly |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  484,712  | 100% | \n",
    "    | RF |  131,564  |  27.14 % | \n",
    "    | MF |  80,422  |  16.59 % | \n",
    "    | Mode |  173,078  |  35.71 % | \n",
    "    | Random |  225,585  |  46.54 % | \n",
    "    | IVEware_seed_0 |  104,722  |  21.6 % | \n",
    "    | IVEware_seed_1 |  141,782  |  29.25 % | \n",
    "\n",
    "\n",
    "\n",
    "    <br><br>\n",
    "- Comparison of number of errors in the 67 features:\n",
    "\n",
    "    | | Number | NaN Imputed Correctly |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  484,712  |  | \n",
    "    | RF |  131,564  |  72.86 % | \n",
    "    | MF |  80,422  |  83.41 % | \n",
    "    | Mode |  173,078  |  64.29 % | \n",
    "    | Random |  225,585  |  53.46 % | \n",
    "    | IVEware_seed_0 |  104,722  |  78.4 % | \n",
    "    | IVEware_seed_1 |  141,782  |  70.75 % | \n",
    "\n",
    "    <br><br>\n",
    "- Number of NaN Imputed Differently by Pairs of Methods\n",
    "\n",
    "    |  | Fewer | Equal | More | Total | \n",
    "    | --- | --- | --- | --- | --- | \n",
    "    | Compare RF to MF |  1  |  14  |  52  | 67  |\n",
    "    | Compare RF to Mode |  41  |  25  |  1  | 67  |\n",
    "    | Compare RF to Random |  56  |  10  |  1  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  2  |  21  |  44  | 67  |\n",
    "    | Compare RF to IVEware_seed_0 |  28  |  12  |  27  | 67  |\n",
    "    | Compare MF to Mode |  55  |  11  |  1  | 67  |\n",
    "    | Compare MF to Random |  57  |  8  |  2  | 67  |\n",
    "    | Compare MF to IVEware_seed_0 |  43  |  19  |  5  | 67  |\n",
    "    | Compare MF to IVEware_seed_1 |  53  |  14  |  0  | 67  |\n",
    "    | Compare Mode to Random |  54  |  12  |  1  | 67  |\n",
    "    | Compare Mode to IVEware_seed_0 |  1  |  18  |  48  | 67  |\n",
    "    | Compare Mode to IVEware_seed_1 |  21  |  8  |  38  | 67  |\n",
    "    | Compare Random to IVEware_seed_0 |  1  |  10  |  56  | 67  |\n",
    "    | Compare Random to IVEware_seed_1 |  5  |  10  |  52  | 67  |\n",
    "\n",
    "    |  | Number |  Percentage |\n",
    "    | --- | --- | -- |\n",
    "    | Total NaN |  484,712  | 100% |\n",
    "    | RF Different from MF |  88,045  |  18.16 % |\n",
    "    | RF Different from Mode |  66,204  |  13.66 % |\n",
    "    | RF Different from Random |  187,311  |  38.64 % |\n",
    "    | RF Different from IVEware_seed_0 |  61,250  |  12.64 % |\n",
    "    | RF Different from IVEware_seed_1 |  137,491  |  28.37 % |\n",
    "    | MF Different from Mode |  137,294  |  28.32 % |\n",
    "    | MF Different from Random |  209,192  |  43.16 % |\n",
    "    | MF Different from IVEware_seed_0 |  63,481  |  13.1 % |\n",
    "    | MF Different from IVEware_seed_1 |  121,428  |  25.05 % |\n",
    "    | Mode Different from Random |  173,048  |  35.7 % |\n",
    "    | Mode Different from IVEware_seed_0 |  113,640  |  23.44 % |\n",
    "    | Mode Different from IVEware_seed_1 |  173,642  |  35.82 % |\n",
    "    | Random Different from IVEware_seed_0 |  203,859  |  42.06 % |\n",
    "    | Random Different from IVEware_seed_1 |  226,543  |  46.74 % |\n",
    "    | IVEware_seed_0 Different from IVEware_seed_1 |  110,441  |  22.78 % |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e79629",
   "metadata": {},
   "source": [
    "## Drop Multicollinear Features before Imputing?  Compare two methods\n",
    "- First Method\n",
    "    - After Binning, reduce dimensionality\n",
    "        - Removes MAX_VSEV, VE_FORMS, VTCONT_F, MAX_SEV, NUM_INJV\n",
    "        - Reduces from 67 to 62 features\n",
    "    - Impute\n",
    "- Second Method\n",
    "    - Impute with all 67 features\n",
    "    - Before evaluating the imputation, remove the five features and only evaluate the results on the 62 features used in the comparison above\n",
    "- We used random seed 42 for both methods\n",
    "- First Method Results\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,007,463  | 100% | \n",
    "    | RF |  569,509  |  28.37 % | \n",
    "    | Mode |  681,753  |  33.96 % | \n",
    "    | Random |  889,794  |  44.32 % | \n",
    "    | IVEware |  606,632  |  30.22 % | \n",
    "    \n",
    "- Second Method Results\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,007,235  | 100% | \n",
    "    | RF |  558,936  |  27.85 % | \n",
    "    | Mode |  681,996  |  33.98 % | \n",
    "    | Random |  888,845  |  44.28 % | \n",
    "    | IVEware |  606,062  |  30.19 % | \n",
    "\n",
    "\n",
    "### Analysis\n",
    "- Mode was the same, as it should be.\n",
    "- Random was slightly different, perhaps because the features were in a different order?\n",
    "- IVEware was not significantly different in the two methods.\n",
    "- Random Forest was slightly but significantly better (0.52%) with the second method, not removing the multicollinear features before imputing, which is surprising.  \n",
    "\n",
    "### Conclusion\n",
    "- Run again with different random seed = 1\n",
    "\n",
    "### Second Round Results\n",
    "- First Method\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,006,643  | 100% | \n",
    "    | RF |  568,909  |  28.35 % | \n",
    "    | Mode |  681,061  |  33.94 % | \n",
    "    | Random |  889,048  |  44.31 % | \n",
    "    | IVEware |  592,233  |  29.51 % | \n",
    "\n",
    "\n",
    "- Second Method\n",
    "\n",
    "\n",
    "    | | Number | Percentage |\n",
    "    | --- | --- | --- | \n",
    "    | Total NaN |  2,005,955  | 100% | \n",
    "    | RF |  558,742  |  27.85 % | \n",
    "    | Mode |  680,715  |  33.93 % | \n",
    "    | Random |  887,944  |  44.27 % | \n",
    "    | IVEware |  564,254  |  28.13 % | \n",
    "    \n",
    "### Analysis\n",
    "\n",
    "- Again, the second method, leaving in multicollinear features, is better for both Random Forest and IVEware\n",
    "\n",
    "### Conclusion\n",
    "- When we do the big runs, try it both ways, and see which one builds the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32425867",
   "metadata": {},
   "source": [
    "## Discussion -- REDO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed07de",
   "metadata": {},
   "source": [
    "- Random imputation is clearly worse than Mode and RF on every feature.\n",
    "- Random is overall worse than IVEware, but on one of our runs there are five features on which Random is better than IVEware.\n",
    "- Random Forest is as good or better than Mode on every feature, which is not surprising, as RF starts at Mode and improves on it.  \n",
    "- Random Forest is as good or better than IVEware on more than half of the features, but not overwhelmingly, and slightly better in the count of missing samples correctly imputed.\n",
    "- IVEware and Mode are comparable in the number of features, but IVEware is much better in the count of missing samples correctly imputed.\n",
    "- Random Forest and Mode make the same mistakes.  \n",
    "- IVEware makes different mistakes from Random Forest and Mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d8b76",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e27f7d",
   "metadata": {},
   "source": [
    "- Try using both MissForest and IVEware with random seed zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6fe8c",
   "metadata": {},
   "source": [
    "## Opportunities for Future Research\n",
    "(or, \"Things we didn't do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bb784",
   "metadata": {},
   "source": [
    "\n",
    "- Is it okay to use one imputation method for some features and another method for other features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd6971",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9db3b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy version: 1.26.4\n",
      "Pandas version:  2.2.2\n",
      "SciKit-Learn version: 1.5.0\n",
      "MissForest version:  2.5.5_Brad_1_Stuff\n",
      "Random seed for Python and NumPy set to  1\n",
      "filename_prefix =  _1_1\n",
      "Finished Importing Libraries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import missforest.missforest_Brad_1\n",
    "from missforest.missforest_Brad_1 import MissForest \n",
    "print ('MissForest version:  {}'.format(missforest.missforest_Brad_1.__version__))\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "random_seed = 1\n",
    "print ('Random seed for Python and NumPy set to ', random_seed)\n",
    "np.random.seed(random_seed) # NumPy\n",
    "random.seed(random_seed) # Python\n",
    "#tf.set_random_seed(random_seed) # Tensorflow\n",
    "#filename_prefix = '_0_0'\n",
    "#filename_prefix = '_0_1'\n",
    "#filename_prefix = '_1_0'\n",
    "filename_prefix = '_1_1'\n",
    "print ('filename_prefix = ', filename_prefix)\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './beep.wav'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Importing Libraries')\n",
    "print ()\n",
    "\n",
    "comment = \"\"\"\n",
    "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
    "NumPy version: 1.26.4\n",
    "Pandas version:  2.2.2\n",
    "SciKit-Learn version: 1.5.0\n",
    "MissForest version:  2.5.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d05f7",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "This notebook pulls in the saved output of Ambulance_Dispatch_2024_02_Binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9ba441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data')\n",
    "#    data = pd.read_csv('../../Big_Files/CRSS_02_Binned_Data.csv', low_memory=False)\n",
    "\n",
    "    # The first '_0' or '_1' is for the random seed for Python and Numpy\n",
    "    # The second '_0' (or '_1') is for the dimensionality not being reduced (being reduced) before imputing\n",
    "    filename = '../../Big_Files/CRSS_02' + filename_prefix + '.csv'\n",
    "    print (filename)\n",
    "    print ()\n",
    "    data = pd.read_csv(filename, low_memory=False) \n",
    "    print ('data.shape = ', data.shape)\n",
    "    print ()\n",
    "\n",
    "    # We already dropped the imputed columns in the Binning stage\n",
    "    print ('Drop Imputed Columns')\n",
    "    for feature in data:\n",
    "        if '_IM' in feature:\n",
    "            print (feature)\n",
    "            data.drop(columns=feature, inplace=True)\n",
    " \n",
    "\n",
    "    # Method for dropping from 67 to 40 features \n",
    "    # to test whether it was just this particular mix of features \n",
    "    # that made the IVEware behave strangely well with random seed of zero.\n",
    "#    print ('data.shape = ', data.shape)\n",
    "#    data = data.sample(n=40, axis='columns')\n",
    "    \n",
    "    print ('data.shape = ', data.shape)\n",
    "    print ()\n",
    "    \n",
    "    print ('Total number of NaN')\n",
    "    print (data.replace({99:np.nan}).isnull().sum().sum())\n",
    "    print ()\n",
    "    \n",
    "#    print (\"Remaining Features:\")\n",
    "#    Features = sorted(list(data.columns))\n",
    "#    for feature in Features:\n",
    "#        print (\"    \",feature)\n",
    "    print ('Finished Get_Data()')\n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d026ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Get_Data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe14831",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Analyze_Data():\n",
    "    data = Get_Data()\n",
    "    print ('Analyze_Data')\n",
    "    data = data.replace({99:np.nan})\n",
    "    \n",
    "    print ('Total NaN')\n",
    "    s = data.isna().sum().sum()\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    print (s, \" \", round(s/(rows*cols)*100,2))\n",
    "    print ()\n",
    "\n",
    "    print ('Percentage of NaN in each feature')\n",
    "    Feature_NaN_Counts = []\n",
    "    for feature in data:\n",
    "        s = data[feature].isna().sum()\n",
    "        n = len(data)\n",
    "#        print (feature, s, round(s/n*100,2))\n",
    "        Feature_NaN_Counts.append([feature, round(s/n*100,6)])\n",
    "    for row in Feature_NaN_Counts:\n",
    "        print (row)\n",
    "    print ()\n",
    "    print ('Distribution of number of NaN in each sample')\n",
    "    A = data.isna().sum(axis=1)\n",
    "    Row_NaN_Counts = A.value_counts(normalize=True)\n",
    "    display(Row_NaN_Counts)\n",
    "    Row_NaN_Counts = Row_NaN_Counts.to_list()\n",
    "    \n",
    "    print ('Finished Analyze_Data()')\n",
    "    print ()\n",
    "    \n",
    "    return Feature_NaN_Counts, Row_NaN_Counts\n",
    "    \n",
    "def Run_Analyze_Data():\n",
    "    Feature_NaN_Counts, Row_NaN_Counts = Analyze_Data()\n",
    "    \n",
    "    print ('| Feature | NaN % |')\n",
    "    print ('|---|---|')\n",
    "    for row in Feature_NaN_Counts:\n",
    "        print ('| %s | %.2f%% |' % (row[0], row[1]))\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "    print ('| Number of NaN in Sample | % of Dataset |')\n",
    "    print ('|---|---|')\n",
    "    for i, row in enumerate(Row_NaN_Counts):\n",
    "        print ('| %d | %.2f%% |' % (i, round(row*100,2)))\n",
    "    print ()\n",
    "    \n",
    "    \n",
    "#Run_Analyze_Data()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a664ab1",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab93b252",
   "metadata": {},
   "source": [
    "## Create_data_NaN_Method_3()\n",
    "- This function makes a dataset for comparing methods for imputing missing values.\n",
    "- It takes data_Ground_Truth and creates data_NaN.\n",
    "    - data_Ground_Truth is the 232,333 samples with no missing values of the 802,700 samples with binned values created in the previous notebook, with 232,333 being 28.94% of 802,700.\n",
    "    - For convenience, we will call the 802,700 samples in 67 features the \"original dataset\" because it is the origin of this notebook, even though it is not the original CRSS dataset that had more samples and features.\n",
    "    - data_NaN will be data_Ground_Truth with sample values removed, and we will compare the imputation methods by how well they impute the data_NaN missing values by comparing the imputed values with the ground truth.  \n",
    "- data_NaN will have missing values in the same patterns as the original dataset of 802,700 samples:\n",
    "    - In each feature of data_NaN, the same proportion of missing values as in the same feature in the original dataset.\n",
    "    \n",
    "    | Feature | NaN % |\n",
    "    |---|---|\n",
    "    | HOSPITAL | 0.00% |\n",
    "    | ACC_TYPE | 9.82% |\n",
    "    | AGE | 4.11% |\n",
    "    | AIR_BAG | 6.47% |\n",
    "    | ALC_STATUS | 16.49% |\n",
    "    | BODY_TYP | 2.65% |\n",
    "    | CARGO_BT | 2.20% |\n",
    "    | DAY_WEEK | 0.00% |\n",
    "    | DEFORMED | 16.19% |\n",
    "    | DR_ZIP | 5.85% |\n",
    "    | EJECTION | 4.23% |\n",
    "    | HARM_EV | 0.03% |\n",
    "    | HIT_RUN | 0.00% |\n",
    "    | HOUR | 0.22% |\n",
    "    | IMPACT1 | 1.14% |\n",
    "    | INJ_SEV | 1.28% |\n",
    "    | INT_HWY | 0.01% |\n",
    "    | J_KNIFE | 0.00% |\n",
    "    | LGT_COND | 0.28% |\n",
    "    | MAKE | 1.50% |\n",
    "    | MAK_MOD | 0.22% |\n",
    "    | MAN_COLL | 0.31% |\n",
    "    | MAX_SEV | 0.24% |\n",
    "    | MAX_VSEV | 1.07% |\n",
    "    | MODEL | 2.29% |\n",
    "    | MONTH | 0.00% |\n",
    "    | M_HARM | 0.04% |\n",
    "    | NUMOCCS | 2.04% |\n",
    "    | NUM_INJ | 0.24% |\n",
    "    | NUM_INJV | 1.07% |\n",
    "    | PCRASH4 | 3.81% |\n",
    "    | PCRASH5 | 0.28% |\n",
    "    | PERMVIT | 0.00% |\n",
    "    | PER_TYP | 0.02% |\n",
    "    | PJ | 0.00% |\n",
    "    | PSU | 0.00% |\n",
    "    | PVH_INVL | 0.00% |\n",
    "    | P_CRASH1 | 1.03% |\n",
    "    | P_CRASH2 | 1.74% |\n",
    "    | REGION | 0.00% |\n",
    "    | RELJCT1 | 18.50% |\n",
    "    | RELJCT2 | 5.02% |\n",
    "    | REL_ROAD | 0.02% |\n",
    "    | REST_MIS | 0.00% |\n",
    "    | REST_USE | 6.70% |\n",
    "    | ROLINLOC | 0.05% |\n",
    "    | ROLLOVER | 0.00% |\n",
    "    | SEAT_POS | 1.47% |\n",
    "    | SEX | 2.24% |\n",
    "    | SPEC_USE | 0.89% |\n",
    "    | SPEEDREL | 1.33% |\n",
    "    | TOWED | 4.97% |\n",
    "    | TOW_VEH | 0.02% |\n",
    "    | TYP_INT | 8.80% |\n",
    "    | URBANICITY | 0.00% |\n",
    "    | VALIGN | 5.14% |\n",
    "    | VEH_AGE | 1.21% |\n",
    "    | VE_FORMS | 0.00% |\n",
    "    | VE_TOTAL | 0.00% |\n",
    "    | VPROFILE | 13.20% |\n",
    "    | VSPD_LIM | 12.60% |\n",
    "    | VSURCOND | 3.95% |\n",
    "    | VTCONT_F | 8.24% |\n",
    "    | VTRAFCON | 8.56% |\n",
    "    | VTRAFWAY | 15.53% |\n",
    "    | WEATHER | 3.33% |\n",
    "    | WRK_ZONE | 0.00% |\n",
    "\n",
    "    - The samples of data_NaN will have the same distribution of number of missing samples.  In the original dataset, 28.94% of the samples had no missing values, so in data_NaN, 29.94% will have no missing values.  Similarly, in the original dataset 24.66% had one missing value, 16% two, 10% three, ..., 0.3487% thirteen missing values, so data_NaN will have the same distribution of number of missing samples.  \n",
    "    \n",
    "| Number of NaN in Sample | % of Dataset |\n",
    "|---|---|\n",
    "| 0 | 28.94% |\n",
    "| 1 | 24.66% |\n",
    "| 2 | 16.04% |\n",
    "| 3 | 9.92% |\n",
    "| 4 | 6.46% |\n",
    "| 5 | 4.56% |\n",
    "| 6 | 3.18% |\n",
    "| 7 | 2.18% |\n",
    "| 8 | 1.40% |\n",
    "| 9 | 0.89% |\n",
    "| 10 | 0.59% |\n",
    "| 11 | 0.45% |\n",
    "| 12 | 0.38% |\n",
    "| 13 | 0.35% |\n",
    "\n",
    "- For clarity of explanation, we will use these numbers to describe the method, though none of the numbers are hard coded in.\n",
    "\n",
    "- Start by creating the distribution of samples with the same number of missing samples as in the original dataset.  \n",
    "    - Make an empty list, data_NaN_list.\n",
    "    - Append $232,333 \\times 28.94\\%$ rows with 67 zeros.\n",
    "    - Append $232,333 \\times 24.66\\%$ rows with 1 one and 66 zeros, each row randomly shuffled.\n",
    "    - Append $232,333 \\times 16.04\\%$ rows with 2 ones and 65 zeros, each row randomly shuffled.\n",
    "    - ...\n",
    "    - Append $232,333 \\times 0.35\\%$ rows with 13 ones and 54 zeros, each row randomly shuffled.\n",
    "    - Due to rounding errors, the total number of rows may be more or less than 232,333.\n",
    "        - Pad each number of rows a bit so we have more than 232,333 rows\n",
    "        - Sample the result down to 232,333 rows.\n",
    "     - data_NaN_list how has (approximately) row sums in the same distribution as in the original data.\n",
    "- Because Pandas has appropriate tools, now change data_NaN_list to a Pandas dataframe, data_NaN.  \n",
    "- Use a greedy algorithm to modify data_NaN to get each feature to have the same proportion of NaN as in the original data.  In each iteration:\n",
    "    - Calculate the percentage of NaN in the feature, and the difference between the current percentage and the goal.  Sort by the difference.\n",
    "    \n",
    "    | Feature | % NaN Needed | Current % NaN | Difference |\n",
    "    |---|---|---|---|\n",
    "    |'J_KNIFE' | 0.0 | 3.165284 | -3.165284 |\n",
    "    |'PERMVIT' | 0.0 | 3.159689 | -3.159689 |\n",
    "    |'PVH_INVL' | 0.0 | 3.154093 | -3.154093 |\n",
    "    |'PJ' | 0.0 | 3.151941 | -3.151941 |\n",
    "    | ... | | | |\n",
    "    |'VTRAFWAY' | 15.526349 | 3.042616 | 12.483733 |\n",
    "    |'DEFORMED' | 16.189485 | 3.020234 | 13.169251 |\n",
    "    |'ALC_STATUS' | 16.487106 | 3.083505 | 13.403601 |\n",
    "    |'RELJCT1' | 18.504547 | 3.105026 | 15.399521 |\n",
    "    \n",
    "    - The first feature in the list has the most NaN to give, and the last feature in the list has the most to take.  Call those \"give_feature\" and \"take_feature\".\n",
    "        - In the example above, J_KNIFE is the give_feature and RELJCT1 is the take_feature\n",
    "    - Count how many 1's the give_feature needs to get rid of; call it \"nGive\".\n",
    "        - nGive = abs(-3.165284)% * 232,333 = 7,354\n",
    "        - J_KNIFE needs 7,354 fewer 1's\n",
    "    - Count how many 0's the take_feature needs to get; call it \"nTake\".\n",
    "        - nTake = 15.399521% * 232,333 = 35,778\n",
    "        - RELJCT1 needs 35,778 1's\n",
    "    - Filter the dataset to samples that have 1 in give_feature and 0 in take_feature; call it \"Swap\".\n",
    "    - The number of samples in Swap, Swap.shape[0], is the number of samples available to swap between the two features.\n",
    "        - Swap.shape[0] = 6,939\n",
    "        - There are 6,939 samples where J_KNIFE == 1 and RELJCT1 == 0\n",
    "    - Take the minimum of nGive, nTake, and Swap.shape[0]; call it nSample.\n",
    "        - nSample = 6,939\n",
    "    - If nSample==0, then go up the list to get a new take_feature and repeat the process.\n",
    "    - Sample Swap down to nSample rows.\n",
    "        - In this first round, nSample is the length of Swap, so sampling just shuffles the rows of Swap.\n",
    "    - In the samples in the (possibly shortened) Swap, change the 1's in give_feature to 0 and the 0's in take_feature to 1.  \n",
    "        - J_KNIFE now only needs to get rid of 7,354 - 6,939 = 415 more 1's.\n",
    "        - RELJCT1 now only needs 35,778 - 6,939 = 28,839 more 1's.\n",
    "        - In this process we do not change the number of 0's and 1's in any sample, so the distribution of number of 1's in rows is preserved.\n",
    "    - Repeat until we're within epsilon of our goal.  \n",
    "        - Maximum percentage error in NaN_Counts:   0.001291%\n",
    "- The stopping mechanism we finally decided on was if nSample==0 twenty times for one give_feature, indicating that we can't find things left for us to swap and we're within rounding error of the number of NaN we need in each feature.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc402d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN_Method_3(data_Ground_Truth):\n",
    "    print ('Create_data_NaN_Method_3()')\n",
    "    nRows = data_Ground_Truth.shape[0]\n",
    "    nCols = data_Ground_Truth.shape[1]\n",
    "    print ('nRows = ', nRows, ' nCols = ', nCols)\n",
    "    \n",
    "    Feature_NaN_Counts, Row_NaN_Counts = Analyze_Data()\n",
    "    data_NaN_list = []\n",
    "    for i in range (len(Row_NaN_Counts)):\n",
    "        Ones = i\n",
    "        Zeros = nCols - i\n",
    "        Number = int(Row_NaN_Counts[i] * nRows + 1)\n",
    "        for j in range (Number):\n",
    "            New_row = [1]*Ones + [0]*Zeros\n",
    "            random.shuffle(New_row)\n",
    "            data_NaN_list.append(New_row.copy())\n",
    "    \n",
    "    data_NaN = pd.DataFrame(data_NaN_list, columns = data_Ground_Truth.columns)\n",
    "#    print (data_NaN.shape)\n",
    "    data_NaN = data_NaN.sample(n = nRows)\n",
    "#    print (data_NaN.shape)\n",
    "#    display(data_NaN.head(20))\n",
    "#    display(data_NaN.tail(20))\n",
    "    \n",
    "#    print ('Distribution of number of NaN in each sample')\n",
    "    A = data_NaN.sum(axis=1)\n",
    "    Row_NaN_Counts = A.value_counts(normalize=True)\n",
    "#    display(Row_NaN_Counts)\n",
    "#    Row_NaN_Counts = Row_NaN_Counts.to_list()\n",
    "\n",
    "    Feature_NaN_Counts = [[x[0], x[1], 0, 0] for x in Feature_NaN_Counts]\n",
    "    Feature_NaN_Counts = Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN)\n",
    "    print ('Feature_NaN_Counts')\n",
    "    for row in Feature_NaN_Counts:\n",
    "        print (row)\n",
    "    print ()\n",
    "    \n",
    "    old_give = ''\n",
    "    old_take = ''\n",
    "    \n",
    "    stop = False\n",
    "    while stop == False:\n",
    "#        if Feature_NaN_Counts[0][3] > -0.001 or Feature_NaN_Counts[-1][3] < 0.001:\n",
    "#            stop = True\n",
    "\n",
    "        give_feature = Feature_NaN_Counts[0][0]\n",
    "        take_i = -1\n",
    "        take_feature = Feature_NaN_Counts[take_i][0]\n",
    "        nGive = int(round(-1/100 * Feature_NaN_Counts[0][3] * nRows,0))\n",
    "        nTake = int(round(1/100 * Feature_NaN_Counts[take_i][3] * nRows,0))\n",
    "        \n",
    "        mask = ((data_NaN[give_feature]==1) & (data_NaN[take_feature] == 0))\n",
    "        Swap = data_NaN[mask]\n",
    "        nSample = min([nGive, nTake, Swap.shape[0]])\n",
    "        print ('give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample')\n",
    "        print (give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample)\n",
    "        while nSample==0:\n",
    "            take_i = take_i - 1\n",
    "            take_feature = Feature_NaN_Counts[take_i][0]\n",
    "            nGive = int(round(-1/100 * Feature_NaN_Counts[0][3] * nRows,0))\n",
    "            nTake = int(round(1/100 * Feature_NaN_Counts[take_i][3] * nRows,0))\n",
    "            mask = ((data_NaN[give_feature]==1) & (data_NaN[take_feature] == 0))\n",
    "            Swap = data_NaN[mask]\n",
    "            nSample = min([nGive, nTake, Swap.shape[0]])\n",
    "            print ('give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample')\n",
    "            print (give_feature, nGive, take_i, take_feature, nTake, Swap.shape[0], nSample)\n",
    "            print ()\n",
    "            if nSample==0 and take_i < -20:\n",
    "                stop = True\n",
    "                break\n",
    "        \n",
    "#        if nSample == 0:\n",
    "#            stop = True\n",
    "        Swap = Swap.sample(n=nSample)\n",
    "#        display(Swap[[give_feature, take_feature]])\n",
    "        mask = Swap.index.values.tolist()\n",
    "        for m in mask:\n",
    "            data_NaN.loc[[m], give_feature] = 0\n",
    "            data_NaN.loc[[m], take_feature] = 1\n",
    "            \n",
    "#        print ()\n",
    "#        data_NaN[give_feature],data_NaN[take_feature]=np.where(mask,(data_NaN[take_feature],data_NaN[give_feature]),(data_NaN[give_feature],data_NaN[take_feature]))\n",
    "        Feature_NaN_Counts = Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN)\n",
    "\n",
    "    print ('Feature_NaN_Counts')\n",
    "    for row in Feature_NaN_Counts:\n",
    "        print (row)\n",
    "    print ()\n",
    "    print ('Maximum percentage error in NaN_Counts:  ', max([abs(x[3]) for x in Feature_NaN_Counts]))\n",
    "\n",
    "    data_NaN = data_NaN.sample(frac=1)\n",
    "#    print ('data_NaN')\n",
    "#    display(data_NaN.head(10))\n",
    "#    print ('data_NaN reindexed')\n",
    "    data_NaN.reset_index(inplace=True, drop=True)\n",
    " #   display(data_NaN.head(10))\n",
    " #   print ('data_Ground_Truth')\n",
    " #   display(data_Ground_Truth.head(10))\n",
    " #   print ('data_NaN')\n",
    " #   display(data_NaN.head(10))\n",
    "    data_NaN = data_Ground_Truth.where(data_NaN==0)\n",
    "#    print ('data_NaN')\n",
    "#    display(data_NaN.head(20))\n",
    "#    display(data_NaN.tail(20))\n",
    "    \n",
    "    \n",
    "    display(data_NaN.isna().sum())\n",
    "    print ('data_NaN.isna().sum().sum() = ', data_NaN.isna().sum().sum())\n",
    "\n",
    "    print ('Finished Create_data_NaN_Method_3()')\n",
    "    print ()\n",
    "    \n",
    "    return data_NaN\n",
    "    \n",
    "\n",
    "def Feature_NaN_Counts_Update(Feature_NaN_Counts, data_NaN):\n",
    "    for row in Feature_NaN_Counts:\n",
    "        feature = row[0]\n",
    "        s = data_NaN[feature].sum()\n",
    "        n = len(data_NaN)\n",
    "#        print (feature, s, round(s/n*100,2))\n",
    "        row[2] = round(s/n*100,6)\n",
    "        row[3] = round(row[1] - row[2], 6)\n",
    "    Feature_NaN_Counts = sorted(Feature_NaN_Counts, key=lambda x:x[3])\n",
    "    \n",
    "    return Feature_NaN_Counts\n",
    "\n",
    "    \n",
    "def Test_Create_data_NaN_Method_3():\n",
    "    print ('Test_Create_data_NaN_Method_3()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "#    data_Ground_Truth.astype('Int64')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head(10))\n",
    "    Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "    print ('Finished Test_Create_data_NaN_Method_3')\n",
    "    print ()\n",
    "\n",
    "    \n",
    "#Test_Create_data_NaN_Method_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef777e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN(data_Ground_Truth):\n",
    "    print ('Create_data_NaN()')\n",
    "    \n",
    "    # Create a 2d list of the same shape as \"data\"\n",
    "    # Each row has \"1\" in 15% of the columns, \"0\" otherwise\n",
    "        # By \"15%,\" we mean that \"(# columns)*0.15\" rounded to the nearest integer.\n",
    "    # The first row is a random shuffle of such a row.\n",
    "    # The next (columns-1) rows are rotations of that row\n",
    "    # Each \"column\" number of rows, shuffle and repeat.\n",
    "    # Each row will have 15% of the samples 1, and each column will have 15% of the samples 1.\n",
    "    # Shuffle the rows.\n",
    "    # Then shuffling the columns would be redundant.\n",
    "    \n",
    "    rows = data_Ground_Truth.shape[0]\n",
    "    columns = data_Ground_Truth.shape[1]\n",
    "    drops_in_row = int(round(columns*0.15,0))\n",
    "    print ('drops_in_row = ', drops_in_row)\n",
    "    \n",
    "    Rand_Drop = []\n",
    "    Row = [1]*(drops_in_row) + [0]*(columns - drops_in_row)\n",
    "\n",
    "    for i in range (rows):\n",
    "        if i%3==0:\n",
    "            Row = [1]*(drops_in_row) + [0]*(columns - drops_in_row)\n",
    "        elif i%3==1:\n",
    "            Row = [1]*(drops_in_row + 1) + [0]*(columns - drops_in_row - 1)\n",
    "        else:\n",
    "            Row = [1]*(drops_in_row - 1) + [0]*(columns - drops_in_row + 1)\n",
    "        random.shuffle(Row)\n",
    "        Rand_Drop.append(Row.copy())\n",
    "\n",
    "#    for i in range (rows):\n",
    "#        if i%columns==0:\n",
    "#            random.shuffle(Row)\n",
    "#        Row.append(Row.pop(0))\n",
    "#        Rand_Drop.append(Row.copy())\n",
    "\n",
    "    random.shuffle(Rand_Drop)\n",
    "\n",
    "#    for i in range (columns):\n",
    "#        print (i, sum([x[i] for x in Rand_Drop]))\n",
    "\n",
    "    # Turn the 2d list into a dataframe\n",
    "        \n",
    "    Rand_Drop_df = pd.DataFrame(Rand_Drop, columns=data_Ground_Truth.columns)\n",
    "    display(Rand_Drop_df)\n",
    "    \n",
    "#    for feature in Rand_Drop_df:\n",
    "#        print (feature, Rand_Drop_df[feature].sum())\n",
    "\n",
    "    # Change the Ground Truth values to NaN where the corresponding value in Rand_Drop_df is 1\n",
    "    data_NaN = data_Ground_Truth.where(Rand_Drop_df==0)\n",
    "#    data_NaN = data_NaN.astype('Int')\n",
    "    \n",
    "    print ('data_NaN')\n",
    "    display(data_NaN)\n",
    "    \n",
    "    print ('data_NaN.isna().sum()')\n",
    "    display(data_NaN.isna().sum())\n",
    "    \n",
    "    print ('data_NaN.dropna().shape')\n",
    "    print (data_NaN.dropna().shape)\n",
    "    \n",
    "    print ('Finished Create_data_NaN()')\n",
    "    print ()\n",
    "\n",
    "    return data_NaN\n",
    "    \n",
    "    \n",
    "def Test_Create_data_NaN():\n",
    "    print ('Test_Create_data_NaN()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int32')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    Create_data_NaN(data_Ground_Truth)\n",
    "    \n",
    "    print ('Finished Test_Create_data_NaN()')\n",
    "    print ()\n",
    "\n",
    "#Test_Create_data_NaN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959fc06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Create_data_NaN_Old(data_Ground_Truth):\n",
    "    \"\"\"\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('int64')\n",
    "\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth = data_Ground_Truth.astype('int64')\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly pick 15% of the values from each row\n",
    "    # and set them to be missing\n",
    "    print ('Remove 15% of values from each row')\n",
    "    frac = .15\n",
    "    data_NaN = data_Ground_Truth.copy(deep=True)\n",
    "    N = data_NaN.shape[0] * frac # Number of NaN in each feature\n",
    "    for c in data_NaN.columns:\n",
    "        idx = np.random.choice(a=data_NaN.index, size=int(len(data_NaN) * frac))\n",
    "        data_NaN.loc[idx, c] = np.NaN\n",
    "#    for feature in data_NaN:\n",
    "#        data_NaN[feature] = pd.to_numeric(data_NaN[feature])\n",
    "#    data_NaN.astype('int64')\n",
    "\n",
    "\n",
    "#    print ('data_NaN.shape')\n",
    "#    print (data_NaN.shape)\n",
    "#    display(data_NaN.head())\n",
    "    \n",
    "    data_NaN = data_NaN.astype('Int8')\n",
    "    \n",
    "#    data_NaN = data_NaN.sample(n=200000)\n",
    "#    print (data_NaN.shape)\n",
    "#    print (data_NaN.head(20))\n",
    "    \n",
    "    return data_NaN\n",
    "\n",
    "#Create_data_NaN_Old()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3627270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_MissForest(data):\n",
    "    print('Impute_MissForest()')\n",
    "\n",
    "    print (data.shape)\n",
    "    display(data.head(20))\n",
    "#    data.replace({np.nan: ''}, inplace=True)\n",
    "#    display(data.head(20))\n",
    "\n",
    "    categorical = list(data)\n",
    "    print ('categorical features: ', categorical)\n",
    "    \n",
    "    clf = RandomForestClassifier(\n",
    "#        n_estimators=100, \n",
    "#        max_depth=10, \n",
    "#        verbose=2,\n",
    "#        max_features=0.5\n",
    "    )\n",
    "    rgr = RandomForestRegressor(\n",
    "#        n_estimators=100, \n",
    "#        max_depth=10, \n",
    "#        verbose=2,\n",
    "#        max_features=0.5\n",
    "    )\n",
    "\n",
    "    data_MF = MissForest(clf, rgr, max_iter = 10).fit_transform(\n",
    "        x = data,\n",
    "        categorical=categorical,\n",
    "    )\n",
    "    display(data_MF.head(20))\n",
    "    print ('Finished Impute_MissForest()')\n",
    "    print ()\n",
    "    \n",
    "    return data_MF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284950c6",
   "metadata": {},
   "source": [
    "## Modifying and Testing our Versions of MissForest\n",
    "- We made some changes to MissForest version 2.5.5\n",
    "    - Change the logic of when to end the iterations\n",
    "    - Optimize memory usage\n",
    "        - Original took about 40 GB on the 232,333-sample test set\n",
    "        - First revision took 1.9 GB\n",
    "        - Second revision took 1.1 GB\n",
    "- Note that we will only talk about what MissForest does for the categorical features, because all of our features are categorical, but there's a parallel system for numerical features.\n",
    "- What we think the Original version was supposed to do.  \n",
    "    - The number of changes from one iteration to the next should be decreasing.  If it starts increasing, stop.\n",
    "    - The metric Gamma is the count of all of the changes from one iteration to the next, divided by the number of categorical features.  \n",
    "    - The list all_x_imp_cat is a list of the dataframes, one appended at the end of each imputing iteration.\n",
    "    - At the end of each iteration, calculate Gamma between all_x_imp_cat[-1] and all_x_imp_cat[-2].\n",
    "    - Append that Gamma to all_gamma_cat.\n",
    "    - If the new Gamma in all_gamma_cat is more than from the last iteration, stop, because the imputation is diverting.\n",
    "- What the original version did instead:\n",
    "    - Because of an indenting error (?), it added the current dataframe to all_x_imp_cat at the end of each feature, not at the end of each iteration.  \n",
    "        - Instead of saving one copy of the dataset each iteration, it saved 67 (number of features) copies of the dataset in each iteration, which took so much memory that it caused the process to crash.\n",
    "        - Instead of the changes from all_x_imp_cat[-2] to all_x_imp_cat[-1] being a whole iteration of changes over 67 features, the difference at the end of the iteration (when Gamma was calculated) was just the change in the last feature, WRK_ZONE.  So all_gamma_cat tracked the number of changes in the last feature in each iteration, and the stopping mechanism only looked at the number of changes in one feature.\n",
    "    - When the imputation started to diverge, it stopped and returned the dataframe from the last iteration, the one that was diverging.  Our opinion is that the returne dataframe should be the one before that, when the changes were minimized.  \n",
    "- Our first revision:\n",
    "    - Moved the indentation back\n",
    "        - We were saving only one copy of the dataframe per iteration\n",
    "        - Gamma measured the change over one iteration, not the change over one feature\n",
    "    - Returned the next-to-last iteration\n",
    "- Our second revision additionally:\n",
    "    - Only kept the current and the previous iteration in memory.  \n",
    "        - Only saving two copies of the dataframe, not nIterations + 1 copies\n",
    "        - Before the first iteration, and at the end of each iteration, ``x_imp_previous = x_imp``\n",
    "        - Gamma measures the difference between x_imp_previous and x_imp.\n",
    "- We tested the two revisions to make sure they gave the same results.  At the end of each iteration they gave the same Gammas (exactly), and we take that as reasonable assurance that they're doing the same thing.  \n",
    "- The revisions should give the same dataset results as the original code if they stop after the same number of iterations.\n",
    "- The revisions should not give the same Gamma values as the original code because they're comparing different things.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30637144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def Test_Impute_MissForest():\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int64')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "\n",
    "    data_NaN = Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "\n",
    "\n",
    "    print ('data_NaN.shape')\n",
    "    print (data_NaN.shape)\n",
    "    display(data_NaN.head())\n",
    "    \n",
    "#    data_NaN = data_NaN.astype('Int8')\n",
    "    \n",
    "#    data_NaN = data_NaN.sample(n=10000)\n",
    "#    print (data_NaN.shape)\n",
    "#    print (data_NaN.head(20))\n",
    "\n",
    "    \n",
    "    # Perform MissForest imputation\n",
    "    print ('Start Imputation')\n",
    "    data_MF = Impute_MissForest(data_NaN)\n",
    "    data_MF.sort_index(inplace=True)\n",
    "    data_MF = data_MF[data.columns]  \n",
    "#    data_MF = data_MF.astype('Int64')\n",
    "    print (data_MF.head(20))\n",
    "    \n",
    "    print ('Finished Test_Impute_MissForest()')\n",
    "    print ()\n",
    "    \n",
    "#Test_Impute_MissForest()\n",
    "\n",
    "# With missforest.py with Brad's modifications that store only the current and previous iterations of the dataframe\n",
    "# all_gamma_cat =  [2001.7611940298507, 337.089552238806, 293.4776119402985, 287.53731343283584, 287.4925373134328, 285.5970149253731, 285.2238805970149, 286.82089552238807]\n",
    "# 7 iterations\n",
    "# 60 minutes\n",
    "# Memory Usage:\n",
    "    # Before running, python3.10 is taking 50 MB.\n",
    "    # Running things up to this test, 130 MB.\n",
    "    # During Iteration 0, between 950 MB and 1400 MB, typically 1100 MB.\n",
    "    # About the same in Iterations 1, 3, and 6.\n",
    "    # \n",
    "\n",
    "# with missforest_Brad_2.py with Brad's modifications that store a new dataframe at each iteration\n",
    "# all_gamma_cat =  [2001.7611940298507, 337.089552238806, 293.4776119402985, 287.53731343283584, 287.4925373134328, 285.5970149253731, 285.2238805970149, 286.82089552238807]\n",
    "# 7 iterations\n",
    "# Same time\n",
    "# Memory usage:\n",
    "    # In iteration 0, 920 MB\n",
    "    # In iteration 1, 1.13 GB\n",
    "    # In iteration 2, 1.36 GB\n",
    "    # In iteration 3, 1.49 GB\n",
    "    # In iteration 4, 1.52 GB\n",
    "    # In iteration 5, 1.7 GB\n",
    "    # In iteration 6, 1.8 GB\n",
    "    # In iteration 7, 1.9 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489df19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test imputing the big dataset\n",
    "def Test_Impute_MissForest_2():\n",
    "    print ('Test_Impute_Miss_Forest_2()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "    display(data.head(10))\n",
    "#    data = data.sample(n=1000)\n",
    "    print (data.shape)\n",
    "    data.replace({99:np.nan}, inplace=True)\n",
    "    display(data.head(10))\n",
    "    data_MF = Impute_MissForest(data)\n",
    "    \n",
    "    print ('Finished Test_Impute_Miss_Forest_2()')\n",
    "    print ()\n",
    "\n",
    "#Test_Impute_MissForest_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defec34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Round_Robin(data):\n",
    "    print ('Impute_Round_Robin()')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "    # Replace 'Unknown' with np.NaN\n",
    "#    data.replace({'Unknown': np.nan}, inplace=True)\n",
    "#    data.replace({99: np.nan}, inplace=True)\n",
    "    display(data.head(20))\n",
    "    print ()\n",
    "    \n",
    "    # Make a list of features with missing samples, \n",
    "    #     ordered by the number of missing samples, \n",
    "    #     from least to most.  \n",
    "    Missing = []\n",
    "    Complete = []\n",
    "    for feature in data:\n",
    "        s = data[feature].isna().sum()\n",
    "        if s==0:\n",
    "            Complete.append([feature, s])\n",
    "        if s>0:\n",
    "            Missing.append([feature, s])\n",
    "    Missing = sorted (Missing, key=lambda x:x[1], reverse=False)\n",
    "#    print ()\n",
    "#    print ('Complete[]')\n",
    "#    display(Complete)\n",
    "#    print ()\n",
    "#    print ('Missing[]')\n",
    "#    display(Missing)\n",
    "#    print ()\n",
    "    \n",
    "#    print ('Make data_Mode')\n",
    "#    print ()\n",
    "    data_Mode = pd.DataFrame()\n",
    "    for X in Complete:\n",
    "        feature = X[0]\n",
    "        data_Mode[feature] = data[feature]\n",
    "    for M in Missing:\n",
    "        feature = M[0]\n",
    "        m = data[feature].mode()[0]\n",
    "#        print (feature, M[1], m)\n",
    "        data_Mode[feature] = data[feature].fillna(m)\n",
    "#    print ('data_Mode')\n",
    "    display(data_Mode.head(20))\n",
    "\n",
    "#    print ()\n",
    "#    print ('Make starting point for data_Imputed')\n",
    "    data_Imputed = pd.DataFrame()\n",
    "    for X in Complete:\n",
    "        feature = X[0]\n",
    "        data_Imputed[feature] = data[feature]\n",
    "    for X in Missing:\n",
    "        feature = X[0]\n",
    "        data_Imputed[feature] = data_Mode[feature]\n",
    "#    print ('data_Imputed')\n",
    "#    display(data_Imputed.head(20))\n",
    "#    print ()\n",
    "\n",
    "    print ('Start Loop')\n",
    "    print ()\n",
    "    n = 0\n",
    "    for M in Missing:\n",
    "        n += 1\n",
    "        print (M)\n",
    "        feature = M[0]\n",
    "        data_Imputed[feature] = data[feature]\n",
    "#        print ()\n",
    "#        print ('data[feature].isna().sum()')\n",
    "#        print (data[feature].isna().sum())\n",
    "#        print ('data_Imputed[feature].isna().sum()')\n",
    "#        print (data_Imputed[feature].isna().sum())\n",
    "#        print ()\n",
    "        W = data_Imputed.dropna(subset=[feature])\n",
    "        X = data_Imputed.dropna(subset=[feature])\n",
    "        y = X[feature]\n",
    "        X.drop(columns=feature, inplace=True)\n",
    "        Z = data_Imputed[data_Imputed[feature].isna()]\n",
    "        Z.drop(columns=feature, inplace=True)\n",
    "#        Z.reset_index(drop=True, inplace=True)\n",
    "#        print (data.shape)\n",
    "#        print (X.shape)\n",
    "#        display(X.head(40))\n",
    "#        display(y.head(40))\n",
    "#        print (Z.shape)\n",
    "#        display(Z)\n",
    "        clf = RandomForestClassifier(max_depth=2, random_state=random_seed)\n",
    "        clf.fit(X,y)\n",
    "#        print ('clf.predict(Z)')\n",
    "        z = clf.predict(Z)\n",
    "#        print (len(z))\n",
    "#        display(z)\n",
    "        Z[feature] = z\n",
    "#        display(Z)\n",
    "        data_Imputed = pd.concat([Z, W])\n",
    "#        display(data_Imputed.head(60))\n",
    "#        print (data_Imputed.shape)\n",
    "#        print ()\n",
    "#        data_Imputed.sort_values(\n",
    "#            by = ['CASENUM', 'VEH_NO', 'PER_NO'], \n",
    "#            ascending = [True, True, True], \n",
    "#            inplace=True\n",
    "#        )\n",
    "#        print ()\n",
    "#        print ('data.PER_NO.equals(data_Imputed.PER_NO)')\n",
    "#        print (data.PER_NO.equals(data_Imputed.PER_NO))\n",
    "#        print ()\n",
    "               \n",
    "#        Check_Feature(data, data_Imputed, feature)\n",
    "#        if n==10:\n",
    "#            return data_Imputed\n",
    "    \n",
    "    \n",
    "    display(data_Imputed.head(20))\n",
    "\n",
    "    print ('Finished Impute_Round_Robin()')\n",
    "    print ()\n",
    "    return data_Imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a34ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(data, data_Imputed):\n",
    "    print ('Check()')\n",
    "    Features = data.columns\n",
    "    print (Features)\n",
    "    for feature in Features:\n",
    "        U = pd.unique(data[feature]).tolist()\n",
    "        print (U)\n",
    "        A = []\n",
    "        for u in U:\n",
    "            a = len(data[data[feature]==u])\n",
    "            b = len(data_Imputed[data_Imputed[feature]==u])\n",
    "            A.append([u, a, b])\n",
    "        display(A)\n",
    "        print ()\n",
    "    print ('Finished Check()')\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Feature(data, data_Imputed, feature):\n",
    "    print ('Check_Feature(%s)' % feature)\n",
    "    U = pd.unique(data[feature]).tolist()\n",
    "    U = [x for x in U if x == x]\n",
    "    print (U)\n",
    "    A = []\n",
    "    for u in U:\n",
    "        a = len(data[data[feature]==u])\n",
    "        b = len(data_Imputed[data_Imputed[feature]==u])\n",
    "        A.append([u, a, b, b-a])\n",
    "    a = data[feature].isna().sum()\n",
    "    b = data_Imputed[feature].isna().sum()\n",
    "    A.append(['NaN', a, b, 0])\n",
    "    A = pd.DataFrame(A, columns=['Value', 'Original', 'Imputed', 'Difference'])\n",
    "    display(A)\n",
    "    \n",
    "    print ('Finished Check_Feature()')\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Impute_Randomly(data):\n",
    "    print ('Impute_Randomly()')\n",
    "    print ()\n",
    "    \n",
    "    data.sample(frac=1, replace=True) # Randomly shuffle the rows of the dataset\n",
    "    for feature in data:\n",
    "        print (feature)\n",
    "#        print ('display(data[feature].head())')\n",
    "#        display(data[feature].head())\n",
    "        dfA = data[feature]\n",
    "#        print ('display(dfA.head())')\n",
    "#        display(dfA.head())\n",
    "        dfA.dropna(inplace=True)\n",
    "#        print ('display(dfA.head()) after dfA.dropna(inplace=True)')\n",
    "#        display(dfA.head())\n",
    "#        print ('Original Value Counts')\n",
    "#        print (dfA.value_counts(normalize=True))\n",
    "        dfA = dfA.sample(n = len(data), replace=True)\n",
    "#        print ('display(dfA.head()) after dfA.sample(n = len(data), replace=True)')\n",
    "#        display(dfA.head())\n",
    "#        print ('Value Counts after Sampling')\n",
    "#        print (dfA.value_counts(normalize=True))\n",
    "        dfA.reset_index(drop=True, inplace=True)\n",
    "#        print ('display(dfA.head()) after dfA.reset_index(drop=True)')\n",
    "#        display(dfA.head())\n",
    "        data[feature].fillna(dfA, inplace=True)\n",
    "#        print ('display(data[feature].head())')\n",
    "#        display(data[feature].head())        \n",
    "#        print ()\n",
    "        \n",
    "    print ('Finished Impute_Randomly()')\n",
    "    print ()\n",
    "    return data\n",
    "        \n",
    "def Test_Impute_Randomly():\n",
    "    Dict = {\n",
    "        'A':[0,0,0,1,np.nan],\n",
    "        'B':[1,2,3,4,np.nan]\n",
    "    }\n",
    "    \n",
    "    data = pd.DataFrame(Dict)\n",
    "    display(data)\n",
    "    data = Impute_Randomly(data)\n",
    "    display(data)\n",
    "    \n",
    "#Test_Impute_Randomly()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c72a63",
   "metadata": {},
   "source": [
    "# Compare Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25a5b4",
   "metadata": {},
   "source": [
    "## Mode Imputation\n",
    "## Random Forest Imputation\n",
    "## Prepare Data for IVEware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_Imputation_Methods_Part_1():\n",
    "    print ('Compare_Imputation_Methods_Part_1()')\n",
    "    data = Get_Data()\n",
    "    print (data.shape)\n",
    "\n",
    "    # Drop all samples with missing data, so we have ground truth\n",
    "    data_Ground_Truth = data.replace({99:np.nan})\n",
    "    data_Ground_Truth = data_Ground_Truth.dropna()\n",
    "    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "    for feature in data_Ground_Truth:\n",
    "        data_Ground_Truth[feature] = pd.to_numeric(data_Ground_Truth[feature])\n",
    "    data_Ground_Truth.astype('Int32')\n",
    "\n",
    "    print ('data_Ground_Truth.shape')\n",
    "    print (data_Ground_Truth.shape)\n",
    "#    data_Ground_Truth = data_Ground_Truth.sample(n=200000)\n",
    "#    data_Ground_Truth.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#    print ('data_Ground_Truth.shape after resampling')\n",
    "#    print (data_Ground_Truth.shape)\n",
    "    display(data_Ground_Truth.head())\n",
    "\n",
    "    print ('Remove values from each row')\n",
    "    data_NaN = Create_data_NaN_Method_3(data_Ground_Truth)\n",
    "    \n",
    "    print ('data_NaN.shape')\n",
    "    print (data_NaN.shape)\n",
    "    display(data_NaN.head(10))\n",
    "    display(data_NaN.tail(10))\n",
    "    \n",
    "    # Perform MissForest imputation\n",
    "    data_MF = data_NaN.copy(deep=True)\n",
    "#    data_MF = data_NaN_Old.copy(deep=True)\n",
    "#    data_MF = data_MF.astype('Int8')\n",
    "    print ('data_MF')\n",
    "    display(data_MF.head(20))\n",
    "    data_MF = Impute_MissForest(data_MF)\n",
    "    data_MF.sort_index(inplace=True)\n",
    "    data_MF = data_MF[data.columns]  \n",
    "#    data_MF = data_MF.astype('Int32')\n",
    "    \n",
    "    print ('data_MF.shape')\n",
    "    print (data_MF.shape)\n",
    "    display(data_MF.head())\n",
    "#    print ()\n",
    "\n",
    "    \n",
    "    # Create .txt file to feed into IVEware imputation\n",
    "    data_IVEware = data_NaN.copy(deep=True)\n",
    "#    data_IVEware = data_IVEware.astype('str')\n",
    "    data_IVEware = data_IVEware.fillna('')\n",
    "    data_IVEware.to_csv('../../Big_Files/data_IVEware.txt', sep='\\t', index=False)\n",
    "    \n",
    "    data_Mode = pd.DataFrame()\n",
    "    for feature in data_NaN:\n",
    "        data_Mode[feature] = data_NaN[feature].fillna(data_NaN[feature].mode()[0])\n",
    "    data_Mode = data_Mode.astype('Int32')\n",
    "    print ('data_Mode.shape')\n",
    "    print (data_Mode.shape)\n",
    "    display(data_Mode.head())\n",
    "    \n",
    "    # Perform Round Robin imputation using Random Forest Classifier\n",
    "    data_RF = Impute_Round_Robin(data_NaN)\n",
    "    data_RF.sort_index(inplace=True)\n",
    "    data_RF = data_RF[data.columns]  \n",
    "    data_RF = data_RF.astype('Int32')\n",
    "    \n",
    "    print ('data_RF.shape')\n",
    "    print (data_RF.shape)\n",
    "    display(data_RF.head())\n",
    "#    print ()\n",
    "\n",
    "    # Impute randomly\n",
    "    data_Random = data_NaN.copy(deep=True)\n",
    "    data_Random = Impute_Randomly(data_Random)\n",
    "    data_Random = data_Random.astype('Int32')\n",
    "    \n",
    "    print ('data_Random.shape')\n",
    "    print (data_Random.shape)\n",
    "    display(data_Random.head())\n",
    "#    print ()\n",
    "\n",
    "    print ('Finished Compare_Imputation_Methods_Part_1()')\n",
    "    print ()\n",
    "\n",
    "    return data_Ground_Truth, data_NaN, data_RF, data_MF, data_Mode, data_Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0ab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# about an hour\n",
    "data_Ground_Truth, data_NaN, data_RF, data_MF, data_Mode, data_Random = Compare_Imputation_Methods_Part_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c93a1",
   "metadata": {},
   "source": [
    "## Do IVEware Imputation (Outside this Jupyter Notebook)\n",
    "- Go to the IVEware folder and run (at the command line) IVE_12_22_22.bat\n",
    "- Requires scrlib and R.  You may need to, in the batch file, change the path to your scrlib installation.\n",
    "- Notes to self:\n",
    "    - Open srcshell\n",
    "    - From srcshell, open IVEware_CRSS_Imputation.xml\n",
    "    - Run\n",
    "- Run time: ./IVEware_CRSS_Imputation.bat  1069.08s user 12.92s system 98% cpu 18:23.92 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_IVEware_seed_0 = pd.read_csv('../../Big_Files/data_IVEware_Compare_seed_0.csv')\n",
    "data_IVEware_seed_0.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "data_IVEware_seed_1 = pd.read_csv('../../Big_Files/data_IVEware_Compare_seed_1.csv')\n",
    "data_IVEware_seed_1.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "print ('data_Ground_Truth', data_Ground_Truth.shape)\n",
    "display(data_Ground_Truth.head(10))\n",
    "print ('data_NaN', data_NaN.shape)\n",
    "display(data_NaN.head(10))\n",
    "print ('data_RF', data_RF.shape)\n",
    "display(data_RF.head(10))\n",
    "print ('data_MF', data_MF.shape)\n",
    "display(data_MF.head(10))\n",
    "print ('data_IVEware_seed_0', data_IVEware_seed_0.shape)\n",
    "display(data_IVEware_seed_0.head(10))\n",
    "print ('data_IVEware_seed_1', data_IVEware_seed_1.shape)\n",
    "display(data_IVEware_seed_1.head(10))\n",
    "print ('data_Mode', data_Mode.shape)\n",
    "display(data_Mode.head(10))\n",
    "print ('data_Random', data_Random.shape)\n",
    "display(data_Random.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7745a",
   "metadata": {},
   "source": [
    "## Compare Six Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf238635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_Imputation_Methods_Part_2(\n",
    "    data_Ground_Truth, \n",
    "    data_NaN, \n",
    "    data_RF, \n",
    "    data_MF, \n",
    "    data_Mode, \n",
    "    data_Random, \n",
    "    data_IVEware_seed_0, \n",
    "    data_IVEware_seed_1\n",
    "):\n",
    "    print ('Compare_Imputation_Methods_Part_2')\n",
    "    \n",
    "    \"\"\"\n",
    "    print ('Drop Multicollinear Features')\n",
    "    Drop = ['MAX_VSEV', 'VE_FORMS', 'VTCONT_F', 'MAX_SEV', 'NUM_INJV']\n",
    "    DF = [data_Ground_Truth, data_NaN, data_RF, data_Mode, data_Random, data_IVEware]\n",
    "    \n",
    "    for df in DF:\n",
    "        for feature in Drop:\n",
    "            if feature in df:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "                print ('Drop ', feature)\n",
    "    print ()\n",
    "    \"\"\"\n",
    "    \n",
    "    Datasets = [\n",
    "        ['data_Ground_Truth', data_Ground_Truth],\n",
    "        ['data_NaN', data_NaN],\n",
    "        ['data_RF', data_RF],\n",
    "        ['data_MF', data_MF],\n",
    "        ['data_Mode', data_Mode],\n",
    "        ['data_Random', data_Random],\n",
    "        ['data_IVEware_seed_0', data_IVEware_seed_0],\n",
    "        ['data_IVEware_seed_1', data_IVEware_seed_1],\n",
    "    ]\n",
    "    \n",
    "    for dataset in Datasets:\n",
    "        name = dataset[0]\n",
    "        data = dataset[1]\n",
    "        print (name, '.shape: ', data.shape)\n",
    "    print ()\n",
    "    \n",
    "    for dataset in Datasets:\n",
    "        name = dataset[0]\n",
    "        data = dataset[1]\n",
    "        print (name)\n",
    "        display(data.head())\n",
    "    print ()\n",
    "    \n",
    "    Datasets = [\n",
    "        ['data_Ground_Truth', data_Ground_Truth],\n",
    "#        ['data_NaN', data_NaN],\n",
    "        ['data_RF', data_RF],\n",
    "        ['data_MF', data_MF],\n",
    "        ['data_Mode', data_Mode],\n",
    "        ['data_Random', data_Random],\n",
    "        ['data_IVEware_seed_0', data_IVEware_seed_0],\n",
    "        ['data_IVEware_seed_1', data_IVEware_seed_1],\n",
    "    ]\n",
    "    \n",
    "    A = []\n",
    "    for feature in data_NaN:\n",
    "        B = []\n",
    "        B.append(feature)\n",
    "        B.append(data_NaN[feature].isna().sum())\n",
    "        for i in range (len(Datasets)-1):\n",
    "            for j in range (i+1, len(Datasets)):\n",
    "                C = (Datasets[i][1][feature] != Datasets[j][1][feature]).sum()\n",
    "                D = round(C/B[1]*100,4)\n",
    "                B.append(C)\n",
    "#                print ('B[', len(B)-1, '] counts differences between ', Datasets[i][0], ' and ', Datasets[j][0], '.')\n",
    "                B.append(D)\n",
    "#                print ('B[', len(B)-1, '] gives the count as a percentage.')\n",
    "#        print ()\n",
    "        \n",
    "#        print (B)\n",
    "        A.append(B)\n",
    "    print ()\n",
    "    \n",
    "    A = sorted(A, key=lambda x:x[3])\n",
    "    B = pd.DataFrame(\n",
    "        A, \n",
    "        columns=[\n",
    "            'Feature', # 0\n",
    "            'nNaN',  # 1\n",
    "            'nRF Incorrect', 'pRF Incorrect', # 2, 3\n",
    "            'nMF Incorrect', 'pMF Incorrect', # 4, 5\n",
    "            'nMode Incorrect', 'pMode Incorrect', # 6, 7\n",
    "            'nRandom Incorrect', 'pRandom Incorrect', # 8, 9\n",
    "            'nIVEware_seed_0 Incorrect', 'pIVEware_seed_0 Incorrect', # 10, 11\n",
    "            'nIVEware_seed_1 Incorrect', 'pIVEware_seed_1 Incorrect', # 12, 13\n",
    "            'RF and MF Different', 'RF v/s MF %', # 14, 15\n",
    "            'RF and Mode Different', 'RF v/s Mode %', # 16, 17\n",
    "            'RF and Random Different', 'RF v/s Random %', # 18, 19\n",
    "            'RF and IVEware_seed_0 Different', 'RF v/s IVEware_seed_0 %', # 20, 21\n",
    "            'RF and IVEware_seed_1 Different', 'RF v/s IVEware_seed_1 %', # 22, 23\n",
    "            'MF and Mode Different', 'MF v/s Mode %', # 24, 25\n",
    "            'MF and Random Different', 'MF v/s Random %', # 26, 27\n",
    "            'MF and IVEware_seed_0 Different', 'MF v/s IVEware_seed_0 %', # 28, 29\n",
    "            'MF and IVEware_seed_1 Different', 'MF v/s IVEware_seed_1 %', # 30, 31\n",
    "            'Mode and Random Different', 'Mode v/s Random %', # 32, 33\n",
    "            'Mode and IVEware_seed_0 Different', 'Mode v/s IVEware_seed_0 %', #, 34, 35\n",
    "            'Mode and IVEware_seed_1 Different', 'Mode v/s IVEware_seed_1 %', #, 36, 37\n",
    "            'Random and IVEware_seed_0 Different', 'Random v/s IVEware_seed_0 %', # 38, 39\n",
    "            'Random and IVEware_seed_1 Different', 'Random v/s IVEware_seed_1 %', # 40, 41\n",
    "            'IVEware_seed_0 and IVEware_seed_1 Different', 'IVEware_seed_0 v/s IVEware_seed_1 %', # 42, 43\n",
    "        ]\n",
    "    )\n",
    "    display(B)\n",
    "    a = sum([x[1] for x in A]) # nNaN\n",
    "    b = sum([x[2] for x in A]) # nRF Incorrect\n",
    "    c = sum([x[4] for x in A]) # nMF Incorrect\n",
    "    d = sum([x[6] for x in A]) # nMode INcorrect\n",
    "    e = sum([x[8] for x in A]) # nRandom Incorrect\n",
    "    f = sum([x[10] for x in A]) # nIVEware_seed_0 Incorrect\n",
    "    g = sum([x[12] for x in A]) # nIVEware_seed_1 Incorrect\n",
    "    h = round(b/a*100,2)\n",
    "    i = round(c/a*100,2)\n",
    "    j = round(d/a*100,2)\n",
    "    k = round(e/a*100,2)\n",
    "    l = round(f/a*100,2)\n",
    "    m = round(g/a*100,2)\n",
    "\n",
    "    RF_less_MF = sum([x[2] < x[4] for x in A])\n",
    "    RF_equal_MF = sum([x[2] == x[4] for x in A])\n",
    "    RF_greater_MF = sum([x[2] > x[4] for x in A])\n",
    "    \n",
    "    RF_less_Mode = sum([x[2] < x[6] for x in A])\n",
    "    RF_equal_Mode = sum([x[2] == x[6] for x in A])\n",
    "    RF_greater_Mode = sum([x[2] > x[6] for x in A])\n",
    "\n",
    "    RF_less_Random = sum([x[2] < x[8] for x in A])\n",
    "    RF_equal_Random = sum([x[2] == x[8] for x in A])\n",
    "    RF_greater_Random = sum([x[2] > x[8] for x in A])\n",
    "\n",
    "    RF_less_IVEware_seed_0 = sum([x[2] < x[10] for x in A])\n",
    "    RF_equal_IVEware_seed_0 = sum([x[2] == x[10] for x in A])\n",
    "    RF_greater_IVEware_seed_0 = sum([x[2] > x[10] for x in A])\n",
    "\n",
    "    RF_less_IVEware_seed_1 = sum([x[2] < x[12] for x in A])\n",
    "    RF_equal_IVEware_seed_1 = sum([x[2] == x[12] for x in A])\n",
    "    RF_greater_IVEware_seed_1 = sum([x[2] > x[12] for x in A])\n",
    "\n",
    "    MF_less_Mode = sum([x[4] < x[6] for x in A])\n",
    "    MF_equal_Mode = sum([x[4] == x[6] for x in A])\n",
    "    MF_greater_Mode = sum([x[4] > x[6] for x in A])\n",
    "\n",
    "    MF_less_Random = sum([x[4] < x[8] for x in A])\n",
    "    MF_equal_Random = sum([x[4] == x[8] for x in A])\n",
    "    MF_greater_Random = sum([x[4] > x[8] for x in A])\n",
    "\n",
    "    MF_less_IVEware_seed_0 = sum([x[4] < x[10] for x in A])\n",
    "    MF_equal_IVEware_seed_0 = sum([x[4] == x[10] for x in A])\n",
    "    MF_greater_IVEware_seed_0 = sum([x[4] > x[10] for x in A])\n",
    "\n",
    "    MF_less_IVEware_seed_1 = sum([x[4] < x[12] for x in A])\n",
    "    MF_equal_IVEware_seed_1 = sum([x[4] == x[12] for x in A])\n",
    "    MF_greater_IVEware_seed_1 = sum([x[4] > x[12] for x in A])\n",
    "\n",
    "    Mode_less_Random = sum([x[6] < x[8] for x in A])\n",
    "    Mode_equal_Random = sum([x[6] == x[8] for x in A])\n",
    "    Mode_greater_Random = sum([x[6] > x[8] for x in A])\n",
    "\n",
    "    Mode_less_IVEware_seed_0 = sum([x[6] < x[10] for x in A])\n",
    "    Mode_equal_IVEware_seed_0 = sum([x[6] == x[10] for x in A])\n",
    "    Mode_greater_IVEware_seed_0 = sum([x[6] > x[10] for x in A])\n",
    "\n",
    "    Mode_less_IVEware_seed_1 = sum([x[6] < x[12] for x in A])\n",
    "    Mode_equal_IVEware_seed_1 = sum([x[6] == x[12] for x in A])\n",
    "    Mode_greater_IVEware_seed_1 = sum([x[6] > x[12] for x in A])\n",
    "\n",
    "    Random_less_IVEware_seed_0 = sum([x[8] < x[10] for x in A])\n",
    "    Random_equal_IVEware_seed_0 = sum([x[8] == x[10] for x in A])\n",
    "    Random_greater_IVEware_seed_0 = sum([x[8] > x[10] for x in A])\n",
    "\n",
    "    Random_less_IVEware_seed_1 = sum([x[8] < x[12] for x in A])\n",
    "    Random_equal_IVEware_seed_1 = sum([x[8] == x[12] for x in A])\n",
    "    Random_greater_IVEware_seed_1 = sum([x[8] > x[12] for x in A])\n",
    "\n",
    "    IVEware_seed_0_less_IVEware_seed_1 = sum([x[10] < x[12] for x in A])\n",
    "    IVEware_seed_0_equal_IVEware_seed_1 = sum([x[10] == x[12] for x in A])\n",
    "    IVEware_seed_0_greater_IVEware_seed_1 = sum([x[10] > x[12] for x in A])\n",
    "\n",
    "    print ()\n",
    "    print ('    | | Number | NaN Imputed Incorrectly |')\n",
    "    print ('    | --- | --- | --- | ')    \n",
    "    print ('    | Total NaN | ', f'{a:,d}', ' | 100% | ')\n",
    "    print ('    | RF | ', f'{b:,d}', ' | ', h, '% | ')\n",
    "    print ('    | MF | ', f'{c:,d}', ' | ', i, '% | ')\n",
    "    print ('    | Mode | ', f'{d:,d}', ' | ', j, '% | ')\n",
    "    print ('    | Random | ', f'{e:,d}', ' | ', k, '% | ')\n",
    "    print ('    | IVEware_seed_0 | ', f'{f:,d}', ' | ', l, '% | ')\n",
    "    print ('    | IVEware_seed_1 | ', f'{g:,d}', ' | ', m, '% | ')\n",
    "    print ()\n",
    "    print ()\n",
    "    print ('    | | Number | NaN Imputed Correctly |')\n",
    "    print ('    | --- | --- | --- | ')    \n",
    "    print ('    | Total NaN | ', f'{a:,d}', ' |  | ')\n",
    "    print ('    | RF | ', f'{b:,d}', ' | ', round(100-h,2), '% | ')\n",
    "    print ('    | MF | ', f'{c:,d}', ' | ', round(100-i,2), '% | ')\n",
    "    print ('    | Mode | ', f'{d:,d}', ' | ', round(100-j,2), '% | ')\n",
    "    print ('    | Random | ', f'{e:,d}', ' | ', round(100-k,2), '% | ')\n",
    "    print ('    | IVEware_seed_0 | ', f'{f:,d}', ' | ', round(100-l,2), '% | ')\n",
    "    print ('    | IVEware_seed_1 | ', f'{g:,d}', ' | ', round(100-m,2), '% | ')\n",
    "    print ()\n",
    "    print ('    |  | Fewer | Equal | More | Total | ')\n",
    "    print ('    | --- | --- | --- | --- | --- | ')\n",
    "    print ('    | Compare RF to MF | ', RF_less_MF, ' | ', RF_equal_MF,  ' | ' ,RF_greater_MF,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to Mode | ', RF_less_Mode, ' | ', RF_equal_Mode,  ' | ' ,RF_greater_Mode,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to Random | ', RF_less_Random, ' | ' , RF_equal_Random,  ' | ' , RF_greater_Random,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to IVEware_seed_0 | ', RF_less_IVEware_seed_0, ' | ' , RF_equal_IVEware_seed_0, ' | ' , RF_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare RF to IVEware_seed_0 | ', RF_less_IVEware_seed_1, ' | ' , RF_equal_IVEware_seed_1, ' | ' , RF_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to Mode | ', MF_less_Mode, ' | ', MF_equal_Mode,  ' | ' ,MF_greater_Mode,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to Random | ', MF_less_Random, ' | ' , MF_equal_Random,  ' | ' , MF_greater_Random,  ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to IVEware_seed_0 | ', MF_less_IVEware_seed_0, ' | ' , MF_equal_IVEware_seed_0, ' | ' , MF_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare MF to IVEware_seed_1 | ', MF_less_IVEware_seed_1, ' | ' , MF_equal_IVEware_seed_1, ' | ' , MF_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to Random | ', Mode_less_Random, ' | ' , Mode_equal_Random, ' | ' , Mode_greater_Random, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to IVEware_seed_0 | ', Mode_less_IVEware_seed_0, ' | ' , Mode_equal_IVEware_seed_0, ' | ' , Mode_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Mode to IVEware_seed_1 | ', Mode_less_IVEware_seed_1, ' | ' , Mode_equal_IVEware_seed_1, ' | ' , Mode_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Random to IVEware_seed_0 | ', Random_less_IVEware_seed_0, ' | ' , Random_equal_IVEware_seed_0, ' | ' , Random_greater_IVEware_seed_0, ' |', len(A), ' |' )\n",
    "    print ('    | Compare Random to IVEware_seed_1 | ', Random_less_IVEware_seed_1, ' | ' , Random_equal_IVEware_seed_1, ' | ' , Random_greater_IVEware_seed_1, ' |', len(A), ' |' )\n",
    "    print ()\n",
    "    \n",
    "    b = sum([x[14] for x in A])\n",
    "    c = sum([x[16] for x in A])\n",
    "    d = sum([x[18] for x in A])\n",
    "    e = sum([x[20] for x in A])\n",
    "    f = sum([x[22] for x in A])\n",
    "    g = sum([x[24] for x in A])\n",
    "    h = sum([x[26] for x in A])\n",
    "    i = sum([x[28] for x in A])\n",
    "    j = sum([x[30] for x in A])\n",
    "    k = sum([x[32] for x in A])\n",
    "    l = sum([x[34] for x in A])\n",
    "    m = sum([x[36] for x in A])\n",
    "    n = sum([x[38] for x in A])\n",
    "    o = sum([x[40] for x in A])\n",
    "    p = sum([x[42] for x in A])\n",
    "    \n",
    "    print ('    |  | Number |  Percentage |')\n",
    "    print ('    | --- | --- | -- |')\n",
    "    print ('    | Total NaN | ', f'{a:,d}', ' | 100% |' )\n",
    "    print ('    | RF Different from MF | ', f'{b:,d}', ' | ', round(b/a*100,2), '% |')\n",
    "    print ('    | RF Different from Mode | ', f'{c:,d}', ' | ', round(c/a*100,2), '% |')\n",
    "    print ('    | RF Different from Random | ', f'{d:,d}', ' | ', round(d/a*100,2), '% |')\n",
    "    print ('    | RF Different from IVEware_seed_0 | ', f'{e:,d}', ' | ', round(e/a*100,2), '% |')\n",
    "    print ('    | RF Different from IVEware_seed_1 | ', f'{f:,d}', ' | ', round(f/a*100,2), '% |')\n",
    "    print ('    | MF Different from Mode | ', f'{g:,d}', ' | ', round(g/a*100,2), '% |')\n",
    "    print ('    | MF Different from Random | ', f'{h:,d}', ' | ',  round(h/a*100,2), '% |')\n",
    "    print ('    | MF Different from IVEware_seed_0 | ', f'{i:,d}', ' | ',  round(i/a*100,2), '% |')\n",
    "    print ('    | MF Different from IVEware_seed_1 | ', f'{j:,d}', ' | ', round(j/a*100,2), '% |')\n",
    "    print ('    | Mode Different from Random | ', f'{k:,d}', ' | ', round(k/a*100,2), '% |')\n",
    "    print ('    | Mode Different from IVEware_seed_0 | ', f'{l:,d}', ' | ', round(l/a*100,2), '% |')\n",
    "    print ('    | Mode Different from IVEware_seed_1 | ', f'{m:,d}', ' | ', round(m/a*100,2), '% |')\n",
    "    print ('    | Random Different from IVEware_seed_0 | ', f'{n:,d}', ' | ', round(n/a*100,2), '% |')\n",
    "    print ('    | Random Different from IVEware_seed_1 | ', f'{o:,d}', ' | ', round(o/a*100,2), '% |')\n",
    "    print ('    | IVEware_seed_0 Different from IVEware_seed_1 | ', f'{p:,d}', ' | ', round(p/a*100,2), '% |')\n",
    "    print ()\n",
    "        \n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "    \n",
    "    print ('Finished Compare_Imputation_Methods_Part_2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fa807",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Compare_Imputation_Methods_Part_2(\n",
    "    data_Ground_Truth, data_NaN, \n",
    "    data_RF, data_MF, \n",
    "    data_Mode, data_Random, \n",
    "    data_IVEware_seed_0, data_IVEware_seed_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03ad4a",
   "metadata": {},
   "source": [
    "# Impute using Random Forest and Save for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Impute_Using_Random_Forest():\n",
    "    print ('Impute_Using_Random_Forest()')\n",
    "    data = Get_Data()\n",
    "    data = data.replace({99:np.nan})\n",
    "    \n",
    "#    data_Imputed = Impute_Full(data)\n",
    "    data_Imputed = Impute_Round_Robin(data)\n",
    "    data_Imputed.to_csv('../../Big_Files/CRSS_Imputed_by_RF_Data.csv', index=False)\n",
    "#    display(data_Imputed.head(50))\n",
    "    \n",
    "    Check(data, data_Imputed)\n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "\n",
    "    print ('Finished Impute_Using_Random_Forest()')\n",
    "    print ()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "#Impute_Using_Random_Forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b60d50",
   "metadata": {},
   "source": [
    "# Impute using IVEware\n",
    "- Get this message:\n",
    "    - Warning: Too many iterations, probably due to colinearity with dependent variable\n",
    "- With filename_prefix '_0_0', meaning that we didn't do dimensionality reduction before imputing, on these features\n",
    "    - Iteration 1:\n",
    "        - ACC_TYPE\n",
    "        - CARGO_BT\n",
    "        - HARM_EV\n",
    "        - MAKE\n",
    "        - MAX_VSEV\n",
    "        - NUM_INJ\n",
    "        - TOWED\n",
    "        - TYP_INT\n",
    "    - Iteration 2:\n",
    "        - AIR_BAG\n",
    "        - BODY_TYP\n",
    "        - CARGO_BT\n",
    "        - HARM_EV\n",
    "        - MAX_SEV\n",
    "        - NUM_INJ\n",
    "        - NUM_INJV\n",
    "        - TOWED\n",
    "        - TYP_INT\n",
    "        - VSPD_LIM\n",
    "     - Iteration 3:\n",
    "         - AIR_BAG\n",
    "         - BODY_TYP\n",
    "         - CARGO_BT\n",
    "         - HARM_EV\n",
    "         - MAKE\n",
    "         - MAK_MOD\n",
    "         - MAX_SEV\n",
    "         - NUM_INJ\n",
    "         - TOWED\n",
    "         - TYP_INT\n",
    "         - VSPD_LIM\n",
    "    - ... and similarly through Iteration 10. \n",
    "- With dimensionality reduction:\n",
    "    - Iteration 1:\n",
    "        - ACC_TYPE\n",
    "        - CARGO_BT\n",
    "        - HARM_EV\n",
    "        - MAKE\n",
    "        - [MAX_VSEV Removed]\n",
    "        - NUM_INJ\n",
    "        - TOWED\n",
    "        - TYP_INT\n",
    "    - Looks the same\n",
    "- Some of these features have an $R^2$ score greater than 0.8, so if we cut our $R^2$ threshold from 0.9 to 0.8 we would cut out some, but not all, of these.\n",
    "    - BODY_TYP\n",
    "    - HARM_EV\n",
    "    - INJ_SEV\n",
    "    - MODEL\n",
    "    - MAX_SEV\n",
    "    - NUM_INJ\n",
    "    - NUM_INJV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af31f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute_Using_IVEware()\n",
      "Get_Data\n",
      "../../Big_Files/CRSS_02_1_1.csv\n",
      "\n",
      "data.shape =  (802700, 64)\n",
      "\n",
      "Drop Imputed Columns\n",
      "data.shape =  (802700, 64)\n",
      "\n",
      "Total number of NaN\n",
      "1599835\n",
      "\n",
      "Finished Get_Data()\n",
      "\n",
      "(802700, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>EJECTION</th>\n",
       "      <th>...</th>\n",
       "      <th>VEH_AGE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>HOSPITAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  DEFORMED  \\\n",
       "0         9    9        4          99         2         0         1         2   \n",
       "1         8    6        4          99         3         0         1         2   \n",
       "2         5    3        1          99         7         0         1         0   \n",
       "3         3    4        1          99         2         0         1         0   \n",
       "4         3    4        3           2         2         0         1         0   \n",
       "5         3    3        3           2         2         0         1         0   \n",
       "6         2    6        4          99         2         0         2         0   \n",
       "7         0    5        4           2         2         0         4         4   \n",
       "8         8    7       99           2         7         0         3         3   \n",
       "9         7    6       99           2         2         0         3         4   \n",
       "\n",
       "   DR_ZIP  EJECTION  ...  VEH_AGE  VE_TOTAL  VPROFILE  VSPD_LIM  VSURCOND  \\\n",
       "0       8         0  ...        6         1         3         2         0   \n",
       "1       8         0  ...        1         1         3         2         0   \n",
       "2       8         0  ...        9         1         2         2         0   \n",
       "3       8         0  ...        5         1         2         2         0   \n",
       "4       8         0  ...        5         1         2         2         0   \n",
       "5       8         0  ...        5         1         2         2         0   \n",
       "6       8         1  ...        6         0         3         5         0   \n",
       "7       8         0  ...        5         1         2         3         1   \n",
       "8       5         0  ...        9         1         0         3         0   \n",
       "9       5         0  ...        2         1         0         3         0   \n",
       "\n",
       "   VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  HOSPITAL  \n",
       "0        99        99        0         0         0  \n",
       "1        99        99        0         0         0  \n",
       "2        99        99        0         0         0  \n",
       "3        99        99        0         0         0  \n",
       "4        99        99        0         0         0  \n",
       "5        99        99        0         0         0  \n",
       "6        99         1        0         0         1  \n",
       "7        99        99        0         0         0  \n",
       "8         0         1        0         0         0  \n",
       "9         0         1        0         0         0  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>EJECTION</th>\n",
       "      <th>...</th>\n",
       "      <th>VEH_AGE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>HOSPITAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ACC_TYPE AGE AIR_BAG ALC_STATUS BODY_TYP CARGO_BT  DAY_WEEK DEFORMED DR_ZIP  \\\n",
       "0        9   9       4                   2        0         1        2      8   \n",
       "1        8   6       4                   3        0         1        2      8   \n",
       "2        5   3       1                   7        0         1        0      8   \n",
       "3        3   4       1                   2        0         1        0      8   \n",
       "4        3   4       3          2        2        0         1        0      8   \n",
       "5        3   3       3          2        2        0         1        0      8   \n",
       "6        2   6       4                   2        0         2        0      8   \n",
       "7        0   5       4          2        2        0         4        4      8   \n",
       "8        8   7                  2        7        0         3        3      5   \n",
       "9        7   6                  2        2        0         3        4      5   \n",
       "\n",
       "  EJECTION  ... VEH_AGE VE_TOTAL VPROFILE VSPD_LIM VSURCOND VTRAFCON  \\\n",
       "0        0  ...       6        1        3        2        0            \n",
       "1        0  ...       1        1        3        2        0            \n",
       "2        0  ...       9        1        2        2        0            \n",
       "3        0  ...       5        1        2        2        0            \n",
       "4        0  ...       5        1        2        2        0            \n",
       "5        0  ...       5        1        2        2        0            \n",
       "6        1  ...       6        0        3        5        0            \n",
       "7        0  ...       5        1        2        3        1            \n",
       "8        0  ...       9        1        0        3        0        0   \n",
       "9        0  ...       2        1        0        3        0        0   \n",
       "\n",
       "   VTRAFWAY WEATHER WRK_ZONE HOSPITAL  \n",
       "0                 0        0        0  \n",
       "1                 0        0        0  \n",
       "2                 0        0        0  \n",
       "3                 0        0        0  \n",
       "4                 0        0        0  \n",
       "5                 0        0        0  \n",
       "6         1       0        0        1  \n",
       "7                 0        0        0  \n",
       "8         1       0        0        0  \n",
       "9         1       0        0        0  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Big_Files/data_IVEware_1_1_1.txt\n",
      "\n",
      "Finished Impute_Using_IVEware()\n",
      "\n",
      "CPU times: user 7.71 s, sys: 686 ms, total: 8.4 s\n",
      "Wall time: 8.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def Impute_Using_IVEware():\n",
    "    print ('Impute_Using_IVEware()')\n",
    "    data = Get_Data()\n",
    "    \n",
    "    # Create .txt file to feed into IVEware imputation\n",
    "    data_IVEware = data.copy(deep=True)\n",
    "    print (data_IVEware.shape)\n",
    "    display(data_IVEware.head(10))\n",
    "    \n",
    "    data_IVEware = data_IVEware.replace(99,'')\n",
    "    display(data_IVEware.head(10))\n",
    "#    data_IVEware.to_csv('../../Big_Files/data_IVEware.txt', sep='\\t', index=False)\n",
    "\n",
    "    # The first '_0' or '_1' is for the random seed for Python and Numpy\n",
    "    # The second '_0' (or '_1') is for the dimensionality not being reduced (being reduced) before binning\n",
    "    # The third '_1' is for using IVEware with R random seed 0\n",
    "    filename = '../../Big_Files/data_IVEware' + filename_prefix + '_1.txt'\n",
    "    print (filename)\n",
    "    print ()\n",
    "    data_IVEware.to_csv(filename, sep='\\t', index=False) # Dimensionality not reduced before binning\n",
    "#    data_IVEware.to_csv('../../Big_Files/data_IVEware_1_1.txt', sep='\\t', index=False) # Dimensionality reduced before binning\n",
    "    \n",
    "    print ('Finished Impute_Using_IVEware()')\n",
    "    print ()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "Impute_Using_IVEware()\n",
    "\n",
    "# Now run IVEware outside this notebook.\n",
    "# Takes about 1 GB of memory\n",
    "# Run twice, once with seed 0 and once with seed 1, writing to different files\n",
    "# About one hour for each of two runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333dbb5",
   "metadata": {},
   "source": [
    "# Impute using MissForest and Save for Next Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18739d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute_Using_MissForest()\n",
      "Get_Data\n",
      "../../Big_Files/CRSS_02_1_1.csv\n",
      "\n",
      "data.shape =  (802700, 64)\n",
      "\n",
      "Drop Imputed Columns\n",
      "data.shape =  (802700, 64)\n",
      "\n",
      "Total number of NaN\n",
      "1599835\n",
      "\n",
      "Finished Get_Data()\n",
      "\n",
      "Impute_MissForest()\n",
      "(802700, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>EJECTION</th>\n",
       "      <th>...</th>\n",
       "      <th>VEH_AGE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>HOSPITAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  \\\n",
       "0        9.0  9.0      4.0         NaN       2.0       0.0         1   \n",
       "1        8.0  6.0      4.0         NaN       3.0       0.0         1   \n",
       "2        5.0  3.0      1.0         NaN       7.0       0.0         1   \n",
       "3        3.0  4.0      1.0         NaN       2.0       0.0         1   \n",
       "4        3.0  4.0      3.0         2.0       2.0       0.0         1   \n",
       "5        3.0  3.0      3.0         2.0       2.0       0.0         1   \n",
       "6        2.0  6.0      4.0         NaN       2.0       0.0         2   \n",
       "7        0.0  5.0      4.0         2.0       2.0       0.0         4   \n",
       "8        8.0  7.0      NaN         2.0       7.0       0.0         3   \n",
       "9        7.0  6.0      NaN         2.0       2.0       0.0         3   \n",
       "10       7.0  1.0      NaN         2.0       2.0       0.0         3   \n",
       "11       7.0  2.0      NaN         2.0       2.0       0.0         3   \n",
       "12       7.0  7.0      NaN         2.0       2.0       0.0         3   \n",
       "13       8.0  6.0      NaN         2.0       7.0       0.0         4   \n",
       "14       7.0  9.0      NaN         2.0       7.0       0.0         4   \n",
       "15       NaN  7.0      NaN         2.0       4.0       0.0         4   \n",
       "16       NaN  7.0      NaN         2.0       4.0       0.0         4   \n",
       "17       8.0  6.0      NaN         2.0       7.0       0.0         1   \n",
       "18       5.0  9.0      NaN         2.0       7.0       0.0         3   \n",
       "19       3.0  7.0      NaN         2.0       7.0       0.0         3   \n",
       "\n",
       "    DEFORMED  DR_ZIP  EJECTION  ...  VEH_AGE  VE_TOTAL  VPROFILE  VSPD_LIM  \\\n",
       "0        2.0     8.0       0.0  ...      6.0         1       3.0       2.0   \n",
       "1        2.0     8.0       0.0  ...      1.0         1       3.0       2.0   \n",
       "2        0.0     8.0       0.0  ...      9.0         1       2.0       2.0   \n",
       "3        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "4        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "5        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "6        0.0     8.0       1.0  ...      6.0         0       3.0       5.0   \n",
       "7        4.0     8.0       0.0  ...      5.0         1       2.0       3.0   \n",
       "8        3.0     5.0       0.0  ...      9.0         1       0.0       3.0   \n",
       "9        4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "10       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "11       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "12       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "13       0.0     5.0       0.0  ...      1.0         3       3.0       4.0   \n",
       "14       0.0     5.0       0.0  ...      5.0         3       3.0       4.0   \n",
       "15       2.0     5.0       0.0  ...      0.0         3       3.0       4.0   \n",
       "16       4.0     4.0       0.0  ...      4.0         3       3.0       4.0   \n",
       "17       3.0     5.0       0.0  ...      1.0         1       3.0       6.0   \n",
       "18       0.0     4.0       0.0  ...      3.0         1       3.0       3.0   \n",
       "19       0.0     9.0       0.0  ...      1.0         1       3.0       3.0   \n",
       "\n",
       "    VSURCOND  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  HOSPITAL  \n",
       "0        0.0       NaN       NaN      0.0         0         0  \n",
       "1        0.0       NaN       NaN      0.0         0         0  \n",
       "2        0.0       NaN       NaN      0.0         0         0  \n",
       "3        0.0       NaN       NaN      0.0         0         0  \n",
       "4        0.0       NaN       NaN      0.0         0         0  \n",
       "5        0.0       NaN       NaN      0.0         0         0  \n",
       "6        0.0       NaN       1.0      0.0         0         1  \n",
       "7        1.0       NaN       NaN      0.0         0         0  \n",
       "8        0.0       0.0       1.0      0.0         0         0  \n",
       "9        0.0       0.0       1.0      0.0         0         0  \n",
       "10       0.0       0.0       1.0      0.0         0         0  \n",
       "11       0.0       0.0       1.0      0.0         0         0  \n",
       "12       0.0       0.0       1.0      0.0         0         0  \n",
       "13       0.0       0.0       1.0      0.0         0         0  \n",
       "14       0.0       0.0       1.0      0.0         0         1  \n",
       "15       0.0       0.0       1.0      0.0         0         0  \n",
       "16       0.0       0.0       1.0      0.0         0         0  \n",
       "17       0.0       0.0       1.0      0.0         0         0  \n",
       "18       0.0       1.0       0.0      0.0         0         0  \n",
       "19       0.0       1.0       0.0      0.0         0         0  \n",
       "\n",
       "[20 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:  ['ACC_TYPE', 'AGE', 'AIR_BAG', 'ALC_STATUS', 'BODY_TYP', 'CARGO_BT', 'DAY_WEEK', 'DEFORMED', 'DR_ZIP', 'EJECTION', 'HARM_EV', 'HIT_RUN', 'HOUR', 'IMPACT1', 'INJ_SEV', 'INT_HWY', 'J_KNIFE', 'LGT_COND', 'MAKE', 'MAK_MOD', 'MAN_COLL', 'MAX_SEV', 'MODEL', 'MONTH', 'M_HARM', 'NUMOCCS', 'NUM_INJ', 'NUM_INJV', 'PCRASH4', 'PCRASH5', 'PERMVIT', 'PER_TYP', 'PJ', 'PSU', 'PVH_INVL', 'P_CRASH1', 'P_CRASH2', 'REGION', 'RELJCT1', 'RELJCT2', 'REL_ROAD', 'REST_MIS', 'REST_USE', 'ROLINLOC', 'ROLLOVER', 'SEAT_POS', 'SEX', 'SPEC_USE', 'SPEEDREL', 'TOWED', 'TOW_VEH', 'TYP_INT', 'URBANICITY', 'VALIGN', 'VEH_AGE', 'VE_TOTAL', 'VPROFILE', 'VSPD_LIM', 'VSURCOND', 'VTRAFCON', 'VTRAFWAY', 'WEATHER', 'WRK_ZONE', 'HOSPITAL']\n",
      "Start fit_transform()\n",
      "Start transform()\n",
      "Iteration  0  of  10 , feature  ACC_TYPE\n",
      "Iteration  0  of  10 , feature  AGE\n",
      "Iteration  0  of  10 , feature  AIR_BAG\n",
      "Iteration  0  of  10 , feature  ALC_STATUS\n",
      "Iteration  0  of  10 , feature  BODY_TYP\n",
      "Iteration  0  of  10 , feature  CARGO_BT\n",
      "Iteration  0  of  10 , feature  DEFORMED\n",
      "Iteration  0  of  10 , feature  DR_ZIP\n",
      "Iteration  0  of  10 , feature  EJECTION\n",
      "Iteration  0  of  10 , feature  HARM_EV\n",
      "Iteration  0  of  10 , feature  HIT_RUN\n",
      "Iteration  0  of  10 , feature  HOUR\n",
      "Iteration  0  of  10 , feature  IMPACT1\n",
      "Iteration  0  of  10 , feature  INJ_SEV\n",
      "Iteration  0  of  10 , feature  INT_HWY\n",
      "Iteration  0  of  10 , feature  LGT_COND\n",
      "Iteration  0  of  10 , feature  MAKE\n",
      "Iteration  0  of  10 , feature  MAK_MOD\n",
      "Iteration  0  of  10 , feature  MAN_COLL\n",
      "Iteration  0  of  10 , feature  MAX_SEV\n",
      "Iteration  0  of  10 , feature  MODEL\n",
      "Iteration  0  of  10 , feature  M_HARM\n",
      "Iteration  0  of  10 , feature  NUMOCCS\n",
      "Iteration  0  of  10 , feature  NUM_INJ\n",
      "Iteration  0  of  10 , feature  NUM_INJV\n",
      "Iteration  0  of  10 , feature  PCRASH4\n",
      "Iteration  0  of  10 , feature  PCRASH5\n",
      "Iteration  0  of  10 , feature  PER_TYP\n",
      "Iteration  0  of  10 , feature  P_CRASH1\n",
      "Iteration  0  of  10 , feature  P_CRASH2\n",
      "Iteration  0  of  10 , feature  RELJCT1\n",
      "Iteration  0  of  10 , feature  RELJCT2\n",
      "Iteration  0  of  10 , feature  REL_ROAD\n",
      "Iteration  0  of  10 , feature  REST_USE\n",
      "Iteration  0  of  10 , feature  ROLINLOC\n",
      "Iteration  0  of  10 , feature  SEAT_POS\n",
      "Iteration  0  of  10 , feature  SEX\n",
      "Iteration  0  of  10 , feature  SPEC_USE\n",
      "Iteration  0  of  10 , feature  SPEEDREL\n",
      "Iteration  0  of  10 , feature  TOWED\n",
      "Iteration  0  of  10 , feature  TOW_VEH\n",
      "Iteration  0  of  10 , feature  TYP_INT\n",
      "Iteration  0  of  10 , feature  VALIGN\n",
      "Iteration  0  of  10 , feature  VEH_AGE\n",
      "Iteration  0  of  10 , feature  VPROFILE\n",
      "Iteration  0  of  10 , feature  VSPD_LIM\n",
      "Iteration  0  of  10 , feature  VSURCOND\n",
      "Iteration  0  of  10 , feature  VTRAFCON\n",
      "Iteration  0  of  10 , feature  VTRAFWAY\n",
      "Iteration  0  of  10 , feature  WEATHER\n",
      "compute_gamma_categorical()\n",
      "len(self.categorical) = 64\n",
      "all_gamma_cat =  [6876.015625]\n",
      "Iteration  1  of  10 , feature  ACC_TYPE\n",
      "Iteration  1  of  10 , feature  AGE\n",
      "Iteration  1  of  10 , feature  AIR_BAG\n",
      "Iteration  1  of  10 , feature  ALC_STATUS\n",
      "Iteration  1  of  10 , feature  BODY_TYP\n",
      "Iteration  1  of  10 , feature  CARGO_BT\n",
      "Iteration  1  of  10 , feature  DEFORMED\n",
      "Iteration  1  of  10 , feature  DR_ZIP\n",
      "Iteration  1  of  10 , feature  EJECTION\n",
      "Iteration  1  of  10 , feature  HARM_EV\n",
      "Iteration  1  of  10 , feature  HIT_RUN\n",
      "Iteration  1  of  10 , feature  HOUR\n",
      "Iteration  1  of  10 , feature  IMPACT1\n",
      "Iteration  1  of  10 , feature  INJ_SEV\n",
      "Iteration  1  of  10 , feature  INT_HWY\n",
      "Iteration  1  of  10 , feature  LGT_COND\n",
      "Iteration  1  of  10 , feature  MAKE\n",
      "Iteration  1  of  10 , feature  MAK_MOD\n",
      "Iteration  1  of  10 , feature  MAN_COLL\n",
      "Iteration  1  of  10 , feature  MAX_SEV\n",
      "Iteration  1  of  10 , feature  MODEL\n",
      "Iteration  1  of  10 , feature  M_HARM\n",
      "Iteration  1  of  10 , feature  NUMOCCS\n",
      "Iteration  1  of  10 , feature  NUM_INJ\n",
      "Iteration  1  of  10 , feature  NUM_INJV\n",
      "Iteration  1  of  10 , feature  PCRASH4\n",
      "Iteration  1  of  10 , feature  PCRASH5\n",
      "Iteration  1  of  10 , feature  PER_TYP\n",
      "Iteration  1  of  10 , feature  P_CRASH1\n",
      "Iteration  1  of  10 , feature  P_CRASH2\n",
      "Iteration  1  of  10 , feature  RELJCT1\n",
      "Iteration  1  of  10 , feature  RELJCT2\n",
      "Iteration  1  of  10 , feature  REL_ROAD\n",
      "Iteration  1  of  10 , feature  REST_USE\n",
      "Iteration  1  of  10 , feature  ROLINLOC\n",
      "Iteration  1  of  10 , feature  SEAT_POS\n",
      "Iteration  1  of  10 , feature  SEX\n",
      "Iteration  1  of  10 , feature  SPEC_USE\n",
      "Iteration  1  of  10 , feature  SPEEDREL\n",
      "Iteration  1  of  10 , feature  TOWED\n",
      "Iteration  1  of  10 , feature  TOW_VEH\n",
      "Iteration  1  of  10 , feature  TYP_INT\n",
      "Iteration  1  of  10 , feature  VALIGN\n",
      "Iteration  1  of  10 , feature  VEH_AGE\n",
      "Iteration  1  of  10 , feature  VPROFILE\n",
      "Iteration  1  of  10 , feature  VSPD_LIM\n",
      "Iteration  1  of  10 , feature  VSURCOND\n",
      "Iteration  1  of  10 , feature  VTRAFCON\n",
      "Iteration  1  of  10 , feature  VTRAFWAY\n",
      "Iteration  1  of  10 , feature  WEATHER\n",
      "compute_gamma_categorical()\n",
      "len(self.categorical) = 64\n",
      "all_gamma_cat =  [6876.015625, 1721.203125]\n",
      "Iteration  2  of  10 , feature  ACC_TYPE\n",
      "Iteration  2  of  10 , feature  AGE\n",
      "Iteration  2  of  10 , feature  AIR_BAG\n",
      "Iteration  2  of  10 , feature  ALC_STATUS\n",
      "Iteration  2  of  10 , feature  BODY_TYP\n",
      "Iteration  2  of  10 , feature  CARGO_BT\n",
      "Iteration  2  of  10 , feature  DEFORMED\n",
      "Iteration  2  of  10 , feature  DR_ZIP\n",
      "Iteration  2  of  10 , feature  EJECTION\n",
      "Iteration  2  of  10 , feature  HARM_EV\n",
      "Iteration  2  of  10 , feature  HIT_RUN\n",
      "Iteration  2  of  10 , feature  HOUR\n",
      "Iteration  2  of  10 , feature  IMPACT1\n",
      "Iteration  2  of  10 , feature  INJ_SEV\n",
      "Iteration  2  of  10 , feature  INT_HWY\n",
      "Iteration  2  of  10 , feature  LGT_COND\n",
      "Iteration  2  of  10 , feature  MAKE\n",
      "Iteration  2  of  10 , feature  MAK_MOD\n",
      "Iteration  2  of  10 , feature  MAN_COLL\n",
      "Iteration  2  of  10 , feature  MAX_SEV\n",
      "Iteration  2  of  10 , feature  MODEL\n",
      "Iteration  2  of  10 , feature  M_HARM\n",
      "Iteration  2  of  10 , feature  NUMOCCS\n",
      "Iteration  2  of  10 , feature  NUM_INJ\n",
      "Iteration  2  of  10 , feature  NUM_INJV\n",
      "Iteration  2  of  10 , feature  PCRASH4\n",
      "Iteration  2  of  10 , feature  PCRASH5\n",
      "Iteration  2  of  10 , feature  PER_TYP\n",
      "Iteration  2  of  10 , feature  P_CRASH1\n",
      "Iteration  2  of  10 , feature  P_CRASH2\n",
      "Iteration  2  of  10 , feature  RELJCT1\n",
      "Iteration  2  of  10 , feature  RELJCT2\n",
      "Iteration  2  of  10 , feature  REL_ROAD\n",
      "Iteration  2  of  10 , feature  REST_USE\n",
      "Iteration  2  of  10 , feature  ROLINLOC\n",
      "Iteration  2  of  10 , feature  SEAT_POS\n",
      "Iteration  2  of  10 , feature  SEX\n",
      "Iteration  2  of  10 , feature  SPEC_USE\n",
      "Iteration  2  of  10 , feature  SPEEDREL\n",
      "Iteration  2  of  10 , feature  TOWED\n",
      "Iteration  2  of  10 , feature  TOW_VEH\n",
      "Iteration  2  of  10 , feature  TYP_INT\n",
      "Iteration  2  of  10 , feature  VALIGN\n",
      "Iteration  2  of  10 , feature  VEH_AGE\n",
      "Iteration  2  of  10 , feature  VPROFILE\n",
      "Iteration  2  of  10 , feature  VSPD_LIM\n",
      "Iteration  2  of  10 , feature  VSURCOND\n",
      "Iteration  2  of  10 , feature  VTRAFCON\n",
      "Iteration  2  of  10 , feature  VTRAFWAY\n",
      "Iteration  2  of  10 , feature  WEATHER\n",
      "compute_gamma_categorical()\n",
      "len(self.categorical) = 64\n",
      "all_gamma_cat =  [6876.015625, 1721.203125, 1615.375]\n",
      "Iteration  3  of  10 , feature  ACC_TYPE\n",
      "Iteration  3  of  10 , feature  AGE\n",
      "Iteration  3  of  10 , feature  AIR_BAG\n",
      "Iteration  3  of  10 , feature  ALC_STATUS\n",
      "Iteration  3  of  10 , feature  BODY_TYP\n",
      "Iteration  3  of  10 , feature  CARGO_BT\n",
      "Iteration  3  of  10 , feature  DEFORMED\n",
      "Iteration  3  of  10 , feature  DR_ZIP\n",
      "Iteration  3  of  10 , feature  EJECTION\n",
      "Iteration  3  of  10 , feature  HARM_EV\n",
      "Iteration  3  of  10 , feature  HIT_RUN\n",
      "Iteration  3  of  10 , feature  HOUR\n",
      "Iteration  3  of  10 , feature  IMPACT1\n",
      "Iteration  3  of  10 , feature  INJ_SEV\n",
      "Iteration  3  of  10 , feature  INT_HWY\n",
      "Iteration  3  of  10 , feature  LGT_COND\n",
      "Iteration  3  of  10 , feature  MAKE\n",
      "Iteration  3  of  10 , feature  MAK_MOD\n",
      "Iteration  3  of  10 , feature  MAN_COLL\n",
      "Iteration  3  of  10 , feature  MAX_SEV\n",
      "Iteration  3  of  10 , feature  MODEL\n",
      "Iteration  3  of  10 , feature  M_HARM\n",
      "Iteration  3  of  10 , feature  NUMOCCS\n",
      "Iteration  3  of  10 , feature  NUM_INJ\n",
      "Iteration  3  of  10 , feature  NUM_INJV\n",
      "Iteration  3  of  10 , feature  PCRASH4\n",
      "Iteration  3  of  10 , feature  PCRASH5\n",
      "Iteration  3  of  10 , feature  PER_TYP\n",
      "Iteration  3  of  10 , feature  P_CRASH1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  3  of  10 , feature  P_CRASH2\n",
      "Iteration  3  of  10 , feature  RELJCT1\n",
      "Iteration  3  of  10 , feature  RELJCT2\n",
      "Iteration  3  of  10 , feature  REL_ROAD\n",
      "Iteration  3  of  10 , feature  REST_USE\n",
      "Iteration  3  of  10 , feature  ROLINLOC\n",
      "Iteration  3  of  10 , feature  SEAT_POS\n",
      "Iteration  3  of  10 , feature  SEX\n",
      "Iteration  3  of  10 , feature  SPEC_USE\n",
      "Iteration  3  of  10 , feature  SPEEDREL\n",
      "Iteration  3  of  10 , feature  TOWED\n",
      "Iteration  3  of  10 , feature  TOW_VEH\n",
      "Iteration  3  of  10 , feature  TYP_INT\n",
      "Iteration  3  of  10 , feature  VALIGN\n",
      "Iteration  3  of  10 , feature  VEH_AGE\n",
      "Iteration  3  of  10 , feature  VPROFILE\n",
      "Iteration  3  of  10 , feature  VSPD_LIM\n",
      "Iteration  3  of  10 , feature  VSURCOND\n",
      "Iteration  3  of  10 , feature  VTRAFCON\n",
      "Iteration  3  of  10 , feature  VTRAFWAY\n",
      "Iteration  3  of  10 , feature  WEATHER\n",
      "compute_gamma_categorical()\n",
      "len(self.categorical) = 64\n",
      "all_gamma_cat =  [6876.015625, 1721.203125, 1615.375, 1594.296875]\n",
      "Iteration  4  of  10 , feature  ACC_TYPE\n",
      "Iteration  4  of  10 , feature  AGE\n",
      "Iteration  4  of  10 , feature  AIR_BAG\n",
      "Iteration  4  of  10 , feature  ALC_STATUS\n",
      "Iteration  4  of  10 , feature  BODY_TYP\n",
      "Iteration  4  of  10 , feature  CARGO_BT\n",
      "Iteration  4  of  10 , feature  DEFORMED\n",
      "Iteration  4  of  10 , feature  DR_ZIP\n",
      "Iteration  4  of  10 , feature  EJECTION\n",
      "Iteration  4  of  10 , feature  HARM_EV\n",
      "Iteration  4  of  10 , feature  HIT_RUN\n",
      "Iteration  4  of  10 , feature  HOUR\n",
      "Iteration  4  of  10 , feature  IMPACT1\n",
      "Iteration  4  of  10 , feature  INJ_SEV\n",
      "Iteration  4  of  10 , feature  INT_HWY\n",
      "Iteration  4  of  10 , feature  LGT_COND\n",
      "Iteration  4  of  10 , feature  MAKE\n",
      "Iteration  4  of  10 , feature  MAK_MOD\n",
      "Iteration  4  of  10 , feature  MAN_COLL\n",
      "Iteration  4  of  10 , feature  MAX_SEV\n",
      "Iteration  4  of  10 , feature  MODEL\n",
      "Iteration  4  of  10 , feature  M_HARM\n",
      "Iteration  4  of  10 , feature  NUMOCCS\n",
      "Iteration  4  of  10 , feature  NUM_INJ\n",
      "Iteration  4  of  10 , feature  NUM_INJV\n",
      "Iteration  4  of  10 , feature  PCRASH4\n",
      "Iteration  4  of  10 , feature  PCRASH5\n",
      "Iteration  4  of  10 , feature  PER_TYP\n",
      "Iteration  4  of  10 , feature  P_CRASH1\n",
      "Iteration  4  of  10 , feature  P_CRASH2\n",
      "Iteration  4  of  10 , feature  RELJCT1\n",
      "Iteration  4  of  10 , feature  RELJCT2\n",
      "Iteration  4  of  10 , feature  REL_ROAD\n",
      "Iteration  4  of  10 , feature  REST_USE\n",
      "Iteration  4  of  10 , feature  ROLINLOC\n",
      "Iteration  4  of  10 , feature  SEAT_POS\n",
      "Iteration  4  of  10 , feature  SEX\n",
      "Iteration  4  of  10 , feature  SPEC_USE\n",
      "Iteration  4  of  10 , feature  SPEEDREL\n",
      "Iteration  4  of  10 , feature  TOWED\n",
      "Iteration  4  of  10 , feature  TOW_VEH\n",
      "Iteration  4  of  10 , feature  TYP_INT\n",
      "Iteration  4  of  10 , feature  VALIGN\n",
      "Iteration  4  of  10 , feature  VEH_AGE\n",
      "Iteration  4  of  10 , feature  VPROFILE\n",
      "Iteration  4  of  10 , feature  VSPD_LIM\n",
      "Iteration  4  of  10 , feature  VSURCOND\n",
      "Iteration  4  of  10 , feature  VTRAFCON\n",
      "Iteration  4  of  10 , feature  VTRAFWAY\n",
      "Iteration  4  of  10 , feature  WEATHER\n",
      "compute_gamma_categorical()\n",
      "len(self.categorical) = 64\n",
      "all_gamma_cat =  [6876.015625, 1721.203125, 1615.375, 1594.296875, 1612.171875]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AIR_BAG</th>\n",
       "      <th>ALC_STATUS</th>\n",
       "      <th>BODY_TYP</th>\n",
       "      <th>CARGO_BT</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>DEFORMED</th>\n",
       "      <th>DR_ZIP</th>\n",
       "      <th>EJECTION</th>\n",
       "      <th>...</th>\n",
       "      <th>VEH_AGE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VPROFILE</th>\n",
       "      <th>VSPD_LIM</th>\n",
       "      <th>VSURCOND</th>\n",
       "      <th>VTRAFCON</th>\n",
       "      <th>VTRAFWAY</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>HOSPITAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ACC_TYPE  AGE  AIR_BAG  ALC_STATUS  BODY_TYP  CARGO_BT  DAY_WEEK  \\\n",
       "0        9.0  9.0      4.0         2.0       2.0       0.0         1   \n",
       "1        8.0  6.0      4.0         2.0       3.0       0.0         1   \n",
       "2        5.0  3.0      1.0         2.0       7.0       0.0         1   \n",
       "3        3.0  4.0      1.0         2.0       2.0       0.0         1   \n",
       "4        3.0  4.0      3.0         2.0       2.0       0.0         1   \n",
       "5        3.0  3.0      3.0         2.0       2.0       0.0         1   \n",
       "6        2.0  6.0      4.0         2.0       2.0       0.0         2   \n",
       "7        0.0  5.0      4.0         2.0       2.0       0.0         4   \n",
       "8        8.0  7.0      4.0         2.0       7.0       0.0         3   \n",
       "9        7.0  6.0      4.0         2.0       2.0       0.0         3   \n",
       "10       7.0  1.0      4.0         2.0       2.0       0.0         3   \n",
       "11       7.0  2.0      4.0         2.0       2.0       0.0         3   \n",
       "12       7.0  7.0      4.0         2.0       2.0       0.0         3   \n",
       "13       8.0  6.0      4.0         2.0       7.0       0.0         4   \n",
       "14       7.0  9.0      4.0         2.0       7.0       0.0         4   \n",
       "15       7.0  7.0      4.0         2.0       4.0       0.0         4   \n",
       "16       7.0  7.0      4.0         2.0       4.0       0.0         4   \n",
       "17       8.0  6.0      4.0         2.0       7.0       0.0         1   \n",
       "18       5.0  9.0      4.0         2.0       7.0       0.0         3   \n",
       "19       3.0  7.0      4.0         2.0       7.0       0.0         3   \n",
       "\n",
       "    DEFORMED  DR_ZIP  EJECTION  ...  VEH_AGE  VE_TOTAL  VPROFILE  VSPD_LIM  \\\n",
       "0        2.0     8.0       0.0  ...      6.0         1       3.0       2.0   \n",
       "1        2.0     8.0       0.0  ...      1.0         1       3.0       2.0   \n",
       "2        0.0     8.0       0.0  ...      9.0         1       2.0       2.0   \n",
       "3        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "4        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "5        0.0     8.0       0.0  ...      5.0         1       2.0       2.0   \n",
       "6        0.0     8.0       1.0  ...      6.0         0       3.0       5.0   \n",
       "7        4.0     8.0       0.0  ...      5.0         1       2.0       3.0   \n",
       "8        3.0     5.0       0.0  ...      9.0         1       0.0       3.0   \n",
       "9        4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "10       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "11       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "12       4.0     5.0       0.0  ...      2.0         1       0.0       3.0   \n",
       "13       0.0     5.0       0.0  ...      1.0         3       3.0       4.0   \n",
       "14       0.0     5.0       0.0  ...      5.0         3       3.0       4.0   \n",
       "15       2.0     5.0       0.0  ...      0.0         3       3.0       4.0   \n",
       "16       4.0     4.0       0.0  ...      4.0         3       3.0       4.0   \n",
       "17       3.0     5.0       0.0  ...      1.0         1       3.0       6.0   \n",
       "18       0.0     4.0       0.0  ...      3.0         1       3.0       3.0   \n",
       "19       0.0     9.0       0.0  ...      1.0         1       3.0       3.0   \n",
       "\n",
       "    VSURCOND  VTRAFCON  VTRAFWAY  WEATHER  WRK_ZONE  HOSPITAL  \n",
       "0        0.0       0.0       0.0      0.0         0         0  \n",
       "1        0.0       0.0       0.0      0.0         0         0  \n",
       "2        0.0       1.0       0.0      0.0         0         0  \n",
       "3        0.0       1.0       0.0      0.0         0         0  \n",
       "4        0.0       1.0       0.0      0.0         0         0  \n",
       "5        0.0       1.0       0.0      0.0         0         0  \n",
       "6        0.0       0.0       1.0      0.0         0         1  \n",
       "7        1.0       1.0       0.0      0.0         0         0  \n",
       "8        0.0       0.0       1.0      0.0         0         0  \n",
       "9        0.0       0.0       1.0      0.0         0         0  \n",
       "10       0.0       0.0       1.0      0.0         0         0  \n",
       "11       0.0       0.0       1.0      0.0         0         0  \n",
       "12       0.0       0.0       1.0      0.0         0         0  \n",
       "13       0.0       0.0       1.0      0.0         0         0  \n",
       "14       0.0       0.0       1.0      0.0         0         1  \n",
       "15       0.0       0.0       1.0      0.0         0         0  \n",
       "16       0.0       0.0       1.0      0.0         0         0  \n",
       "17       0.0       0.0       1.0      0.0         0         0  \n",
       "18       0.0       1.0       0.0      0.0         0         0  \n",
       "19       0.0       1.0       0.0      0.0         0         0  \n",
       "\n",
       "[20 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Impute_MissForest()\n",
      "\n",
      "../../Big_Files/CRSS_03_1_1_0.csv\n",
      "\n",
      "Check()\n",
      "Index(['ACC_TYPE', 'AGE', 'AIR_BAG', 'ALC_STATUS', 'BODY_TYP', 'CARGO_BT',\n",
      "       'DAY_WEEK', 'DEFORMED', 'DR_ZIP', 'EJECTION', 'HARM_EV', 'HIT_RUN',\n",
      "       'HOUR', 'IMPACT1', 'INJ_SEV', 'INT_HWY', 'J_KNIFE', 'LGT_COND', 'MAKE',\n",
      "       'MAK_MOD', 'MAN_COLL', 'MAX_SEV', 'MODEL', 'MONTH', 'M_HARM', 'NUMOCCS',\n",
      "       'NUM_INJ', 'NUM_INJV', 'PCRASH4', 'PCRASH5', 'PERMVIT', 'PER_TYP', 'PJ',\n",
      "       'PSU', 'PVH_INVL', 'P_CRASH1', 'P_CRASH2', 'REGION', 'RELJCT1',\n",
      "       'RELJCT2', 'REL_ROAD', 'REST_MIS', 'REST_USE', 'ROLINLOC', 'ROLLOVER',\n",
      "       'SEAT_POS', 'SEX', 'SPEC_USE', 'SPEEDREL', 'TOWED', 'TOW_VEH',\n",
      "       'TYP_INT', 'URBANICITY', 'VALIGN', 'VEH_AGE', 'VE_TOTAL', 'VPROFILE',\n",
      "       'VSPD_LIM', 'VSURCOND', 'VTRAFCON', 'VTRAFWAY', 'WEATHER', 'WRK_ZONE',\n",
      "       'HOSPITAL'],\n",
      "      dtype='object')\n",
      "[9.0, 8.0, 5.0, 3.0, 2.0, 0.0, 7.0, nan, 6.0, 4.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9.0, 30801, 31358],\n",
       " [8.0, 206437, 230173],\n",
       " [5.0, 94292, 100590],\n",
       " [3.0, 65072, 72917],\n",
       " [2.0, 21069, 21802],\n",
       " [0.0, 27759, 31336],\n",
       " [7.0, 126924, 153341],\n",
       " [nan, 0, 0],\n",
       " [6.0, 82681, 90268],\n",
       " [4.0, 43129, 44378],\n",
       " [1.0, 25715, 26537]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9.0, 6.0, 3.0, 4.0, 5.0, 7.0, 1.0, 2.0, 8.0, nan, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9.0, 36859, 36871],\n",
       " [6.0, 402658, 432524],\n",
       " [3.0, 29902, 29920],\n",
       " [4.0, 20657, 20658],\n",
       " [5.0, 40729, 40735],\n",
       " [7.0, 125355, 125490],\n",
       " [1.0, 29420, 30838],\n",
       " [2.0, 22889, 23347],\n",
       " [8.0, 43629, 43634],\n",
       " [nan, 0, 0],\n",
       " [0.0, 17579, 18683]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 1.0, 3.0, nan, 0.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 595378, 644724],\n",
       " [1.0, 45696, 46710],\n",
       " [3.0, 16772, 16841],\n",
       " [nan, 0, 0],\n",
       " [0.0, 41608, 42481],\n",
       " [2.0, 51326, 51944]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[nan, 2.0, 0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[nan, 0, 0], [2.0, 655465, 787788], [0.0, 14643, 14662], [1.0, 250, 250]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 3.0, 7.0, 4.0, 1.0, 6.0, 9.0, 8.0, 0.0, 5.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 282564, 299292],\n",
       " [3.0, 52838, 52841],\n",
       " [7.0, 26388, 26392],\n",
       " [4.0, 170645, 174859],\n",
       " [1.0, 38348, 38353],\n",
       " [6.0, 124124, 124302],\n",
       " [9.0, 16278, 16278],\n",
       " [8.0, 16775, 16786],\n",
       " [0.0, 21659, 21659],\n",
       " [5.0, 31825, 31938],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, 2.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 762384, 765714], [1.0, 4402, 4404], [2.0, 18241, 32582], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1, 2, 4, 3, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 110914, 110914],\n",
       " [2, 115126, 115126],\n",
       " [4, 136970, 136970],\n",
       " [3, 238408, 238408],\n",
       " [0, 201282, 201282]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 0.0, 4.0, 3.0, nan, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 163477, 202488],\n",
       " [0.0, 266824, 289480],\n",
       " [4.0, 204651, 271029],\n",
       " [3.0, 14457, 14527],\n",
       " [nan, 0, 0],\n",
       " [1.0, 23338, 25176]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8.0, 5.0, 4.0, 9.0, 7.0, nan, 2.0, 6.0, 1.0, 0.0, 3.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[8.0, 84289, 88561],\n",
       " [5.0, 211789, 235383],\n",
       " [4.0, 90819, 92020],\n",
       " [9.0, 32233, 33273],\n",
       " [7.0, 102896, 113912],\n",
       " [nan, 0, 0],\n",
       " [2.0, 44370, 44858],\n",
       " [6.0, 109844, 112399],\n",
       " [1.0, 19411, 19586],\n",
       " [0.0, 30566, 33102],\n",
       " [3.0, 29526, 29606]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, nan, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 743562, 777468], [1.0, 2758, 2774], [nan, 0, 0], [2.0, 22427, 22458]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 2.0, 0.0, 4.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 702923, 702994],\n",
       " [2.0, 24911, 24948],\n",
       " [0.0, 27548, 27561],\n",
       " [4.0, 19208, 19209],\n",
       " [1.0, 27849, 27988],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 777792, 777810], [1.0, 24890, 24890], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 6.0, 2.0, 5.0, 0.0, 3.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 261736, 262448],\n",
       " [6.0, 114099, 114601],\n",
       " [2.0, 28830, 28834],\n",
       " [5.0, 269181, 269599],\n",
       " [0.0, 25094, 25096],\n",
       " [3.0, 74425, 74427],\n",
       " [1.0, 27590, 27695],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 0.0, 1.0, 2.0, 7.0, 5.0, 6.0, 3.0, 8.0, 9.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 38883, 39221],\n",
       " [0.0, 46527, 46954],\n",
       " [1.0, 357264, 364912],\n",
       " [2.0, 44192, 44511],\n",
       " [7.0, 189288, 189469],\n",
       " [5.0, 27331, 27394],\n",
       " [6.0, 26654, 26727],\n",
       " [3.0, 29867, 29936],\n",
       " [8.0, 16441, 16482],\n",
       " [9.0, 17068, 17094],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 2.0, 0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 565203, 575072],\n",
       " [2.0, 120354, 120584],\n",
       " [0.0, 39430, 39474],\n",
       " [1.0, 67474, 67570],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 715981, 716044], [1.0, 86649, 86656], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 781759, 781759], [2, 20511, 20511], [1, 430, 430]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 1.0, 2.0, nan, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 576924, 578517],\n",
       " [1.0, 134512, 134756],\n",
       " [2.0, 23647, 23647],\n",
       " [nan, 0, 0],\n",
       " [0.0, 65396, 65780]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 6.0, 7.0, 1.0, 5.0, nan, 4.0, 9.0, 0.0, 8.0, 3.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 112792, 113400],\n",
       " [6.0, 277003, 281097],\n",
       " [7.0, 76241, 76256],\n",
       " [1.0, 22608, 22955],\n",
       " [5.0, 117352, 117694],\n",
       " [nan, 0, 0],\n",
       " [4.0, 47870, 47880],\n",
       " [9.0, 18407, 21088],\n",
       " [0.0, 16078, 19282],\n",
       " [8.0, 37062, 37297],\n",
       " [3.0, 65261, 65751]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 6.0, 3.0, 1.0, 8.0, 0.0, 2.0, 9.0, 5.0, 7.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 165878, 166211],\n",
       " [6.0, 136372, 136488],\n",
       " [3.0, 204176, 205406],\n",
       " [1.0, 41832, 41832],\n",
       " [8.0, 22508, 22559],\n",
       " [0.0, 21988, 21988],\n",
       " [2.0, 67768, 67770],\n",
       " [9.0, 35429, 35429],\n",
       " [5.0, 78366, 78369],\n",
       " [7.0, 26648, 26648],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 0.0, 1.0, 3.0, 2.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 102961, 103441],\n",
       " [0.0, 34060, 34069],\n",
       " [1.0, 114844, 114904],\n",
       " [3.0, 315553, 316366],\n",
       " [2.0, 232793, 233920],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 2.0, 0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 395534, 397467],\n",
       " [2.0, 193231, 193231],\n",
       " [0.0, 96097, 96098],\n",
       " [1.0, 115904, 115904],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 6.0, 7.0, 5.0, 2.0, 1.0, 4.0, 8.0, 9.0, 0.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 194410, 205086],\n",
       " [6.0, 57842, 57897],\n",
       " [7.0, 94828, 95052],\n",
       " [5.0, 106717, 106961],\n",
       " [2.0, 139600, 143316],\n",
       " [1.0, 20631, 20886],\n",
       " [4.0, 83154, 86262],\n",
       " [8.0, 34055, 34131],\n",
       " [9.0, 31444, 31445],\n",
       " [0.0, 21664, 21664],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 177800, 177800],\n",
       " [1, 123494, 123494],\n",
       " [2, 134577, 134577],\n",
       " [3, 145610, 145610],\n",
       " [4, 150456, 150456],\n",
       " [5, 70763, 70763]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 0.0, 4.0, 1.0, 3.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 689764, 689854],\n",
       " [0.0, 35444, 35454],\n",
       " [4.0, 20106, 20108],\n",
       " [1.0, 42225, 42401],\n",
       " [3.0, 14872, 14883],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 2.0, 3.0, 1.0, 4.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 428660, 443089],\n",
       " [2.0, 81781, 82011],\n",
       " [3.0, 46859, 46972],\n",
       " [1.0, 195875, 197392],\n",
       " [4.0, 33169, 33236],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 4.0, 1.0, 2.0, 3.0, 5.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 395550, 397483],\n",
       " [4.0, 19303, 19303],\n",
       " [1.0, 224915, 224916],\n",
       " [2.0, 99933, 99933],\n",
       " [3.0, 42718, 42718],\n",
       " [5.0, 18347, 18347],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, 3.0, 2.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 516528, 524946],\n",
       " [1.0, 194943, 195079],\n",
       " [3.0, 28089, 28089],\n",
       " [2.0, 54584, 54586],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 0.0, nan, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 743219, 767803], [0.0, 12689, 16185], [nan, 0, 0], [1.0, 16207, 18712]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 1.0, 0.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 79101, 79232], [1.0, 633795, 635716], [0.0, 87552, 87752], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5, 3, 0, 4, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5, 268478, 268478],\n",
       " [3, 313905, 313905],\n",
       " [0, 66121, 66121],\n",
       " [4, 115648, 115648],\n",
       " [1, 16509, 16509],\n",
       " [2, 22039, 22039]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.0, 0.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 587731, 587858], [0.0, 214839, 214842], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9, 5, 0, 1, 4, 2, 6, 3, 7, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9, 32598, 32598],\n",
       " [5, 140700, 140700],\n",
       " [0, 32531, 32531],\n",
       " [1, 30131, 30131],\n",
       " [4, 116361, 116361],\n",
       " [2, 34286, 34286],\n",
       " [6, 110607, 110607],\n",
       " [3, 112608, 112608],\n",
       " [7, 106310, 106310],\n",
       " [8, 86568, 86568]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9, 2, 3, 4, 6, 5, 8, 0, 1, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9, 20484, 20484],\n",
       " [2, 69248, 69248],\n",
       " [3, 197221, 197221],\n",
       " [4, 139149, 139149],\n",
       " [6, 71467, 71467],\n",
       " [5, 97129, 97129],\n",
       " [8, 70128, 70128],\n",
       " [0, 29027, 29027],\n",
       " [1, 56499, 56499],\n",
       " [7, 52348, 52348]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 785238, 785238], [1, 14278, 14278], [2, 3184, 3184]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.0, 1.0, 2.0, 4.0, nan, 0.0, 3.0, 6.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5.0, 40511, 40796],\n",
       " [1.0, 413264, 418832],\n",
       " [2.0, 80341, 81047],\n",
       " [4.0, 129042, 130128],\n",
       " [nan, 0, 0],\n",
       " [0.0, 45520, 45878],\n",
       " [3.0, 49470, 49575],\n",
       " [6.0, 36315, 36444]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8.0, 6.0, 3.0, 0.0, 9.0, 7.0, 5.0, 2.0, 4.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[8.0, 293670, 302652],\n",
       " [6.0, 106561, 107350],\n",
       " [3.0, 48798, 49388],\n",
       " [0.0, 21502, 21839],\n",
       " [9.0, 103939, 104772],\n",
       " [7.0, 47690, 48651],\n",
       " [5.0, 71247, 71627],\n",
       " [2.0, 23047, 23895],\n",
       " [4.0, 41268, 41295],\n",
       " [1.0, 31043, 31231],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3, 2, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 120799, 120799],\n",
       " [2, 142507, 142507],\n",
       " [1, 451311, 451311],\n",
       " [0, 88083, 88083]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 615492, 760794], [1.0, 38672, 41906], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.0, 0.0, 3.0, 2.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 280013, 318298],\n",
       " [0.0, 226950, 226973],\n",
       " [3.0, 185240, 185265],\n",
       " [2.0, 70237, 72164],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 0.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 723058, 723131], [0.0, 60746, 60787], [1.0, 18768, 18782], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 734023, 734023], [1, 6870, 6870], [0, 61807, 61807]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.0, 0.0, 3.0, nan, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 659701, 679584],\n",
       " [0.0, 45223, 78971],\n",
       " [3.0, 23782, 23940],\n",
       " [nan, 0, 0],\n",
       " [2.0, 20204, 20205]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 1.0, 0.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 774586, 774586], [1.0, 6596, 6750], [0.0, 21109, 21364], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 774586, 774586], [0, 13800, 13800], [1, 14314, 14314]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 0.0, 3.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 587598, 587728],\n",
       " [0.0, 117231, 125851],\n",
       " [3.0, 53461, 56330],\n",
       " [1.0, 32586, 32791],\n",
       " [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.0, 0.0, nan, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 428354, 439915], [0.0, 356326, 362783], [nan, 0, 0], [2.0, 2, 2]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.0, 2.0, 0.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0, 788331, 795433], [2.0, 3391, 3411], [0.0, 3856, 3856], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 1.0, nan, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 742749, 753337], [1.0, 36619, 36688], [nan, 0, 0], [0.0, 12675, 12675]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0, 0.0, 3.0, nan, 1.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, 430720, 466808],\n",
       " [0.0, 223212, 226165],\n",
       " [3.0, 34150, 34643],\n",
       " [nan, 0, 0],\n",
       " [1.0, 25348, 25348],\n",
       " [2.0, 49360, 49736]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 2.0, 1.0, nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 781136, 781332], [2.0, 20652, 20652], [1.0, 716, 716], [nan, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, nan, 2.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 408835, 415382],\n",
       " [nan, 0, 0],\n",
       " [2.0, 230256, 282458],\n",
       " [1.0, 92939, 104860]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 181758, 181758], [1, 620942, 620942]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, nan, 1.0, 0.0, 3.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 678108, 719317],\n",
       " [nan, 0, 0],\n",
       " [1.0, 38391, 38416],\n",
       " [0.0, 25978, 25979],\n",
       " [3.0, 18988, 18988]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6.0, 1.0, 9.0, 5.0, 2.0, 0.0, 4.0, 3.0, 7.0, nan, 8.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[6.0, 58041, 58199],\n",
       " [1.0, 251976, 259703],\n",
       " [9.0, 58931, 59521],\n",
       " [5.0, 64698, 64838],\n",
       " [2.0, 82224, 82366],\n",
       " [0.0, 46126, 46141],\n",
       " [4.0, 66100, 66198],\n",
       " [3.0, 103794, 104578],\n",
       " [7.0, 23841, 23844],\n",
       " [nan, 0, 0],\n",
       " [8.0, 37275, 37312]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1, 0, 3, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 585044, 585044], [0, 93192, 93192], [3, 31598, 31598], [2, 92866, 92866]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.0, 2.0, 0.0, nan, 4.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 578378, 684174],\n",
       " [2.0, 57261, 57412],\n",
       " [0.0, 19585, 19585],\n",
       " [nan, 0, 0],\n",
       " [4.0, 18988, 18988],\n",
       " [1.0, 22541, 22541]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.0, 5.0, 3.0, 4.0, 6.0, 1.0, nan, 7.0, 0.0, 8.0, 9.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2.0, 45401, 45870],\n",
       " [5.0, 25534, 25957],\n",
       " [3.0, 210720, 281436],\n",
       " [4.0, 155749, 164094],\n",
       " [6.0, 79711, 84955],\n",
       " [1.0, 68774, 80307],\n",
       " [nan, 0, 0],\n",
       " [7.0, 14136, 14522],\n",
       " [0.0, 19237, 19237],\n",
       " [8.0, 41069, 41925],\n",
       " [9.0, 41224, 44397]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1.0, nan, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 633413, 663569],\n",
       " [1.0, 110557, 112100],\n",
       " [nan, 0, 0],\n",
       " [2.0, 26988, 27031]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[nan, 0.0, 1.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[nan, 0, 0],\n",
       " [0.0, 463988, 520559],\n",
       " [1.0, 192172, 202791],\n",
       " [2.0, 77793, 79350]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[nan, 1.0, 0.0, 2.0, 3.0, 4.0, 5.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[nan, 0, 0],\n",
       " [1.0, 156860, 169766],\n",
       " [0.0, 303273, 407390],\n",
       " [2.0, 152529, 159592],\n",
       " [3.0, 18348, 18855],\n",
       " [4.0, 28072, 28109],\n",
       " [5.0, 18988, 18988]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, nan, 1.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 576937, 601935],\n",
       " [nan, 0, 0],\n",
       " [1.0, 70198, 71328],\n",
       " [2.0, 128840, 129437]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 788008, 788008], [1, 7959, 7959], [2, 6733, 6733]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 676444, 676444], [1, 126256, 126256]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Check()\n",
      "\n",
      "Finished Impute_Using_MissForest()\n",
      "\n",
      "CPU times: user 1h 43min 19s, sys: 2min 44s, total: 1h 46min 4s\n",
      "Wall time: 1h 48min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def Impute_Using_MissForest():\n",
    "    print ('Impute_Using_MissForest()')\n",
    "    data = Get_Data()\n",
    "    data = data.replace({99:np.nan})\n",
    "    \n",
    "#    data_Imputed = Impute_Full(data)\n",
    "    data_Imputed = Impute_MissForest(data)\n",
    "#    data_Imputed.to_csv('../../Big_Files/CRSS_03_Imputed_by_MF_Data.csv', index=False)\n",
    "\n",
    "\n",
    "    # The first '_0' or '_1' is for the random seed for Python and Numpy\n",
    "    # The second '_0' (or '_1') is for the dimensionality not being reduced (being reduced) before binning\n",
    "    # The third '_0' is for imputation with MissForest.  '_1' will be for IVEware with R random seed 0.\n",
    "    filename = '../../Big_Files/CRSS_03' + filename_prefix + '_0.csv'\n",
    "    print (filename)\n",
    "    print ()\n",
    "    data_Imputed.to_csv(filename, index=False) \n",
    "\n",
    "#    display(data_Imputed.head(50))\n",
    "    \n",
    "    Check(data, data_Imputed)\n",
    "#    display(Audio(sound_file, autoplay=True))\n",
    "\n",
    "    print ('Finished Impute_Using_MissForest()')\n",
    "    print ()\n",
    "\n",
    "    return 0\n",
    "\n",
    "Impute_Using_MissForest()\n",
    "\n",
    "# Takes 2-3 GB of memory\n",
    "# CPU times: user 1h 17min 49s, sys: 1min 25s, total: 1h 19min 14s\n",
    "# Wall time: 1h 19min 35s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
