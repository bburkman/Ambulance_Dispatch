{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38eb74b",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "- This file \n",
    "\n",
    "    - Pulls the data from the '../../Big_Files' folder (not in the GitHub repository) for each year,\n",
    "    - From each year pulls the Accident, Vehicle, and Person .csv files,\n",
    "    - Drops features in the Vehicle and Person files that are repeats of features in the other files,\n",
    "    - Drops features that are not useful, even for imputing missing data, because they are just noise or were not collected in some years,\n",
    "    - Merges the Accident, Vehicle, and Person files so that each sample represents a crash person,\n",
    "    - Drops crashes involving a pedestrian, \n",
    "    - Drops features with NaN values:  Three of the \"_IM\" imputed features that were only imputed in some years,\n",
    "    - Engineers a \"VEH_AGE\" feature from \"VEH_YEAR\" and \"YEAR,\" because a 2016 car in the 2016 data is different from a 2016 car in the 2022 data, but a new car is a new car,\n",
    "    - Drops features where more than 20% of the samples are missing,\n",
    "    - Drops features where one value is more than 99% of the samples,\n",
    "    - Drops samples where the target variable, HOSPITAL, is unknown, \n",
    "    - Bins HOSPITAL into 0 (did not go to hospital) and 1 (went to hospital by any means).  We will use this binary HOSPITAL to order the values unordered features by correlation to hospitalization, then bin them by gaps in correlation,\n",
    "    - Writes the results to one big file, '../../Big_Files/CRSS_Merged_Raw_Data.csv'.\n",
    "    - Also writes a sample to '../../Big_Files/CRSS_Merged_Raw_Data_Sample.csv' for testing code.\n",
    "\n",
    "- This file also creates two dictionaries and writes them in JSON (JavaScript Object Notation) format to file.\n",
    "\n",
    "    - '../../Big_Files/Imputed_Features_Dict.json' gives the Raw:Imputed pairs for features imputed by CRSS,\n",
    "    - '../../Big_Files/Missing_Unknown_Dict.json' gives the values for each feature that signify \"Missing\" or \"Unknown.\"  It's complicated because some values signify unknown subcategory within a category, not entirely unknown.  \n",
    "    \n",
    "- We started with 220 features and end with 69 raw features and 19 imputed features.\n",
    "\n",
    "- We started with 910,183 crash persons, \n",
    "    - then dropped to 833,798 crash persons when we eliminated crashes with a pedestrian, \n",
    "    - then down to 817,623 crash persons when we deleted crash persons with unknown hospitalization.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054af39c",
   "metadata": {},
   "source": [
    "# readme\n",
    "## Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ac92",
   "metadata": {},
   "source": [
    "We designed this code to fit in a GitHub repository with files under 100MB.  Many of the files we input are over this limit, and after preprocessing (selecting features, binning features) we saved the data as a .csv file of about 150 MB so we can tweak later code without having to run the preprocessing again.  We saved it again after imputing missing data.  To keep the files in our GitHub repository under 100 MB, we saved these into a different directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884df33",
   "metadata": {},
   "source": [
    "- CRSS Data Files\n",
    "    - We use seven years of data, 2016-2022. Once later years come out, they can be easily added. \n",
    "    - Each year's data is 100-200 MB\n",
    "    - The files we're really interested in each year are these.  The names were uppercase until 2018, then lowercase, and also after 2018 the file sizes jumped.\n",
    "        - accident.csv or ACCIDENT.csv, now about 30 MB\n",
    "        - vehicle.csv or VEHICLE.csv, now about 180 MB\n",
    "        - person.csv or PERSON.csv, now about 150 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698bb8d",
   "metadata": {},
   "source": [
    "- Big_Files\n",
    "    - CRSS_Files\n",
    "        - CRSS2016CSV (22 files, 160 MB)\n",
    "        - CRSS2017CSV (22 files, 189 MB)\n",
    "        - CRSS2018CSV (22 files, 169 MB)\n",
    "        - CRSS2019CSV (23 files, 633 MB)\n",
    "        - CRSS2020CSV (29 files, 719 MB)\n",
    "        - CRSS2021CSV (29 files, 736 MB)\n",
    "        - CRSS2022CSV (29 files, 738 MB)\n",
    "        \n",
    "        \n",
    "    - *Intermediate .csv files*\n",
    "- GitHub_Repository\n",
    "    - Code_Files\n",
    "        - Analyze_Proba\n",
    "        - Confusion_Matrices\n",
    "        - Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd8689",
   "metadata": {},
   "source": [
    "## Note on DR_ZIP Field\n",
    "\n",
    "- The field DR_ZIP in the Vehicle file signifies the driver's zip code.\n",
    "- The Zone Improvement Program assigns a five-digit code to each postal zone in the US.  We use it as a proxy for small geographic regions.  \n",
    "- The ZIP code is five digits, but can start with zeros, so it is not exactly a five-digit number.\n",
    "- In DR_ZIP, some of the entries are three or four digits, like 1846, which appears 602 times in CRSS 2016-2022, and 802, which appears seven times.  \n",
    "- None of the five-digit DR_ZIP entries begin with a zero, so we suspect that CRSS has dropped leading zeros.\n",
    "- ZIP code 01846 is around Reading, MA, and ZIP code 00802 is in the US Virgin Islands, so if we padded leading zeroes we would get legitimate ZIP codes (at least in these two examples).\n",
    "- CRSS did not cut off the leading zeros in fields MCARR_I2 or MCARR_ID (Motor Carrier Identification Number), where 00000000000 signifies \"Not Applicable.\"  When one opens the .csv in Excel it appears as \"0\", but when one opens it in a terminal window all of the digits appear.  I don't see any technical reason why CRSS had to cut off leading zeroes in the ZIP codes.\n",
    "- We could pad zeroes onto the DR_ZIP entries and save them as strings, but that may be unnecessary.  Each three- and four-digit codes still signify a unique place just as well as it would as a five-character string.  \n",
    "- We will leave the DR_ZIP as they are.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0562cc2",
   "metadata": {},
   "source": [
    "# Notes on NaN\n",
    "- Three of the features imputed by CRSS have NaN values for entire years, because they were not imputed in those years.\n",
    "- RELJCT1 was not imputed in 2019, so RELJCT1_IM has NaN in that year.\n",
    "- HIT_RUN was not imputed in 2020, 2021, or 2022, so HITRUN_IM has NaN in those years.\n",
    "- BODY_TYP was not imputed in 2021 or 2022, so BDYTYP_IM has NaN in those years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbbc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy version: 1.26.4\n",
      "Pandas version:  2.2.2\n",
      "JSON version:  2.0.9\n",
      "Finished Installing Packages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import json # We will use json ('JavaScript Object Notation') to write and read dictionaries to/from files\n",
    "print ('JSON version:  {}'.format(json.__version__))\n",
    "\n",
    "# THERE IS NO RANDOMNESS IN THIS NOTEBOOK\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "#import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d0f6e",
   "metadata": {},
   "source": [
    "# Get Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b290fc",
   "metadata": {},
   "source": [
    "## Read CRSS Files\n",
    "- We have the CRSS dataset in \n",
    "    - Big_Files/CRSS_2020_Update/\n",
    "- In one directory for each year,\n",
    "    - CRSS2016CSV\n",
    "    - CRSS2017CSV\n",
    "    - CRSS2018CSV\n",
    "    - CRSS2019CSV\n",
    "    - CRSS2020CSV    \n",
    "    - CRSS2021CSV    \n",
    "    - CRSS2022CSV    \n",
    "- In each year, the CRSS dataset comes in three main files, \n",
    "    - Accident.csv\n",
    "    - Vehicle.csv\n",
    "    - Person.csv\n",
    "- Collect those and merge into three files,\n",
    "    - Accident_Raw.csv\n",
    "    - Vehicle_Raw.csv\n",
    "    - Person_Raw.csv\n",
    "- and also three files with category names,\n",
    "    - Accident_Raw_with_Names.csv\n",
    "    - Vehicle_Raw_with_Names.csv\n",
    "    - Person_Raw_with_Names.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13504c",
   "metadata": {},
   "source": [
    "### accident.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cc11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Accident(NAMES):\n",
    "    print ('Import_Data_Accident()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "#    for year in ['2018']:\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/ACCIDENT.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "#    for year in ['2020']:\n",
    "    for year in ['2019','2020','2021', '2022']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/accident.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "    \n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bca27",
   "metadata": {},
   "source": [
    "### vehicle.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250e2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Vehicle(NAMES):\n",
    "    print ('Import_Data_Vehicle()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/VEHICLE.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None, low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021', '2022']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/vehicle.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1', low_memory=False)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7b6cf",
   "metadata": {},
   "source": [
    "### person.csv from CRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62699818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Data_Person(NAMES):\n",
    "    print ('Import_Data_Person()')\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "    for year in ['2016','2017','2018']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/PERSON.CSV'\n",
    "        temp = pd.read_csv(filename, index_col=None)\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    for year in ['2019','2020','2021', '2022']:\n",
    "        filename = '../../Big_Files/CRSS_2020_Update/CRSS' + year + 'CSV/person.csv'\n",
    "        temp = pd.read_csv(filename, index_col=None, encoding='latin1')\n",
    "        print (year, len(temp))\n",
    "        df = df.append(temp)\n",
    "\n",
    "    if NAMES==0:\n",
    "        for feature in df:\n",
    "            if 'NAME' in feature:\n",
    "                df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print (df.shape)\n",
    "    print ()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d05f7",
   "metadata": {},
   "source": [
    "### Get Data\n",
    "- The Get_Data_from_Original() reads the (original) CRSS files from the CRSS directory, preprocesses it, and writes it to files in a folder outside this GitHub repo (because the files are too large for my subscription), and returns the dataframes.\n",
    "- The Get_Data_from_Temp_Files() reads the temp files and returns the dataframes.  I created this option for running repeatedly during writing and debugging, because it's much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93a66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Original():\n",
    "    print ('Get_Data_from_Original()')\n",
    "    \n",
    "    df_Accident = Import_Data_Accident(0)\n",
    "    df_Vehicle = Import_Data_Vehicle(0)\n",
    "    df_Person = Import_Data_Person(0)\n",
    "    \n",
    "    df_Accident.to_csv('../../Big_Files/Accident_Raw.csv', index=False)\n",
    "    df_Vehicle.to_csv('../../Big_Files/Vehicle_Raw.csv', index=False)\n",
    "    df_Person.to_csv('../../Big_Files/Person_Raw.csv', index=False)\n",
    "    \n",
    "\n",
    "    df_Accident_NAMES = Import_Data_Accident(1)\n",
    "    df_Vehicle_NAMES = Import_Data_Vehicle(1)\n",
    "    df_Person_NAMES = Import_Data_Person(1)\n",
    "    \n",
    "    df_Accident_NAMES.to_csv('../../Big_Files/Accident_Raw_with_NAMES.csv', index=False)\n",
    "    df_Vehicle_NAMES.to_csv('../../Big_Files/Vehicle_Raw_with_NAMES.csv', index=False)\n",
    "    df_Person_NAMES.to_csv('../../Big_Files/Person_Raw_with_NAMES.csv', index=False)\n",
    "    \n",
    "\n",
    "    return df_Accident, df_Vehicle, df_Person\n",
    "\n",
    "#df_Accident, df_Vehicle, df_Person = Get_Data_from_Original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a5add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data_from_Temp_Files():\n",
    "    print ('Get_Data_from_Temp_Files()')\n",
    "    df_Acc = pd.read_csv('../../Big_Files/Accident_Raw.csv', low_memory=False)\n",
    "    df_Veh = pd.read_csv('../../Big_Files/Vehicle_Raw.csv', low_memory=False)\n",
    "    df_Per = pd.read_csv('../../Big_Files/Person_Raw.csv', low_memory=False)\n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "\n",
    "#df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabf849",
   "metadata": {},
   "source": [
    "## Drop Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2e435",
   "metadata": {},
   "source": [
    "- We now have three dataframes from the Accident, Vehicle, and Person files.  \n",
    "- Some features are repeated, so we will drop the ones in Vehicle or Person that appear in Accident, and drop those in Person that appear in Vehicle. \n",
    "- There are two repeated features we need to keep for merging the three data:\n",
    "    - CASENUM tells us to which accident the vehicle and person correspond\n",
    "    - VEH_NO tells us which vehicle the person was in.\n",
    "- Some features have no predictive power and/or resemble random numbers, like the VIN (Vehicle Identification Number) and the minute of the accident time.  \n",
    "- For details on the features, see the *Crash Report Sampling System Analytical User's Manual 2016-2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cc781",
   "metadata": {},
   "source": [
    "### Drop Repeated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4717cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Repeated_Features(df_Acc, df_Veh, df_Per):\n",
    "    print ('Drop_Repeated_Features()')\n",
    "    Acc_Cols = df_Acc.columns.tolist()\n",
    "    Veh_Cols = df_Veh.columns.tolist()\n",
    "    Per_Cols = df_Per.columns.tolist()\n",
    "    \n",
    "    Drop_Veh = [x for x in Veh_Cols if x in Acc_Cols]\n",
    "    Drop_Per = [x for x in Per_Cols if (x in Acc_Cols or x in Veh_Cols)]\n",
    "        \n",
    "#    print ('Drop_Veh:')\n",
    "#    for item in Drop_Veh:\n",
    "#        print (item)\n",
    "#    print ()\n",
    "\n",
    "#    print ('Drop_Per:')\n",
    "#    for item in sorted(Drop_Per):\n",
    "#        print (item)\n",
    "#    print ()\n",
    "    \n",
    "    # We need to keep these for merging the dataframes.\n",
    "    Drop_Veh.remove('CASENUM')\n",
    "    Drop_Per.remove('CASENUM')\n",
    "    Drop_Per.remove('VEH_NO')\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Veh, inplace=True)\n",
    "    df_Per.drop(columns=Drop_Per, inplace=True)\n",
    "\n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819611e1",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Features\n",
    "\n",
    "We will later drop features that are unknowable from the notification, like drug test results, but we still need those features for imputing missing data.  Here we drop features that are either \n",
    "\n",
    "    - Just noise (like VIN, Vehicle Identification Number), \n",
    "    - Only given for a small number of crashes (like trailer weight), or \n",
    "    - Only present for some years but not others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d43160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Irrelevant_Features(df_Acc, df_Veh, df_Per):\n",
    "    print ('Drop_Irrelevant_Features()')\n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    Drop_Accident = [\n",
    "        'CF1',\n",
    "        'CF2',\n",
    "        'CF3',\n",
    "        'MINUTE',\n",
    "        'MINUTE_IM',\n",
    "        'PSU_VAR',\n",
    "        'PSUSTRAT',\n",
    "        'STRATUM',\n",
    "        'WEATHER1',\n",
    "        'WEATHER2',\n",
    "        'WEIGHT',\n",
    "    ]\n",
    "    \n",
    "    df_Acc.drop(columns=Drop_Accident, inplace=True)\n",
    "    \n",
    "    print ('Drop Names of Dropped Features in df_Acc')\n",
    "    for feature in Drop_Accident:\n",
    "        feature_name = feature + 'NAME'\n",
    "        if feature_name in df_Acc:\n",
    "#            print (feature_name)\n",
    "            df_Acc.drop(columns=[feature_name], inplace=True)\n",
    "    \n",
    "    # List of features in df_Veh that aren't repeats from df_Acc \n",
    "    # that we don't want to use, even for imputation, because\n",
    "    # they're only for some years or are like random numbers\n",
    "    Drop_Vehicle = [\n",
    "        'DR_SF1',\n",
    "        'DR_SF2',\n",
    "        'DR_SF3',\n",
    "        'DR_SF4',\n",
    "#        'DR_ZIP',\n",
    "        'GVWR',\n",
    "        'GVWR_FROM',\n",
    "        'GVWR_TO',\n",
    "        'HAZ_ID',\n",
    "        'ICFINALBODY',\n",
    "        'MCARR_I1',\n",
    "        'MCARR_I2',\n",
    "        'MCARR_ID',\n",
    "        'TRLR1GVWR',\n",
    "        'TRLR1VIN',\n",
    "        'TRLR2GVWR',\n",
    "        'TRLR2VIN',\n",
    "        'TRLR3GVWR',\n",
    "        'TRLR3VIN',\n",
    "        'UNDEROVERRIDE',\n",
    "        'UNITTYPE',\n",
    "        'V_CONFIG',\n",
    "        'V_Config',\n",
    "        'VEH_SC1',\n",
    "        'VEH_SC2',\n",
    "        'VIN',\n",
    "        'VPICBODYCLASS',\n",
    "        'VPICMAKE',\n",
    "        'VPICMODEL',\n",
    "    ]\n",
    "    \n",
    "    df_Veh.drop(columns=Drop_Vehicle, inplace=True)\n",
    "    \n",
    "    print ('Drop Names of Dropped Features in df_Veh')\n",
    "    for feature in Drop_Vehicle:\n",
    "        feature_name = feature + 'NAME'\n",
    "        if feature_name in df_Veh:\n",
    "#            print (feature_name)\n",
    "            df_Veh.drop(columns=[feature_name], inplace=True)\n",
    "    \n",
    "    Drop_Person = [\n",
    "        'ATST_TYP',\n",
    "        'DEVTYPE',\n",
    "        'DEVMOTOR',\n",
    "        'DRUGRES1',\n",
    "        'DRUGRES2',\n",
    "        'DRUGRES3',\n",
    "        'DRUGTST1',\n",
    "        'DRUGTST2',\n",
    "        'DRUGTST3',\n",
    "        'DSTATUS',\n",
    "        'HELM_MIS',\n",
    "        'HELM_USE',\n",
    "        'P_SF1',\n",
    "        'P_SF2',\n",
    "        'P_SF3',\n",
    "        'STR_VEH',\n",
    "    ]\n",
    "    \n",
    "    df_Per.drop(columns=Drop_Person, inplace=True)\n",
    "    \n",
    "    print ('Drop Names of Dropped Features in df_Per')\n",
    "    for feature in Drop_Person:\n",
    "        feature_name = feature + 'NAME'\n",
    "        if feature_name in df_Per:\n",
    "#            print (feature_name)\n",
    "            df_Per.drop(columns=[feature_name], inplace=True)\n",
    "    \n",
    "    \n",
    "    print ('df_Acc.shape = ', df_Acc.shape)\n",
    "    print ('df_Veh.shape = ', df_Veh.shape)\n",
    "    print ('df_Per.shape = ', df_Per.shape)\n",
    "    print ()\n",
    "    \n",
    "    return df_Acc, df_Veh, df_Per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8142873",
   "metadata": {},
   "source": [
    "## Merge Accident, Vehicle, and Person Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05306685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(df_Acc, df_Veh, df_Per):\n",
    "    print ('Merge()')\n",
    "    print ()\n",
    "\n",
    "    data = pd.merge(\n",
    "        df_Acc, df_Veh, \n",
    "        on=['CASENUM'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "    # In this step we go from 910,183 Person samples to 873,784 merged samples \n",
    "    #     because 36,399 of the Person samples don't have a corresponding Vehicle because they're pedestrians.\n",
    "    # In a later feature we'll also delete the merged samples that describe the people in the vehicles \n",
    "    #     involved in pedestrian crashes.\n",
    "    data = pd.merge(\n",
    "        data, df_Per, \n",
    "        on=['CASENUM', 'VEH_NO'],\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    data.drop(columns=['CASENUM', 'VEH_NO', 'PER_NO'], inplace=True)\n",
    "    \n",
    "    print ('df_Acc.shape')\n",
    "    print (df_Acc.shape)\n",
    "    print ('df_Veh.shape')\n",
    "    print (df_Veh.shape)\n",
    "    print ('df_Per.shape')\n",
    "    print (df_Per.shape)\n",
    "    print ('data.shape')\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "\n",
    "\n",
    "#    print (data.head())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eff2d1",
   "metadata": {},
   "source": [
    "## Drop Pedestrian Crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a5c26",
   "metadata": {},
   "source": [
    "A vehicle hitting another vehicle, a tree, or something else large can result in sudden deceleration different enough from hard braking to trigger an automated notification, but an impact with a pedestrian or bicycle is not.  Our work needs to focus on crashes likely to trigger an automated notification, so we will drop pedestrian crashes from our dataset.  \n",
    "\n",
    "One could argue that we should keep pedestrian crashes for information relevant for imputing missing data, but a sample with a pedestrian lacks all vehicle information, so those records would be more harm than help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9950da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Pedestrian_Crashes(data):\n",
    "    print ('Remove_Pedestrian_Crashes()')\n",
    "    display(data.PEDS.value_counts())\n",
    "    n = len(data[data.PEDS>0])\n",
    "    print ('Removing %d crashes that involve a pedestrian.' % n)\n",
    "    data = data[data.PEDS==0]\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    # Drop the PEDS column, which now only has value \"0\".\n",
    "    # Drop the LOCATION feature, which now only has value \"0\".\n",
    "    data.drop(columns=['PEDS','LOCATION'], inplace=True)\n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e01e4",
   "metadata": {},
   "source": [
    "## Remove Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d29277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Feature_Names(data):\n",
    "    print ('Remove_Feature_Names()')\n",
    "    for feature in Feature:\n",
    "        if feature[-4:] == 'NAME':\n",
    "            data.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316646c",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- Merge YEAR and MOD_YEAR into VEH_AGE\n",
    "- Why?  Because MOD_YEAR is mostly useful for telling us the age of the car, but we have seven years of data, so a car that was new in 2016 won't be new in 2022.  Much more useful to have a feature that tells the age of the vehicle in the year of the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bdb7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering(data, Missing_Unknown_Dict):\n",
    "    print ('Feature_Engineering()')\n",
    "    print (data.shape)\n",
    "    \n",
    "    data['VEH_AGE'] = data['YEAR'] - data['MOD_YEAR']\n",
    "    data['VEH_AGE'].where(\n",
    "        ~data['MOD_YEAR'].isin(Missing_Unknown_Dict['MOD_YEAR']),\n",
    "        99, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    data.drop(columns=['YEAR','MOD_YEAR', 'MDLYR_IM'], inplace=True)\n",
    "    print (data.shape)\n",
    "    print ()\n",
    "    \n",
    "    return data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40874ebd",
   "metadata": {},
   "source": [
    "## Make Imputed Feature Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff3ba3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Imputed_Feature_Dict(data):\n",
    "    print ('Make_Imputed_Feature_Dict()')\n",
    "\n",
    "    Imputed_Feature_Dict = {\n",
    "        'AGE':'AGE_IM',\n",
    "        'ALCOHOL':'ALCHL_IM',\n",
    "        'BODY_TYP':'BDYTYP_IM',\n",
    "        'DAY_WEEK':'WKDY_IM',\n",
    "        'DRINKING':'PERALCH_IM',\n",
    "        'EJECTION':'EJECT_IM',\n",
    "        'HARM_EV':'EVENT1_IM',\n",
    "        'HIT_RUN':'HITRUN_IM',\n",
    "        'HOUR':'HOUR_IM',\n",
    "        'IMPACT1':'IMPACT1_IM',\n",
    "        'INJ_SEV':'INJSEV_IM',\n",
    "        'LGT_COND':'LGTCON_IM',\n",
    "        'M_HARM':'VEVENT_IM',\n",
    "        'MAN_COLL':'MANCOL_IM',\n",
    "        'MAX_SEV':'MAXSEV_IM',\n",
    "        'MAX_VSEV':'MXVSEV_IM',\n",
    "        'MINUTE':'MINUTE_IM',\n",
    "        'MOD_YEAR':'MDLYR_IM',\n",
    "        'NUM_INJ':'NO_INJ_IM',\n",
    "        'NUM_INJV':'NUMINJ_IM',\n",
    "        'P_CRASH1':'PCRASH1_IM',\n",
    "        'RELJCT1':'RELJCT1_IM',\n",
    "        'RELJCT2':'RELJCT2_IM',\n",
    "        'SEAT_POS':'SEAT_IM',\n",
    "        'SEX':'SEX_IM',\n",
    "        'VEH_ALCH':'V_ALCH_IM',\n",
    "        'WEATHER':'WEATHR_IM',\n",
    "    }\n",
    "        \n",
    "    with open(\"../../Big_Files/Imputed_Feature_Dict.json\", \"w\") as outfile: \n",
    "        json.dump(Imputed_Feature_Dict, outfile)\n",
    "        \n",
    "    print ('Reading in Imputed_Feature__Dict')\n",
    "    with open('../../Big_Files/Imputed_Feature_Dict.json') as json_file:\n",
    "        D = json.load(json_file)\n",
    "    print (\"Did the read/write work? \", D == Imputed_Feature_Dict)\n",
    "    print ()\n",
    "\n",
    "    return Imputed_Feature_Dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d40677",
   "metadata": {},
   "source": [
    "## Make_Missing_Unknown_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecea8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Missing_Unknown_Dict(data, Imputed_Feature_Dict):\n",
    "    print ('Make_Missing_Unknown_Dict()')\n",
    "    \n",
    "    Missing_Unknown_Dict = {\n",
    "        # Features Imputed by CRSS.  The \"Missing\" values are the same as the ones imputed by CRSS\n",
    "        'AGE': [998,999],\n",
    "        'ALCOHOL': [9], # The value 8 is converted to attribute code 2 [User's Manual 2016-2021, page 64]\n",
    "        'BODY_TYP': [49,79,98,99], # Imputation discontinued after 2020\n",
    "        'DAY_WEEK': [9],\n",
    "        'DRINKING': [8,9],\n",
    "        'EJECTION': [7,9],\n",
    "        'HARM_EV': [98,99],\n",
    "        'HOUR': [99],\n",
    "        'IMPACT1': [98,99],\n",
    "        'INJ_SEV': [9],\n",
    "        'LGT_COND': [8,9],\n",
    "        'M_HARM': [98,99],\n",
    "        'MAN_COLL': [98,99],\n",
    "        'MAX_SEV': [9],\n",
    "        'MAX_VSEV': [9],\n",
    "        'MOD_YEAR': [9998,9999],\n",
    "        'NUM_INJ': [99], # The value 98 is converted to code 0 in the imputed version [User's Manual 2016-2021, page 63]\n",
    "        'NUM_INJV': [99], # The value 98 is converted to code 0 in the imputed version [User's Manual 2016-2021, page 114]\n",
    "        'RELJCT1': [8,9], # Imputed data element discontinued in 2019 and added back in 2020\n",
    "        'SEAT_POS': [19,29,39,49,98,99],\n",
    "        'SEX': [8,9],\n",
    "        'VEH_ALCH': [9], # Value 8 is converted to code 2 in the imputed version [User's Manual 2016-21, page 115]\n",
    "        'WEATHER': [98,99],\n",
    "        # Features Not Imputed by CRSS.  What qualifies as \"Missing\" is a little subjective.\n",
    "        'ACC_TYPE': [98,99],\n",
    "        'AIR_BAG': [98,99],\n",
    "        'ALC_RES': [996,997,998,999],\n",
    "        'ALC_STATUS': [8,9],\n",
    "        'BUS_USE': [98,99],\n",
    "        'CARGO_BT': [97,98,99],\n",
    "        'DEFORMED': [8,9],\n",
    "        'DRUGS': [8,9],\n",
    "        'DR_PRES': [9],\n",
    "        'DR_ZIP': [99998,99999],\n",
    "        'EMER_USE': [8,9],\n",
    "        'FIRE_EXP': [],\n",
    "        'HAZ_CNO': [88],\n",
    "        'HAZ_INV': [],\n",
    "        'HAZ_PLAC': [8],\n",
    "        'HAZ_REL': [8],\n",
    "        'HIT_RUN': [9],\n",
    "        'HOSPITAL': [],\n",
    "        'INT_HWY': [9],\n",
    "        'J_KNIFE': [],\n",
    "        'MAKE': [97,98,99],\n",
    "        'MAK_MOD': [97997,97998,97999,98997,98998,98999,99997,99998,99999],\n",
    "        'MODEL': [997,998,999],\n",
    "        'MONTH': [],\n",
    "        'M_HARM': [98,99],\n",
    "        'NUMOCCS': [99],\n",
    "        'PCRASH4': [9],\n",
    "        'PCRASH5': [9],\n",
    "        'PERMVIT': [],\n",
    "        'PERNOTMVIT': [],\n",
    "        'PER_TYP': [9],\n",
    "        'PJ': [],\n",
    "        'PSU': [],\n",
    "        'PVH_INVL': [],\n",
    "        'P_CRASH1': [98,99],\n",
    "        'P_CRASH2': [98,99],\n",
    "        'P_CRASH3': [98,99],\n",
    "        'REGION': [],\n",
    "        'RELJCT2': [98,99],\n",
    "        'REL_ROAD': [98,99],\n",
    "        'REST_MIS': [],\n",
    "        'REST_USE': [98,99],\n",
    "        'ROLINLOC': [9],\n",
    "        'ROLLOVER': [],\n",
    "        'SCH_BUS': [],\n",
    "        'SPEC_USE': [98,99],\n",
    "        'SPEEDREL': [9],\n",
    "        'TOWED': [8,9],\n",
    "        'TOW_VEH': [9],\n",
    "        'TRAV_SP': [998,999],\n",
    "        'TYP_INT': [98,99],\n",
    "        'URBANICITY': [],\n",
    "        'VALIGN': [8,9],\n",
    "        'VE_FORMS': [],\n",
    "        'VE_TOTAL': [],\n",
    "        'VNUM_LAN': [8,9],\n",
    "        'VPROFILE': [8,9],\n",
    "        'VSPD_LIM': [98,99],\n",
    "        'VSURCOND': [98,99],\n",
    "        'VTCONT_F': [8,9],\n",
    "        'VTRAFCON': [97,98,99],\n",
    "        'VTRAFWAY': [8,9],\n",
    "        'WRK_ZONE': [],\n",
    "        'YEAR': [],\n",
    "        'VEH_AGE': [99],\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    Features = [feature for feature in data]\n",
    "    Keys = Missing_Unknown_Dict.keys()\n",
    "    Features.sort()\n",
    "    N = len(data)\n",
    "    for feature in Features:\n",
    "        if '_IM' not in feature:\n",
    "            if feature in Keys:\n",
    "                print (feature, Missing_Unknown_Dict[feature])\n",
    "                s = 0\n",
    "                D = Missing_Unknown_Dict[feature]\n",
    "                for d in D:\n",
    "                    t = len(data[data[feature]==d])\n",
    "                    s += t\n",
    "                    print (feature, d, t)\n",
    "                print ()\n",
    "                print (feature, s, round(s/N*100,2), \"% Missing\")\n",
    "                if feature in Imputed_Feature_Dict.keys():\n",
    "                    print (Imputed_Feature_Dict[feature], \" in data\")\n",
    "                else:\n",
    "                    print (\"Imputed Feature not in data\")\n",
    "                print ()\n",
    "                    \n",
    "            else:\n",
    "                print (feature, \" MISSING\")\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"../../Big_Files/Missing_Unknown_Dict.json\", \"w\") as outfile: \n",
    "        json.dump(Missing_Unknown_Dict, outfile)\n",
    "        \n",
    "    print ('Reading in Missing_Unknown_Dict')\n",
    "    with open('../../Big_Files/Missing_Unknown_Dict.json') as json_file:\n",
    "        D = json.load(json_file)\n",
    "    print (\"Did the read/write work? \", D == Missing_Unknown_Dict)\n",
    "    print ()\n",
    "\n",
    "    return Missing_Unknown_Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0afa1",
   "metadata": {},
   "source": [
    "## Consolidate Values following CRSS Imputation\n",
    "- When CRSS imputed some features, they consolidated the \"Not Applicable\" code into the \"None\" code.  \n",
    "- Here we change those codes in the unimputed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81339ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Consolidate_Values_following_CRSS_Imputation(data):\n",
    "    print ('Consolidate_Values_following_CRSS_Imputation()')\n",
    "    \n",
    "    print ('ALCOHOL should change 8 to 2')\n",
    "    print (data['ALCOHOL'].value_counts())\n",
    "#    data['ALCOHOL'].where( ~data['ALCOHOL'] != 8, 2, inplace=True)\n",
    "    data['ALCOHOL'][data['ALCOHOL']==8] = 2\n",
    "    print (data['ALCOHOL'].value_counts())\n",
    "    \n",
    "    \n",
    "    data['NUM_INJ'][data['NUM_INJ']==98] = 0\n",
    "    data['NUM_INJV'][data['NUM_INJV']==98] = 0\n",
    "    data['VEH_ALCH'][data['VEH_ALCH']==8] = 2\n",
    "    \n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663ab69",
   "metadata": {},
   "source": [
    "## Drop Features with Excessive Missing\n",
    "- Drop features with more than 20% of values missing or unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ea3fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Features_with_Excessive_Missing(data, Missing_Unknown_Dict, Imputed_Feature_Dict):\n",
    "    print ('Drop_Features_with_Excessive_Missing()')\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    for feature in data:\n",
    "        if feature[-3:] != '_IM':\n",
    "            s = len(data[data[feature].isin(Missing_Unknown_Dict[feature])])\n",
    "            if s/N > 0.20:\n",
    "                print (feature, s, round(s/N*100,2))\n",
    "                data.drop(columns=[feature], inplace=True)\n",
    "                print ('Dropped ', feature)\n",
    "                if feature in Imputed_Feature_Dict.keys():\n",
    "                    data.drop(columns=[Imputed_Feature_Dict[feature]], inplace=True)\n",
    "                    print ('Dropped ', Imputed_Feature_Dict[feature])\n",
    "                print (data.shape)\n",
    "                print ()\n",
    "    print ()\n",
    "                \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb2fa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_NaN(data):\n",
    "    print ('Count NaN in each feature and drop column if it has NaN')\n",
    "    print ('For comparison, here are the YEAR value counts for the whole dataset:')\n",
    "    print (data['YEAR'].value_counts())\n",
    "    print ()\n",
    "    for feature in data:\n",
    "        n = data[feature].isna().sum()\n",
    "        if n>0 and 'NAME' not in feature:\n",
    "            print (feature, n)\n",
    "            B = data[[feature, 'YEAR']].copy()\n",
    "            B = B[pd.isnull(B[feature])]\n",
    "#            B.dropna(subset = [feature], inplace=True)\n",
    "            print (B['YEAR'].value_counts())\n",
    "            print ()\n",
    "            data.drop(columns=[feature], inplace=True)\n",
    "            print (data.shape)\n",
    "    print ()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e582a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_Features_with_Dominant_Value(data, Missing_Unknown_Dict, Imputed_Feature_Dict):\n",
    "    print ('Drop_Features_with_Dominant_Value()')\n",
    "    for feature in data:\n",
    "        if '_IM' not in feature:\n",
    "            MU = Missing_Unknown_Dict[feature]\n",
    "            Feature = data[feature]\n",
    "            for mu in MU:\n",
    "                Feature = Feature[Feature != mu]\n",
    "            U = Feature.unique()\n",
    "            V = list(Feature.value_counts(normalize=True))\n",
    "            if V[0] > 0.95:\n",
    "                print (feature, len(U), V[0])\n",
    "                if V[0] > 0.99:\n",
    "                    data.drop(columns=[feature], inplace=True)\n",
    "                    print ('Dropped ', feature)\n",
    "                    if feature in Imputed_Feature_Dict.keys():\n",
    "                        data.drop(columns=[Imputed_Feature_Dict[feature]], inplace=True)\n",
    "                        print ('Dropped ', Imputed_Feature_Dict[feature])\n",
    "                    print (data.shape)\n",
    "            if V[0] < 0.05:\n",
    "                print (feature,len(U), V[0])\n",
    "    print ()\n",
    "    \n",
    "    return data            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d9a0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Features(data):\n",
    "    n = 0\n",
    "    m = 0\n",
    "    for feature in data:\n",
    "        if feature[-3:] == '_IM':\n",
    "            m += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    print (n, ' Raw Features')\n",
    "    print (m, ' Imputed Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5af9a",
   "metadata": {},
   "source": [
    "## Run:  Get Data and Preprocess\n",
    "- CPU times: user 3min 33s, sys: 19.2 s, total: 3min 52s\n",
    "- Wall time: 3min 56s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d75392b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess_Data()\n",
      "Get_Data_from_Temp_Files()\n",
      "df_Acc.shape =  (367232, 51)\n",
      "df_Veh.shape =  (647855, 98)\n",
      "df_Per.shape =  (910183, 71)\n",
      "\n",
      "Drop_Repeated_Features()\n",
      "df_Acc.shape =  (367232, 51)\n",
      "df_Veh.shape =  (647855, 84)\n",
      "df_Per.shape =  (910183, 40)\n",
      "\n",
      "Drop_Irrelevant_Features()\n",
      "df_Acc.shape =  (367232, 51)\n",
      "df_Veh.shape =  (647855, 84)\n",
      "df_Per.shape =  (910183, 40)\n",
      "\n",
      "Drop Names of Dropped Features in df_Acc\n",
      "Drop Names of Dropped Features in df_Veh\n",
      "Drop Names of Dropped Features in df_Per\n",
      "df_Acc.shape =  (367232, 40)\n",
      "df_Veh.shape =  (647855, 56)\n",
      "df_Per.shape =  (910183, 24)\n",
      "\n",
      "Merge()\n",
      "\n",
      "df_Acc.shape\n",
      "(367232, 40)\n",
      "df_Veh.shape\n",
      "(647855, 56)\n",
      "data.shape\n",
      "(647855, 95)\n",
      "\n",
      "df_Acc.shape\n",
      "(367232, 40)\n",
      "df_Veh.shape\n",
      "(647855, 56)\n",
      "df_Per.shape\n",
      "(910183, 24)\n",
      "data.shape\n",
      "(873784, 114)\n",
      "\n",
      "Remove_Pedestrian_Crashes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PEDS\n",
       "0     833798\n",
       "1      38631\n",
       "2       1168\n",
       "3        134\n",
       "4         36\n",
       "6          8\n",
       "5          6\n",
       "11         1\n",
       "7          1\n",
       "8          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 39986 crashes that involve a pedestrian.\n",
      "data.shape:  (833798, 114)\n",
      "\n",
      "Count NaN in each feature and drop column if it has NaN\n",
      "For comparison, here are the YEAR value counts for the whole dataset:\n",
      "YEAR\n",
      "2017    127396\n",
      "2019    123958\n",
      "2021    122262\n",
      "2020    120404\n",
      "2022    120232\n",
      "2018    111020\n",
      "2016    108526\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RELJCT1_IM 123958\n",
      "YEAR\n",
      "2019    123958\n",
      "Name: count, dtype: int64\n",
      "\n",
      "(833798, 111)\n",
      "HITRUN_IM 362898\n",
      "YEAR\n",
      "2021    122262\n",
      "2020    120404\n",
      "2022    120232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "(833798, 110)\n",
      "BDYTYP_IM 242494\n",
      "YEAR\n",
      "2021    122262\n",
      "2022    120232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "(833798, 109)\n",
      "\n",
      "Make_Imputed_Feature_Dict()\n",
      "Reading in Imputed_Feature__Dict\n",
      "Did the read/write work?  True\n",
      "\n",
      "Make_Missing_Unknown_Dict()\n",
      "Reading in Missing_Unknown_Dict\n",
      "Did the read/write work?  True\n",
      "\n",
      "Consolidate_Values_following_CRSS_Imputation()\n",
      "ALCOHOL should change 8 to 2\n",
      "ALCOHOL\n",
      "2    589293\n",
      "9    210364\n",
      "1     34027\n",
      "8       114\n",
      "Name: count, dtype: int64\n",
      "ALCOHOL\n",
      "2    589407\n",
      "9    210364\n",
      "1     34027\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature_Engineering()\n",
      "(833798, 109)\n",
      "(833798, 107)\n",
      "\n",
      "Drop_Features_with_Excessive_Missing()\n",
      "ALCOHOL 210364 25.23\n",
      "Dropped  ALCOHOL\n",
      "Dropped  ALCHL_IM\n",
      "(833798, 105)\n",
      "\n",
      "TRAV_SP 427327 51.25\n",
      "Dropped  TRAV_SP\n",
      "(833798, 104)\n",
      "\n",
      "VEH_ALCH 179202 21.49\n",
      "Dropped  VEH_ALCH\n",
      "Dropped  V_ALCH_IM\n",
      "(833798, 102)\n",
      "\n",
      "VNUM_LAN 223404 26.79\n",
      "Dropped  VNUM_LAN\n",
      "(833798, 101)\n",
      "\n",
      "P_CRASH3 605556 72.63\n",
      "Dropped  P_CRASH3\n",
      "(833798, 100)\n",
      "\n",
      "DRINKING 329697 39.54\n",
      "Dropped  DRINKING\n",
      "Dropped  PERALCH_IM\n",
      "(833798, 98)\n",
      "\n",
      "ALC_RES 687309 82.43\n",
      "Dropped  ALC_RES\n",
      "(833798, 97)\n",
      "\n",
      "DRUGS 402575 48.28\n",
      "Dropped  DRUGS\n",
      "(833798, 96)\n",
      "\n",
      "\n",
      "Drop_Features_with_Dominant_Value()\n",
      "PSU 60 0.04984660553275493\n",
      "PJ 425 0.034388425014212076\n",
      "PVH_INVL 12 0.9749627607645976\n",
      "PERNOTMVIT 9 0.9964523781539414\n",
      "Dropped  PERNOTMVIT\n",
      "(833798, 95)\n",
      "WRK_ZONE 5 0.9805000731592064\n",
      "SCH_BUS 2 0.9949700047253651\n",
      "Dropped  SCH_BUS\n",
      "(833798, 94)\n",
      "HIT_RUN 2 0.9520900198135701\n",
      "MAK_MOD 1216 0.030099148421446374\n",
      "TOW_VEH 8 0.9733051386797505\n",
      "J_KNIFE 4 0.9734827859985272\n",
      "CARGO_BT 15 0.9714709925627028\n",
      "HAZ_INV 2 0.9995790347302345\n",
      "Dropped  HAZ_INV\n",
      "(833798, 93)\n",
      "HAZ_PLAC 3 0.9996197964891744\n",
      "Dropped  HAZ_PLAC\n",
      "(833798, 92)\n",
      "HAZ_CNO 9 0.9997600903020103\n",
      "Dropped  HAZ_CNO\n",
      "(833798, 91)\n",
      "HAZ_REL 3 0.9996449745305208\n",
      "Dropped  HAZ_REL\n",
      "(833798, 90)\n",
      "BUS_USE 8 0.9955781542156585\n",
      "Dropped  BUS_USE\n",
      "(833798, 89)\n",
      "SPEC_USE 18 0.9884651824357072\n",
      "EMER_USE 6 0.9967588786806645\n",
      "Dropped  EMER_USE\n",
      "(833798, 88)\n",
      "ROLLOVER 6 0.9650622812719628\n",
      "ROLINLOC 9 0.9655590740716298\n",
      "FIRE_EXP 2 0.9975677562191322\n",
      "Dropped  FIRE_EXP\n",
      "(833798, 87)\n",
      "DR_PRES 2 0.999854877263345\n",
      "Dropped  DR_PRES\n",
      "(833798, 86)\n",
      "DR_ZIP 19367 0.005146606200861794\n",
      "PCRASH4 7 0.9625992284526245\n",
      "AGE 117 0.026853137942019753\n",
      "EJECTION 5 0.9665511265164645\n",
      "ALC_STATUS 3 0.9779733777086257\n",
      "\n",
      "len(data) before dropping missing hospital values =  833798\n",
      "len(data) after dropping missing hospital values =  817623\n",
      "\n",
      "67  Raw Features\n",
      "19  Imputed Features\n",
      "        'ACC_TYPE',\n",
      "        'AGE',\n",
      "        'AIR_BAG',\n",
      "        'ALC_STATUS',\n",
      "        'BODY_TYP',\n",
      "        'CARGO_BT',\n",
      "        'DAY_WEEK',\n",
      "        'DEFORMED',\n",
      "        'DR_ZIP',\n",
      "        'EJECTION',\n",
      "        'HARM_EV',\n",
      "        'HIT_RUN',\n",
      "        'HOSPITAL',\n",
      "        'HOUR',\n",
      "        'IMPACT1',\n",
      "        'INJ_SEV',\n",
      "        'INT_HWY',\n",
      "        'J_KNIFE',\n",
      "        'LGT_COND',\n",
      "        'MAKE',\n",
      "        'MAK_MOD',\n",
      "        'MAN_COLL',\n",
      "        'MAX_SEV',\n",
      "        'MAX_VSEV',\n",
      "        'MODEL',\n",
      "        'MONTH',\n",
      "        'M_HARM',\n",
      "        'NUMOCCS',\n",
      "        'NUM_INJ',\n",
      "        'NUM_INJV',\n",
      "        'PCRASH4',\n",
      "        'PCRASH5',\n",
      "        'PERMVIT',\n",
      "        'PER_TYP',\n",
      "        'PJ',\n",
      "        'PSU',\n",
      "        'PVH_INVL',\n",
      "        'P_CRASH1',\n",
      "        'P_CRASH2',\n",
      "        'REGION',\n",
      "        'RELJCT1',\n",
      "        'RELJCT2',\n",
      "        'REL_ROAD',\n",
      "        'REST_MIS',\n",
      "        'REST_USE',\n",
      "        'ROLINLOC',\n",
      "        'ROLLOVER',\n",
      "        'SEAT_POS',\n",
      "        'SEX',\n",
      "        'SPEC_USE',\n",
      "        'SPEEDREL',\n",
      "        'TOWED',\n",
      "        'TOW_VEH',\n",
      "        'TYP_INT',\n",
      "        'URBANICITY',\n",
      "        'VALIGN',\n",
      "        'VEH_AGE',\n",
      "        'VE_FORMS',\n",
      "        'VE_TOTAL',\n",
      "        'VPROFILE',\n",
      "        'VSPD_LIM',\n",
      "        'VSURCOND',\n",
      "        'VTCONT_F',\n",
      "        'VTRAFCON',\n",
      "        'VTRAFWAY',\n",
      "        'WEATHER',\n",
      "        'WRK_ZONE',\n",
      "Finished Preprocess_Data()\n",
      "CPU times: user 27.3 s, sys: 6.44 s, total: 33.7 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Preprocess_Data():\n",
    "    print ('Preprocess_Data()')\n",
    "    \n",
    "#    df_Acc, df_Veh, df_Per = Get_Data_from_Original()\n",
    "    \n",
    "#    for feature in df_Acc:\n",
    "#        print ('df_Acc', feature)\n",
    "#    print ()\n",
    "#    for feature in df_Veh:\n",
    "#        print ('df_Veh', feature)\n",
    "#    print ()\n",
    "#    for feature in df_Per:\n",
    "#        print ('df_Per', feature)\n",
    "#    print ()\n",
    "\n",
    "    df_Acc, df_Veh, df_Per = Get_Data_from_Temp_Files()\n",
    "    df_Acc, df_Veh, df_Per = Drop_Repeated_Features(df_Acc, df_Veh, df_Per)    \n",
    "    df_Acc, df_Veh, df_Per = Drop_Irrelevant_Features (df_Acc, df_Veh, df_Per)    \n",
    "\n",
    "    data = Merge (df_Acc, df_Veh, df_Per)\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Count_NaN(data)    \n",
    "    \n",
    "    Imputed_Feature_Dict = Make_Imputed_Feature_Dict(data)\n",
    "    Missing_Unknown_Dict = Make_Missing_Unknown_Dict(data, Imputed_Feature_Dict)\n",
    "\n",
    "    data = Consolidate_Values_following_CRSS_Imputation(data) \n",
    "    data = Feature_Engineering(data, Missing_Unknown_Dict)\n",
    "    \n",
    "    data = Drop_Features_with_Excessive_Missing(data, Missing_Unknown_Dict, Imputed_Feature_Dict)\n",
    "    data = Drop_Features_with_Dominant_Value(data, Missing_Unknown_Dict, Imputed_Feature_Dict)\n",
    "    \n",
    "    # Drop samples where we don't know whether the person went to the hospital.\n",
    "#    print (data['HOSPITAL'].value_counts())\n",
    "    print ('len(data) before dropping missing hospital values = ', len(data))\n",
    "    data = data[data['HOSPITAL'] < 8]\n",
    "    print ('len(data) after dropping missing hospital values = ', len(data))\n",
    "    print ()\n",
    "    \n",
    "    # Bin the target variable.  \n",
    "    # Either the person went to the hospital or didn't; we don't care how the person got to the hospital.\n",
    "    data['HOSPITAL'] = data['HOSPITAL'].apply(lambda x:1 if x in [1,2,3,4,5] else 0)\n",
    "    \n",
    "#    Analyze_Stuff(data)\n",
    "\n",
    "    Count_Features(data)\n",
    "    \n",
    "    A = []\n",
    "    for feature in data:\n",
    "        if '_IM' not in feature:\n",
    "            A.append(feature)\n",
    "    A.sort()\n",
    "    for feature in A:\n",
    "        print (\"        '%s',\" % feature)\n",
    "    \n",
    "#    data.to_csv('../../Big_Files/CRSS_Merged_Raw_Data.csv', index=False)\n",
    "    data.to_csv('../../Big_Files/CRSS_01.csv', index=False)\n",
    "    \n",
    "    # Make a sample of the dataset for testing while writing code\n",
    "#    data.sample(frac=0.1).to_csv('../../Big_Files/CRSS_Merged_Raw_Data_Sample_frac_01.csv', index=False)\n",
    "\n",
    "    # Make a really small sample of the dataset for testing while writing code\n",
    "#    data.sample(n=1000).to_csv('../../Big_Files/CRSS_Merged_Raw_Data_Sample_n_1000.csv', index=False)\n",
    "\n",
    "    print ('Finished Preprocess_Data()')\n",
    "\n",
    "Preprocess_Data()\n",
    "\n",
    "#CPU times: user 19.4 s, sys: 4.52 s, total: 24 s\n",
    "#Wall time: 25.2 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bface0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c9b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86570e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec6303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
