{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea489d64",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa423799",
   "metadata": {},
   "source": [
    "- We will build models reflecting three levels of available data\n",
    "    - \"Easy\" is mostly data already available to the emergency dispatcher before the notification comes in, like month, day of week, hour, weather, urban/rural, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e2353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "NumPy version: 1.24.2\n",
      "SciPy version:  1.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bburkman/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n",
      "Keras version:  2.11.0\n",
      "Pandas version:  1.5.3\n",
      "SciKit-Learn version: 1.2.2\n",
      "Imbalanced-Learn version: 0.10.1\n",
      "Finished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "#from keras.layers import IntegerLookup\n",
    "#from keras.layers import Normalization\n",
    "#from keras.layers import StringLookup\n",
    "#from keras.utils import get_custom_objects\n",
    "#from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f109",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919fb2db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data()')\n",
    "    data = pd.read_csv(\n",
    "        '../../Big_Files/CRSS_Imputed.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Get_Data()')\n",
    "    print ()\n",
    "    return data\n",
    "\n",
    "def Test_Get_Data():\n",
    "    data = Get_Data()\n",
    "    display (data.head())\n",
    "    \n",
    "#Test_Get_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146091c",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb9dce",
   "metadata": {},
   "source": [
    "## Engineer Features\n",
    "- AGE_x_SEX\n",
    "    - We had found that the correlation between age and hospitalization varied by sex, so we made a new feature that captured the complexities\n",
    "- AGE_x_SCH_BUS\n",
    "    - We also found that those on a school bus had different rates of hospitalization based on age, so we created this more complex feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85fa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_Cross_Two(data):\n",
    "    print ('Feature_Engineering_Cross_Two')\n",
    "    Pairs = [\n",
    "        ['AGE', 'SEX', 'AGE_x_SEX'],\n",
    "        ['AGE', 'SCH_BUS', 'AGE_x_SCH_BUS']\n",
    "    ]\n",
    "    for P in Pairs:\n",
    "        data[P[2]] = data[P[0]].map(str) + '_x_' + data[P[1]].map(str)\n",
    "    \n",
    "    print ()\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900000",
   "metadata": {},
   "source": [
    "## Thin Features \n",
    "### Thin Features to only \"Hard\" Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Hard_Features(data):\n",
    "    print ('Thin_to_Hard_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "        'PERMVIT',\n",
    "        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "        'BUS_USE',\n",
    "        'EMER_USE',\n",
    "        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "        'LOCATION',\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Hard_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Hard_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Hard_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Hard_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e759eda",
   "metadata": {},
   "source": [
    "### Thin Features to \"Medium\" Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4202cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Medium_Features(data):\n",
    "    print ('Thin_to_Medium_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Medium_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Medium_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Medium_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Medium_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc4855",
   "metadata": {},
   "source": [
    "### Thin Features to \"Easy\" Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11264ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Easy_Features(data):\n",
    "    print ('Thin_to_Easy_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "#        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "#        'REL_ROAD',\n",
    "#        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "#        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "#        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "#        'VPROFILE',\n",
    "#        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "#        'VTRAFCON',\n",
    "#        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "#        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "#        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "#        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_to_Easy_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Easy_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Easy_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Easy_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52366e1a",
   "metadata": {},
   "source": [
    "## Get Dummies\n",
    "- Transform categorical data into one-hot-encoded features\n",
    "- For each value in the category, make a new feature that is \"1\" when the feature has that value, \"0\" otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Dummies(data, target):\n",
    "    print ('Get_Dummies')\n",
    "    data = data.astype('category')\n",
    "    Target = data.pop(target)\n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns)\n",
    "    data_Dummies = data_Dummies.join(Target)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print ()\n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "def Test_Get_Dummies():\n",
    "    print ('Test_Get_Dummies')\n",
    "    A = pd.DataFrame({\n",
    "        'A': ['a', 'b', 'a'], \n",
    "        'B': ['b', 'a', 'c'], \n",
    "        'C': [1, 2, 3]})\n",
    "    C = Get_Dummies(A, 'C')\n",
    "    display(C)\n",
    "    print ()\n",
    "\n",
    "#Test_Get_Dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3917b",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d83bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chart_and_Plots(y_test, y_proba, y_pred, filename, title):\n",
    "    \n",
    "    \n",
    "#    Analyze_Prediction(y_test, y_proba, filename, title)\n",
    "    Analyze_Prediction_2(y_test, y_proba, filename, title)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 5000)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 2000)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 1500)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 1200)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 800)\n",
    "    Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, 500)\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "#    print (\"type(y_proba): \", type(y_proba))\n",
    "    left = min(y_proba)\n",
    "    right = max(y_proba)\n",
    "#    print (left, right)\n",
    "    Plot_Prediction_Zoom(y_test, y_proba, filename, title, left, right)\n",
    "    Plot_Prediction_Zoom_Wide(y_test, y_proba, filename, title, left, right)\n",
    "\n",
    "    ROC(y_test, y_proba, [], filename)\n",
    "    TeX_Results(filename)\n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.0, 1.0)\n",
    "    Filename = filename + '_Transformed_100'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "    TeX_Results(Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.01,0.99)\n",
    "    Filename = filename + '_Transformed_98'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "    TeX_Results(Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.025,0.975)\n",
    "    Filename = filename + '_Transformed_95'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "    TeX_Results(Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.05,0.95)\n",
    "    Filename = filename + '_Transformed_90'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "    TeX_Results(Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.1,0.9)\n",
    "    Filename = filename + '_Transformed_80'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "    TeX_Results(Filename)\n",
    "\n",
    "    \n",
    "    print ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6adde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TeX_Results(filename):\n",
    "    TeX = open('../Paper_01_11_24/Results_Temp.tex', 'a')\n",
    "    TeX.write('\\\\verb|%s|\\n\\n' % filename)\n",
    "    TeX.write('\\\\noindent\\\\begin{tabular}{@{\\\\hspace{-6pt}}p{4.3in} @{\\\\hspace{-6pt}}p{2.0in}}\\n\\n')\n",
    "    TeX.write('\\\\vskip 0pt\\n\\n')\n",
    "    TeX.write('\\\\hfil Raw Model Output\\n\\n')\n",
    "    TeX.write('\\\\input{../Keras/Images/%s_Pred_Wide.pgf}\\n\\n' % filename)\n",
    "    TeX.write('&\\n\\n\\\\vskip 0pt\\n\\n\\\\hfil ROC Curve\\n\\n')\n",
    "    TeX.write('\\\\input{../Keras/Images/%s_ROC.pgf}\\n\\n' % filename)\n",
    "    TeX.write('\\\\end{tabular}\\n\\n\\\\vskip 12pt\\n\\n')\n",
    "    \n",
    "    TeX.write('\\\\input{../Keras/Analyze_Proba/%s_20}\\n\\n' % filename)\n",
    "    TeX.write('\\\\newpage\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31ae0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba(y_test, y_proba):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],0.025)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],0.975)\n",
    "#    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba_Specified(y_test, y_proba, left, right):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],left)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],right)\n",
    "#    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97848247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Model(y_test, y_proba, y_pred, center, filename):\n",
    "    print ('Evaluate_Model()')\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = [round(x) for x in y_proba]\n",
    "    y_pred = np.array(y_pred)\n",
    "#    print ('np.unique(y_proba) = ', np.unique(y_proba))\n",
    "#    print ('np.unique(y_pred) = ', np.unique(y_pred))\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "#    print(CM)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    f = open('./Analyze_Proba/ROC_AUC.csv', 'a')\n",
    "    f.write('%s,%f\\n' % (filename, auc_value) )\n",
    "    f.close()\n",
    "    \n",
    "    CSV = [[filename, CM[0][0], CM[0][1], CM[1][0], CM[1][1], center, auc_value]]\n",
    "    np.savetxt('./Confusion_Matrices/' + filename + '.csv', \n",
    "        CSV,\n",
    "        delimiter =\", \", \n",
    "        fmt ='% s'\n",
    "              )\n",
    "#    print ()\n",
    "    CM = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "#    print(CM)\n",
    "#    print ()\n",
    "\n",
    "#    y_pred = y_pred.ravel()\n",
    "#    y_test = tf.convert_to_tensor(y_test)\n",
    "#    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "#    print ('%.3f & Precision \\cr ' %  Precision_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & Recall \\cr ' %  Recall_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & F1 \\cr ' %  F1_Metric(y_test, y_pred).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fee38",
   "metadata": {},
   "source": [
    "# Plots and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf928217",
   "metadata": {},
   "source": [
    "## Plot Prediction\n",
    "\n",
    "How to insert a .pgf plot into a \\LaTeX document:\n",
    "\n",
    "\\begin{figure}\n",
    "    \\begin{center}\n",
    "        \\input{Plot.pgf}\n",
    "    \\end{center}\n",
    "    \\caption{A PGF histogram from \\texttt{matplotlib}.}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3496ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (0, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "    \n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    plt.xticks(\n",
    "        ticks = [0, 2.5, 5, 7.5, 10], \n",
    "        labels = ['0.0', '0.25', '0.5', '0.75', '1.0'],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred.png', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.savefig('./Images/' + filename + '_Pred.pgf', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    print ('./Images/' + filename + '_Pred.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b634349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Zoom(y_test, y_proba, filename, title, left, right):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    B = B[A['HOSPITAL'] > left]\n",
    "    B = B[A['HOSPITAL'] < right]\n",
    "    A = A[A['HOSPITAL'] > left]\n",
    "    A = A[A['HOSPITAL'] < right]\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [left + (right-left)*x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "\n",
    "    ticks = [0, 5, 10]\n",
    "    num_prec = int(-(math.log10((right-left)/2)))+1\n",
    "    num_prec = max(num_prec,2)\n",
    "    \n",
    "    if num_prec==2:\n",
    "        labels = [\"{:.2f}\".format(round(left + (right-left) * t/10,num_prec)) for t in ticks]\n",
    "    if num_prec==3:\n",
    "        labels = [\"{:.3f}\".format(round(left + (right-left) * t/10,num_prec)) for t in ticks]\n",
    "    if num_prec==4:\n",
    "        labels = [\"{:.4f}\".format(round(left + (right-left) * t/10,num_prec)) for t in ticks]\n",
    "    if num_prec>4:\n",
    "        labels = [\"{:.5f}\".format(round(left + (right-left) * t/10,num_prec)) for t in ticks]\n",
    "    \n",
    "    \n",
    "#    labels = [str(round(left + (right-left) * t/10,3)) for t in ticks]\n",
    "    plt.xticks(\n",
    "        ticks = ticks, \n",
    "        labels = labels,\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.png', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.pgf', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    print ('./Images/' + filename + '_Pred_Zoom.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4131e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Zoom_Wide(y_test, y_proba, filename, title, left, right):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    B = B[A['HOSPITAL'] > left]\n",
    "    B = B[A['HOSPITAL'] < right]\n",
    "    A = A[A['HOSPITAL'] > left]\n",
    "    A = A[A['HOSPITAL'] < right]\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 20\n",
    "    bins= [left + (right-left)*x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(4.5,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "\n",
    "#    ticks = [0, 2.5, 5, 7.5, 10]\n",
    "    ticks = [0, 4, 8, 12, 16, 20]\n",
    "    num_prec = int(-(math.log10((right-left)/4)))+1\n",
    "    num_prec = max(num_prec,2)\n",
    "#    print (\"left, right, (right-left)/5, -(math.log10((right-left)/5)), num_prec\")\n",
    "#    print (left, right, (right-left)/5, -(math.log10((right-left)/5)), num_prec)\n",
    "    \n",
    "    if num_prec<3:\n",
    "        labels = [\"{:.2f}\".format(round(left + (right-left) * t/20,num_prec)) for t in ticks]\n",
    "    if num_prec==3:\n",
    "        labels = [\"{:.3f}\".format(round(left + (right-left) * t/20,num_prec)) for t in ticks]\n",
    "    if num_prec==4:\n",
    "        labels = [\"{:.4f}\".format(round(left + (right-left) * t/20,num_prec)) for t in ticks]\n",
    "    if num_prec>4:\n",
    "        labels = [\"{:.5f}\".format(round(left + (right-left) * t/20,num_prec)) for t in ticks]\n",
    "#    labels = [str(round(left + (right-left) * t/20,num_prec)) for t in ticks]\n",
    "    plt.xticks(\n",
    "        ticks = ticks, \n",
    "        labels = labels,\n",
    "        rotation=0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom_Wide.png', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom_Wide.pgf', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    print ('./Images/' + filename + '_Pred_Zoom_Wide.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9eda862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot_Prediction()\n",
      "Test\n",
      "./Images/Test_Pred_Wide.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Plot_Prediction_Wide(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print ('y_test = ', y_test)\n",
    "#    print ('y_proba = ',y_proba)\n",
    "\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    print (\"A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\")\n",
    "#    display(A)\n",
    "#    print (\"B = pd.DataFrame(y_test, columns=['HOSPITAL'])\")\n",
    "#    display(B)\n",
    "#    print (\"C = A[B['HOSPITAL']==0]\")\n",
    "#    display(C)\n",
    "#    print (\"D = A[B['HOSPITAL']==1]\")\n",
    "#    display(D)\n",
    "    n = 20\n",
    "#    bins= [x/n - 1/(2*n) for x in range (-1, n+3)]\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print ('Bins = ', bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "#    print (\"E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(E)\n",
    "#    print (\"F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(F)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "#    print (\"G = E.value_counts(sort=False)\")\n",
    "#    display(G)\n",
    "#    print (\"H = F.value_counts(sort=False)\")\n",
    "#    display(H)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "#    print (\"G = G/len(y_proba)*100\")\n",
    "#    display(G)\n",
    "#    print (\"H = H/len(y_proba)*100\")\n",
    "#    display(H)\n",
    "\n",
    "    fig = plt.figure(figsize=(4.5,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    ticks = [n/20*i for i in range (-1,22)]\n",
    "#    print ('ticks = ', ticks)\n",
    "    plt.xticks(\n",
    "        ticks = ticks,\n",
    "        labels = ['','0.0', '', '0.1', '', '0.2', '', '0.3', '', '0.4', '', '0.5', '', '0.6', '', '0.7', '', '0.8', '', '0.9', '', '1.0', ''],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.png', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.pgf', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    print ('./Images/' + filename + '_Pred_Wide.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "\n",
    "def Test_Plot_Prediction_Wide():\n",
    "    \n",
    "    y_proba = (\n",
    "        [0.0]*5 + \n",
    "        [0.0]*0 + \n",
    "        [0.1]*6 + \n",
    "        [0.1]*1 + \n",
    "        [0.2]*7 + \n",
    "        [0.2]*2 + \n",
    "        [0.3]*6 + \n",
    "        [0.3]*1 + \n",
    "        [0.4]*8 + \n",
    "        [0.4]*2 + \n",
    "        [0.5]*9 + \n",
    "        [0.5]*2 + \n",
    "        [0.6]*8 + \n",
    "        [0.6]*2 + \n",
    "        [0.7]*6 + \n",
    "        [0.7]*3 + \n",
    "        [0.8]*5 + \n",
    "        [0.8]*3 + \n",
    "        [0.9]*3 + \n",
    "        [0.9]*2 + \n",
    "        [1.0]*0 + \n",
    "        [1.0]*2 \n",
    "    )\n",
    "    y_test = (\n",
    "        [0]*5 + \n",
    "        [1]*0 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*7 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*9 + \n",
    "        [1]*2 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*3 + \n",
    "        [0]*5 + \n",
    "        [1]*3 + \n",
    "        [0]*3 + \n",
    "        [1]*2 + \n",
    "        [0]*0 + \n",
    "        [1]*2 \n",
    "    )\n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')\n",
    "    \n",
    "Test_Plot_Prediction_Wide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e84dc",
   "metadata": {},
   "source": [
    "## Switching between FP/TP and Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee52a47",
   "metadata": {},
   "source": [
    "$$\\text{Precision} = \\frac{TP}{FP+TP}$$\n",
    "\n",
    "$$\\frac{1}{\\text{Precision}} = \\frac{FP+TP}{TP} = \\frac{FP}{TP} + \\frac{TP}{TP} = \\frac{FP}{TP} +  1$$\n",
    "\n",
    "$$\\frac{FP}{TP} + 1 = \\frac{1}{\\text{Precision}}$$\n",
    "\n",
    "$$\\frac{FP}{TP} = \\frac{1}{\\text{Precision}} - 1 = \\frac{1}{\\text{Precision}} - \\frac{\\text{Precision}}{\\text{Precision}}  = \\frac{1 - \\text{Precision}}{\\text{Precision}}$$\n",
    "\n",
    "- In a previous version I had wanted $FP/TP$ to equal either 2.0, 1.0, or 0.5, indicating that we were willing to send 2 unnecessary ambulances for each necessary one, etc.  \n",
    "    - $FP/TP = 2.0$ corresponds to precision = 1/3\n",
    "    - $FP/TP = 1.0$ corresponds to precision = 1/2\n",
    "    - $FP/TP = 0.5$ corresponds to precision = 2/3\n",
    "\n",
    "- Neg/Pos corresponds to marginal precision similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26ec6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_Counts_y_proba\n",
      "430 332\n",
      "310\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def Value_Counts_y_proba(y_proba, filename):\n",
    "#    print ()\n",
    "    print ('Value_Counts_y_proba')\n",
    "#    print (type(y_proba))\n",
    "    Y_proba = pd.Series(y_proba)\n",
    "    A = Y_proba.value_counts().reset_index(drop=True)\n",
    "    n = len(y_proba)\n",
    "    nA = len(A)\n",
    "#    display(Y_proba)\n",
    "#    display(A)\n",
    "    B = A.cumsum()\n",
    "#    display(B)\n",
    "#    print (B[10])\n",
    "#    print ()\n",
    "    cutoff_95 = B.sub(0.95*n).abs().idxmin() + 1\n",
    "    cutoff_90 = B.sub(0.90*n).abs().idxmin() + 1\n",
    "    cutoff_80 = B.sub(0.80*n).abs().idxmin() + 1\n",
    "    m = Y_proba.min()\n",
    "    M = Y_proba.max()\n",
    "    print (n, nA)\n",
    "    print (cutoff_95)\n",
    "#    print ()\n",
    "\n",
    "    n100 = min(100, len(B)-1)\n",
    "    n200 = min(200, len(B)-1)\n",
    "#    print ('n200 = ', n200)\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'a')\n",
    "    f.write('%s,%d,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%0.4f,%0.4f\\n' % (\n",
    "        filename, n, nA, nA/n, \n",
    "        cutoff_95, cutoff_95/n,\n",
    "        cutoff_90, cutoff_90/n,\n",
    "        cutoff_80, cutoff_80/n,\n",
    "        B[10], B[10]/n,\n",
    "        B[20], B[20]/n,\n",
    "        B[n100], B[n100]/n,\n",
    "        B[n200], B[n200]/n,\n",
    "        m,M,\n",
    "    ))\n",
    "    f.close()\n",
    "    \n",
    "    H = Y_proba.value_counts().head(100)\n",
    "    Filename = './Analyze_Proba/' + filename + '_Value_Counts.csv'\n",
    "    H.to_csv(Filename)\n",
    "    \n",
    "    \n",
    "    print ('Finished')\n",
    "    return 0\n",
    "    \n",
    "def Create_Files_for_Value_Counts_y_proba():\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'w')\n",
    "    f.write(\"Filename,n,nUnique,nUnique/n,95%,95%/n,90%,90%/n,80%,80%/n,B[10],B[10]/n,B[20],B[20]/n,B[100],B[100]/n,B[200],B[200],min,max,\\n\")\n",
    "    f.close()\n",
    "    \n",
    "#Create_Files_for_Value_Counts_y_proba()\n",
    "\n",
    "def Create_Files_for_Lengths_of_fpr_tpr():\n",
    "    f = open('./Analyze_Proba/Lengths_of_fpr_tpr.csv', 'w')\n",
    "    f.write(\"Filename,len(y_proba),nUnique(y_proba),len(fpr),nUnique(fpr),len(tpr),nUnique(tpr),len(fpr tpr),nUnique(fpr tpr)\\n\")\n",
    "    f.close()\n",
    "    \n",
    "def Create_Files_for_ROC_AUC():\n",
    "    f = open('./Analyze_Proba/ROC_AUC.csv', 'w')\n",
    "    f.write(\"Filename,ROC_AUC\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def Test_Value_Counts_y_proba():\n",
    "    A = [5]*50 + [6]*20 + [i for i in range (10,40)]*2 + [i for i in range (100,400)]\n",
    "    Value_Counts_y_proba(A, 'Test')\n",
    "\n",
    "Test_Value_Counts_y_proba()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de839f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Analyze_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    Value_Counts_y_proba(y_proba, filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    print ('print (len(A), len(C), len(D), len(C) + len(D))')\n",
    "#    print (len(A), len(C), len(D), len(C) + len(D))\n",
    "\n",
    "    N = len(C)\n",
    "    P = len(D)\n",
    "    \n",
    "    ##### 10 bins\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$']=Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$']=Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_10.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_10.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrcrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ##### 20 bins\n",
    "    n = 20\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$']=Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$']=Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_20.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_20.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrcrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### 100 bins\n",
    "    n = 100\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'] =  Analyze['FP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    \n",
    "    A = Analyze.copy(deep=True)\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$']=Analyze['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$']=Analyze['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_100.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_100.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrcrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Append CSV files with results from multiple models\n",
    "    A.set_index('p', inplace=True)\n",
    "    A.insert(0, 'Filename', filename)\n",
    "    \n",
    "    # Remove rows with negligible number of samples\n",
    "    A = A[A['Neg'] >= 20]\n",
    "    A = A[A['Pos'] >= 20]\n",
    "    \n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_333.csv', mode='a', \n",
    "        index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_333.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_5.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/mProb_0_667.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_333.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_5.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_667.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\\\frac{\\\\text{FP}}{\\\\text{P}}$'] - 0.05).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/FP_P_0_05.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest['Filename'] = A_closest['Filename'].str.replace('_','\\\\_')\n",
    "    A_closest.to_csv('./Analyze_Proba/FP_P_0_05.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    \n",
    "    \n",
    "def Create_Files_for_Analyze_Prediction():\n",
    "    f = open('./Analyze_Proba/mPrec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/mPrec_0_5.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/mPrec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/mPrec_0_667.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/mPrec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/mPrec_0_333.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/Prec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/Prec_0_5.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/Prec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/Prec_0_667.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/Prec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/Prec_0_333.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/p_hat_0_05.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/p_hat_0_05.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/p_hat_0_10.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/p_hat_0_10.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/p_hat_0_15.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/p_hat_0_15.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "    \n",
    "    f = open('./Analyze_Proba/FP_P_0_05.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba/FP_P_0_05.tex', 'w')\n",
    "    g.write(\"p & Filename & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P\\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "\n",
    "\n",
    "#Create_Files_for_Analyze_Prediction()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f53a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prediction_2(y_test, y_proba, filename, title):\n",
    "    print ('Analyze_Prediction_2()')\n",
    "    print (filename)\n",
    "    \n",
    "    Value_Counts_y_proba(y_proba, filename)\n",
    "\n",
    "#    print ('y_proba: ',y_proba)\n",
    "#    print ('y_test: ', y_test)\n",
    "    A = pd.DataFrame(y_proba, columns=['y_proba'])\n",
    "    B = pd.DataFrame(y_test, columns=['y_test'])\n",
    "    C = pd.concat([A,B], axis=1)\n",
    "    \n",
    "    # Sort the y_proba values with p==1 at the top.\n",
    "    # Make a feature, 'custom_cut'\n",
    "    # Make a cut large enough that it has at least 1000 elements of each class.\n",
    "    # Do not cut between two y_proba of the same value; \n",
    "    #    keep going until you get to a different y_proba value.\n",
    "    # Label that cut \"0,\" and the next cut \"1,\" etc.\n",
    "    \n",
    "    C.sort_values(by=['y_proba'], ascending=False, inplace=True)\n",
    "    C = C.reset_index(drop=True)\n",
    "    C['custom_cut'] = 0\n",
    "    \n",
    "    n0 = 0\n",
    "    n1 = 0\n",
    "    j = 0\n",
    "    C['custom_cut'][0] = j\n",
    "    if C['y_test'][0]==0:\n",
    "        n0 += 1\n",
    "    else:\n",
    "        n1 += 1\n",
    "    for i in range (1,len(C)):\n",
    "#        if i%1000==0:\n",
    "#            print (i, j)\n",
    "        if (\n",
    "            min(n0,n1)>=1000 and \n",
    "            C['y_proba'][i] != C['y_proba'][i-1] \n",
    "        ):\n",
    "            n0 = 0\n",
    "            n1 = 0\n",
    "            j = j+1\n",
    "        if C['y_test'][i]==0:\n",
    "            n0 += 1\n",
    "        else:\n",
    "            n1 += 1\n",
    "        C['custom_cut'][i] = j\n",
    "    print (filename, ' has ', j, ' custom_cut intervals')\n",
    "#    print (C)\n",
    "    \n",
    "    # Count the positive and negative elements in each of the custom_cuts\n",
    "    \n",
    "    D = C[C['y_test']==0]\n",
    "    E = C[C['y_test']==1]\n",
    "\n",
    "    F = D['custom_cut'].value_counts(sort=False).rename(\"Neg\")\n",
    "    G = E['custom_cut'].value_counts(sort=False).rename(\"Pos\")\n",
    "    H = pd.concat([F,G], axis=1, names=['Neg','Pos'])\n",
    "\n",
    "    H['index1'] = H.index\n",
    "    H.sort_values(by=['index1'], ascending=False, inplace=True)\n",
    "    H = H.reset_index()\n",
    "    H['TN'] = H['Neg'].cumsum()\n",
    "    H['FP'] = len(D) - H['TN']\n",
    "    H['FN'] = H['Pos'].cumsum()\n",
    "    H['TP'] = len(E) - H['FN']\n",
    "    H['Prec'] = H['TP']/(H['FP'] + H['TP'])\n",
    "    H['Rec'] = H['TP']/(H['FN'] + H['TP'])\n",
    "    H['FP/P'] = H['FP']/(H['FN'] + H['TP'])\n",
    "    \n",
    "\n",
    "    H['min'] = 0\n",
    "    H['max'] = 0\n",
    "    \n",
    "    for i in range (len(H)):\n",
    "        I = C[C['custom_cut']==H['index1'][i]]\n",
    "        H['min'][i] = I['y_proba'].min()\n",
    "        H['max'][i] = I['y_proba'].max()\n",
    "    \n",
    "    H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] = H['Pos']/(H['Neg']+H['Pos'])\n",
    "    \n",
    "    H = H.drop('index1', axis='columns')\n",
    "    H = H.loc[:,['min','max','Neg','Pos','$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$','TN','FP','FN','TP','Prec','Rec','FP/P']]\n",
    "\n",
    "    P = H.copy(deep=True)\n",
    "    \n",
    "    H['min']=H['min'].apply('{:.4f}'.format)\n",
    "    H['max']=H['max'].apply('{:.4f}'.format)\n",
    "    H['Neg']=H['Neg'].apply('{:,}'.format)\n",
    "    H['Pos']=H['Pos'].apply('{:,}'.format)\n",
    "    H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$']=H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'].apply('{:.4f}'.format)\n",
    "    H['TN']=H['TN'].apply('{:,}'.format)\n",
    "    H['FP']=H['FP'].apply('{:,}'.format)\n",
    "    H['FN']=H['FN'].apply('{:,}'.format)\n",
    "    H['TP']=H['TP'].apply('{:,}'.format)\n",
    "    H['Prec']=H['Prec'].apply('{:.4f}'.format)\n",
    "    H['Rec']=H['Rec'].apply('{:.4f}'.format)\n",
    "    H['FP/P']=H['FP/P'].apply('{:.4f}'.format)\n",
    "    \n",
    "    H.to_csv('./Analyze_Proba_2/' + filename + '_Slices.csv', index=True)\n",
    "    H.to_latex(\n",
    "        './Analyze_Proba_2/' + filename + '_Slices.tex', \n",
    "        index=True, \n",
    "#        float_format=\"{:.4f}\".format, \n",
    "        column_format='rrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "#    print (H)\n",
    "    \n",
    "    # Append CSV files with results from multiple models\n",
    "    P.insert(0, 'Filename', filename)\n",
    "    \n",
    "    P_closest = P.iloc[(P['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_333.csv', mode='a', \n",
    "        index=True, header=False, float_format=\"{:.2f}\".format)\n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_333.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, \n",
    "#                     float_format=\"{:.4f}\".format\n",
    "                    )\n",
    "    \n",
    "    P_closest = P.iloc[(P['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_5.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    P_closest = P.iloc[(P['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/mProb_0_667.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    P_closest = P.iloc[(P['Prec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_333.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    P_closest = P.iloc[(P['Prec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_5.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    P_closest = P.iloc[(P['Prec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/Prec_0_667.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    P_closest = P.iloc[(P['FP/P'] - 0.05).abs().argsort()[:1]].head(1)\n",
    "    P_closest.to_csv('./Analyze_Proba_2/FP_P_0_05.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    P_closest['Filename'] = P_closest['Filename'].str.replace('_','\\\\_')\n",
    "    P_closest.to_csv('./Analyze_Proba_2/FP_P_0_05.tex', \n",
    "                     mode='a', sep='&', lineterminator='\\\\cr\\n',\n",
    "                    index=True, header=False, float_format=\"{:.4f}\".format)\n",
    "    \n",
    "    \n",
    "    \n",
    "def Analyze_Prediction_Custom_Cut(y_test, y_proba, filename, title, minCut):\n",
    "    print ('Analyze_Prediction_Custom_Cut ', minCut)\n",
    "    filename = filename + '_' + str(minCut)\n",
    "    print (filename)\n",
    "    \n",
    "#    print ('y_proba: ',y_proba)\n",
    "#    print ('y_test: ', y_test)\n",
    "    A = pd.DataFrame(y_proba, columns=['y_proba'])\n",
    "    B = pd.DataFrame(y_test, columns=['y_test'])\n",
    "    C = pd.concat([A,B], axis=1)\n",
    "    \n",
    "    # Sort the y_proba values with p==1 at the top.\n",
    "    # Make a feature, 'custom_cut'\n",
    "    # Make a cut large enough that it has at least 1000 elements of each class.\n",
    "    # Do not cut between two y_proba of the same value; \n",
    "    #    keep going until you get to a different y_proba value.\n",
    "    # Label that cut \"0,\" and the next cut \"1,\" etc.\n",
    "    \n",
    "    C.sort_values(by=['y_proba'], ascending=False, inplace=True)\n",
    "    C = C.reset_index(drop=True)\n",
    "    C['custom_cut'] = 0\n",
    "    \n",
    "    n0 = 0\n",
    "    n1 = 0\n",
    "    j = 0\n",
    "    C['custom_cut'][0] = j\n",
    "    if C['y_test'][0]==0:\n",
    "        n0 += 1\n",
    "    else:\n",
    "        n1 += 1\n",
    "    for i in range (1,len(C)):\n",
    "#        if i%1000==0:\n",
    "#            print (i, j)\n",
    "        if (\n",
    "            min(n0,n1)>=minCut and \n",
    "            C['y_proba'][i] != C['y_proba'][i-1] \n",
    "        ):\n",
    "            n0 = 0\n",
    "            n1 = 0\n",
    "            j = j+1\n",
    "        if C['y_test'][i]==0:\n",
    "            n0 += 1\n",
    "        else:\n",
    "            n1 += 1\n",
    "        C['custom_cut'][i] = j\n",
    "    print (filename, ' has ', j, ' custom_cut intervals')\n",
    "#    print (C)\n",
    "    \n",
    "    # Count the positive and negative elements in each of the custom_cuts\n",
    "    \n",
    "    D = C[C['y_test']==0]\n",
    "    E = C[C['y_test']==1]\n",
    "\n",
    "    F = D['custom_cut'].value_counts(sort=False).rename(\"Neg\")\n",
    "    G = E['custom_cut'].value_counts(sort=False).rename(\"Pos\")\n",
    "    H = pd.concat([F,G], axis=1, names=['Neg','Pos'])\n",
    "\n",
    "    H['index1'] = H.index\n",
    "    H.sort_values(by=['index1'], ascending=False, inplace=True)\n",
    "    H = H.reset_index()\n",
    "    H['TN'] = H['Neg'].cumsum()\n",
    "    H['FP'] = len(D) - H['TN']\n",
    "    H['FN'] = H['Pos'].cumsum()\n",
    "    H['TP'] = len(E) - H['FN']\n",
    "    H['Prec'] = H['TP']/(H['FP'] + H['TP'])\n",
    "    H['Rec'] = H['TP']/(H['FN'] + H['TP'])\n",
    "    H['FP/P'] = H['FP']/(H['FN'] + H['TP'])\n",
    "    \n",
    "\n",
    "    H['min'] = 0\n",
    "    H['max'] = 0\n",
    "    \n",
    "    for i in range (len(H)):\n",
    "        I = C[C['custom_cut']==H['index1'][i]]\n",
    "        H['min'][i] = I['y_proba'].min()\n",
    "        H['max'][i] = I['y_proba'].max()\n",
    "    \n",
    "    H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'] = H['Pos']/(H['Neg']+H['Pos'])\n",
    "    \n",
    "    H = H.drop('index1', axis='columns')\n",
    "    H = H.loc[:,['min','max','Neg','Pos','$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$','TN','FP','FN','TP','Prec','Rec','FP/P']]\n",
    "\n",
    "    P = H.copy(deep=True)\n",
    "    \n",
    "    H['min']=H['min'].apply('{:.4f}'.format)\n",
    "    H['max']=H['max'].apply('{:.4f}'.format)\n",
    "    H['Neg']=H['Neg'].apply('{:,}'.format)\n",
    "    H['Pos']=H['Pos'].apply('{:,}'.format)\n",
    "    H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$']=H['$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$'].apply('{:.4f}'.format)\n",
    "    H['TN']=H['TN'].apply('{:,}'.format)\n",
    "    H['FP']=H['FP'].apply('{:,}'.format)\n",
    "    H['FN']=H['FN'].apply('{:,}'.format)\n",
    "    H['TP']=H['TP'].apply('{:,}'.format)\n",
    "    H['Prec']=H['Prec'].apply('{:.4f}'.format)\n",
    "    H['Rec']=H['Rec'].apply('{:.4f}'.format)\n",
    "    H['FP/P']=H['FP/P'].apply('{:.4f}'.format)\n",
    "    \n",
    "    H.to_csv('./Analyze_Proba_2/' + filename + '_Slices.csv', index=True)\n",
    "    H.to_latex(\n",
    "        './Analyze_Proba_2/' + filename + '_Slices.tex', \n",
    "        index=True, \n",
    "#        float_format=\"{:.4f}\".format, \n",
    "        column_format='rrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "def Create_Files_for_Analyze_Prediction_2():\n",
    "    f = open('./Analyze_Proba_2/mProb_0_5.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/mProb_0_5.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/mProb_0_667.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/mProb_0_667.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/mProb_0_333.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/mProb_0_333.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/Prec_0_5.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/Prec_0_5.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/Prec_0_667.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/Prec_0_667.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/Prec_0_333.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/Prec_0_333.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "    f = open('./Analyze_Proba_2/FP_P_0_05.csv', 'w')\n",
    "    f.write(\"Index,Filename,min,max,Neg,Pos,$\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$,TN,FP,FN,TP,Prec,Rec,FP/P\\n\")\n",
    "    f.close()\n",
    "    g = open('./Analyze_Proba_2/FP_P_0_05.tex', 'w')\n",
    "    g.write(\"Index & Filename & min & max & Neg & Pos & $\\\\frac{\\\\text{Pos}}{\\\\text{Neg}+\\\\text{Pos}}$ & TN & FP & FN & TP & Prec & Rec & FP/P \\\\cr\\\\hline\\n\")\n",
    "    g.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e092785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Plot_Prediction_Zoom():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)  \n",
    "    \n",
    "    display(y_proba[:20])\n",
    "    display(y_pred[:20])\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.45, 0.55)\n",
    "    Analyze_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    \n",
    "#Test_Plot_Prediction_Zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a04113",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y_test, y_proba, p_values, filename):\n",
    "    print ('ROC()')\n",
    "    print (filename)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    A = np.asarray((fpr,tpr)).T\n",
    "#    print ()\n",
    "#    print ('len(fpr/tpr) = ', len(A))\n",
    "#    print (A[:10])\n",
    "    A = np.unique(A, axis=0)\n",
    "#    print (A[:10])\n",
    "    nU = len(A)\n",
    "#    print ('Unique fpr/tpr = ', nU)\n",
    "#    print ()\n",
    "    \n",
    "    f = open('./Analyze_Proba/Lengths_of_fpr_tpr.csv', 'a')\n",
    "    f.write('%s,' % (filename))\n",
    "    f.write('%d,' % len(y_proba))\n",
    "    f.write('%d,' % len(np.unique(y_proba)))\n",
    "    f.write('%d,' % len(fpr))\n",
    "    f.write('%d,' % len(np.unique(fpr)))\n",
    "    f.write('%d,' % len(tpr))\n",
    "    f.write('%d,' % len(np.unique(tpr)))\n",
    "    f.write('%d,' % len(np.asarray((fpr,tpr)).T))\n",
    "    f.write('%d,' % len(np.unique(np.asarray((fpr,tpr)).T, axis=0)))\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    print ('N_median, P_median = ', N_median, P_median)\n",
    "\n",
    "    m = np.quantile(y_proba,0.50)\n",
    "    p = np.quantile(y_proba,0.25)\n",
    "    q = np.quantile(y_proba,0.75)\n",
    "    \n",
    "    Y = []\n",
    "#    print ('p_values = ', p_values)\n",
    "    for X in p_values:\n",
    "        difference_array = np.absolute(thresholds-X)\n",
    "        index = difference_array.argmin()\n",
    "        F = fpr[index]\n",
    "        T = tpr[index]\n",
    "        Y.append([X,str(round(X,3)),F,T])\n",
    "    \n",
    "    auc_value = auc(fpr, tpr)\n",
    "    auc_value = round(auc_value,3)\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, color='black', label='AUC={:.3f}'.format(auc_value))\n",
    "    \n",
    "    for y in Y:\n",
    "#        plt.plot([y[2]], [y[3]], marker=\"o\", markersize=20, markeredgecolor=\"white\", markerfacecolor=\"white\")\n",
    "#        plt.annotate(\n",
    "#            y[1], # this is the text\n",
    "#            (y[2], y[3]), # these are the coordinates to position the label\n",
    "#            ha='center' # horizontal alignment can be left, right or center\n",
    "#        )\n",
    "        plt.text(\n",
    "            y[2], y[3], # these are the coordinates to position the label\n",
    "            y[1], # this is the text\n",
    "            backgroundcolor='white', # horizontal alignment can be left, right or center\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='square,pad=0.3')\n",
    "        )\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "#    plt.title('ROC with AUC {:.3f}'.format(auc_value))\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('./Images/' + filename + '_ROC.png', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.savefig('./Images/' + filename + '_ROC.pgf', bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    print ('./Images/' + filename + '_ROC.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "    return 0\n",
    "\n",
    "def Test_ROC():\n",
    "    y_test = [0,0,0,0,0,1]*10000\n",
    "#    y_proba = [abs(0.45 - y)+round(0.45*random.random(),2) for y in y_test]\n",
    "    y_proba = [abs(0.45 - y)+round(0.45*random.normalvariate(mu=0.2, sigma=0.2),3) for y in y_test]\n",
    "#    random.normalvariate(mu=0.0, sigma=1.0)\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    print (y_test)\n",
    "    print (y_proba)\n",
    "    ROC(y_test, y_proba, [0.5], \"tmp\")\n",
    "    \n",
    "#Test_ROC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49200a4f",
   "metadata": {},
   "source": [
    "# Five-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "536ee811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Five_Fold_Cross_Validation(data, model, filename, title):\n",
    "    print ()\n",
    "    print ('------------------------')\n",
    "    print ()\n",
    "    print (filename)\n",
    "    print ()\n",
    "    \n",
    "    target = 'HOSPITAL'\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = random.randint(1,100))\n",
    "    target_column = data.loc[:,target]\n",
    "    y_test = []\n",
    "    y_proba = []\n",
    "    y_pred = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in skf.split(data, target_column):\n",
    "        print ('K-fold iteration = ', iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "#        print ('len(train_index) = ', len(train_index))\n",
    "#        print (train_index)\n",
    "#        print ('len(test_index) = ', len(test_index))\n",
    "#        print (test_index)\n",
    "        \n",
    "        train_fold = data.iloc[train_index]\n",
    "#        print ()\n",
    "#        print ('train_fold')\n",
    "#        display(train_fold)\n",
    "        \n",
    "        test_fold = data.iloc[test_index]\n",
    "#        print ()\n",
    "#        print ('test_fold')\n",
    "#        display(test_fold)\n",
    "#        print ('type(test_fold) = ', type(test_fold))\n",
    "        \n",
    "        \n",
    "        X_train_fold = train_fold.drop(columns=[target])\n",
    "        X_test_fold = test_fold.drop(columns=[target])\n",
    "        y_train_fold = train_fold[target].squeeze()        \n",
    "        y_test_fold = test_fold[target].squeeze()\n",
    "#        print ('type(y_test_fold) = ', type(y_test_fold))\n",
    "        \n",
    "#        print ()\n",
    "        model.fit(X_train_fold, y_train_fold.values.ravel())\n",
    "        y_proba_fold = model.predict_proba(X_test_fold)\n",
    "        y_proba_fold = [x[1] for x in y_proba_fold]\n",
    "        y_pred_fold = list(np.around(np.array(y_proba_fold),0))\n",
    "        \n",
    "        ###\n",
    "#        print ('X_train_fold')\n",
    "#        display(X_train_fold)\n",
    "#        print ('y_train_fold')\n",
    "#        display(y_train_fold)\n",
    "#        print ('y_train_fold.value_counts()')\n",
    "#        display(y_train_fold.value_counts())\n",
    "#        print ('y_proba_fold')\n",
    "#        print (y_proba_fold)\n",
    "#        ###\n",
    "#        \n",
    "        y_test = y_test + y_test_fold.to_list()\n",
    "        y_proba = y_proba + y_proba_fold\n",
    "#        print ('len(y_proba) = ', len(y_proba))\n",
    "        y_pred = y_pred + y_pred_fold\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    DF = pd.DataFrame(y_test, columns=['y_test'])\n",
    "    DF['y_proba'] = y_proba\n",
    "    DF['y_pred'] = y_pred\n",
    "    DF.to_csv('../../Big_Files/' + filename + '.csv')\n",
    "#    print (DF)\n",
    "    \n",
    "    \n",
    "#    Chart_and_Plots(y_test, y_proba, y_pred, filename, title)\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "#    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2658775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BRFC_5_Fold(data, target, alpha, filename):\n",
    "     \n",
    "    title = ''\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        bootstrap = True, ccp_alpha = 0.0, criterion = 'gini', \n",
    "        max_depth = None,\n",
    "#        max_depth = 40, \n",
    "        max_features = 'sqrt', \n",
    "        max_leaf_nodes = None,\n",
    "#        max_leaf_nodes = 10000,  \n",
    "        max_samples = None, \n",
    "        min_impurity_decrease = 0.0, \n",
    "        min_samples_leaf = 1, \n",
    "        min_samples_split = 2, \n",
    "        min_weight_fraction_leaf = 0.0, \n",
    "        n_estimators = 100, \n",
    "#        n_estimators = 1000, \n",
    "        n_jobs = None, \n",
    "        oob_score = False, \n",
    "        random_state = random.randint(1,100), \n",
    "        replacement = False, \n",
    "        sampling_strategy = 'auto', \n",
    "        verbose = 0, \n",
    "        warm_start = False,\n",
    "        class_weight = {0:1-alpha, 1:alpha}\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae040db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    model = RandomForestClassifier(max_depth=2, random_state = random.randint(1,100))\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac53d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    model = AdaBoostClassifier(n_estimators=100, random_state = random.randint(1,100))\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "855780c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUSBoost_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    estimator = DecisionTreeClassifier(\n",
    "        max_depth=1,\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )    \n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=1000, \n",
    "        estimator=estimator,\n",
    "        algorithm='SAMME.R', \n",
    "        random_state = random.randint(1,100)\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f8dac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BalancedBagging_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    model = BalancedBaggingClassifier(\n",
    "        random_state = random.randint(1,100)\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff286004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EasyEnsemble_5_Fold(data, target, filename):\n",
    "    title = ''\n",
    "    estimator = AdaBoostClassifier(n_estimators=10, random_state = random.randint(1,100))\n",
    "    model = EasyEnsembleClassifier(n_estimators=10, estimator=estimator, random_state = random.randint(1,100))\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a00e6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_5_Fold(data, target, alpha, filename):\n",
    "    title = ''\n",
    "    model = LogisticRegression(\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "        class_weight = {0:1-alpha, 1:alpha},\n",
    "        max_iter=1000,\n",
    "        random_state = random.randint(1,100),\n",
    "    )\n",
    "    Five_Fold_Cross_Validation(data, model, filename, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f39874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KBFC_5_Fold(data, target, alpha, gamma, filename):\n",
    "    print ()\n",
    "    print ('------------------------')\n",
    "    print ()\n",
    "    print (filename)\n",
    "    print ()\n",
    "    \n",
    "    target = 'HOSPITAL'\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state  = random.randint(1,100))\n",
    "    target_column = data.loc[:,target]\n",
    "    y_test = []\n",
    "    y_proba = []\n",
    "    y_pred = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in skf.split(data, target_column):\n",
    "        print ()\n",
    "        print ()\n",
    "        print ('K-fold iteration = ', iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "#        print ('len(train_index) = ', len(train_index))\n",
    "#        print (train_index)\n",
    "#        print ('len(test_index) = ', len(test_index))\n",
    "#        print (test_index)\n",
    "        \n",
    "        train_fold = data.iloc[train_index]\n",
    "#        print ()\n",
    "#        print ('train_fold')\n",
    "#        display(train_fold)\n",
    "        \n",
    "        test_fold = data.iloc[test_index]\n",
    "#        print ()\n",
    "#        print ('test_fold')\n",
    "#        display(test_fold)\n",
    "#        print ('type(test_fold) = ', type(test_fold))\n",
    "        \n",
    "        \n",
    "        X_train_fold = train_fold.drop(columns=[target])\n",
    "        X_test_fold = test_fold.drop(columns=[target])\n",
    "        y_train_fold = train_fold[target].squeeze()        \n",
    "        y_test_fold = test_fold[target].squeeze()\n",
    "#        print ('type(y_test_fold) = ', type(y_test_fold))\n",
    "\n",
    "#        print ('len(X_train_fold) = ', len(X_train_fold))\n",
    "#        print ('len(X_test_fold) = ', len(X_test_fold))\n",
    "#        print ('len(y_train_fold) = ', len(y_train_fold))\n",
    "#        print ('len(y_test_fold) = ', len(y_test_fold))\n",
    "#        print ()\n",
    "        \n",
    "#        print ()\n",
    "\n",
    "        loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "            apply_class_balancing=True,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "    #        from_logits=False,\n",
    "    #        label_smoothing=0.0,\n",
    "    #        axis=-1,\n",
    "    #        reduction=losses_utils.ReductionV2.AUTO,\n",
    "    #        name='binary_focal_crossentropy'\n",
    "        )   \n",
    "    \n",
    "        # create model\n",
    "        model = Sequential()\n",
    "#        print ('data.shape = ', data.shape, data.shape[-1])\n",
    "        model.add(Dense(60, input_shape=(data.shape[-1]-1,), activation='relu'))\n",
    "#        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))    \n",
    "        # Compile model\n",
    "        metrics = [\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\"),\n",
    "    #        F1_Metric,\n",
    "        ]\n",
    "        model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "        estimator = KerasClassifier(\n",
    "            model=model, \n",
    "            random_state = random.randint(1,100),\n",
    "            metrics=metrics,\n",
    "            batch_size=128, \n",
    "            verbose=0,\n",
    "            epochs=20,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "        estimator.fit(X_train_fold, y_train_fold.values.ravel())\n",
    "        y_proba_fold = estimator.predict_proba(X_test_fold)\n",
    "        y_proba_fold = [x[1] for x in y_proba_fold]\n",
    "        y_pred_fold = list(np.around(np.array(y_proba_fold),0))\n",
    "        \n",
    "        ###\n",
    "#        print ('X_train_fold')\n",
    "#        display(X_train_fold.head())\n",
    "#        print ('y_train_fold')\n",
    "#        display(y_train_fold.head())\n",
    "#        print ('y_train_fold.value_counts()')\n",
    "#        display(y_train_fold.value_counts())\n",
    "#        print ('X_test_fold')\n",
    "#        display(X_test_fold.head())\n",
    "#        print ('y_test_fold')\n",
    "#        display(y_test_fold.head())\n",
    "#        print ('y_test_fold.value_counts()')\n",
    "#        display(y_test_fold.value_counts())\n",
    "#        print ('y_proba_fold')\n",
    "#        print (y_proba_fold[:10])\n",
    "#        ###\n",
    "#        \n",
    "        y_test = y_test + y_test_fold.to_list()\n",
    "        y_proba = y_proba + y_proba_fold\n",
    "#        print ('len(y_proba) = ', len(y_proba))\n",
    "        y_pred = y_pred + y_pred_fold\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    DF = pd.DataFrame(y_test, columns=['y_test'])\n",
    "    DF['y_proba'] = y_proba\n",
    "    DF['y_pred'] = y_pred\n",
    "    DF.to_csv('../../Big_Files/' + filename + '.csv')\n",
    "    \n",
    "#    Chart_and_Plots(y_test, y_proba, y_pred, filename, '')\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "#    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52566066",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72178501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Run_with_Hard_Features(run):\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Hard_Features(data)\n",
    "    write_filename_features = '_Hard' + run\n",
    "    data = Get_Dummies(data, target)\n",
    "    \n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    filename = 'RFC' + write_filename_features\n",
    "    RFC_5_Fold(data, target, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 1.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_1_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 2.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_2_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eafbf6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_with_Medium_Features(run):\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Medium_Features(data)\n",
    "    write_filename_features = '_Medium' + run\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    filename = 'RFC' + write_filename_features\n",
    "    RFC_5_Fold(data, target, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 1.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_1_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 2.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_2_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1213be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_with_Easy_Features(run):\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Easy_Features(data)\n",
    "    write_filename_features = '_Easy' + run\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    y = data[target]\n",
    "    N = len(y)\n",
    "    n = len(y[y==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)    \n",
    "\n",
    "    filename = 'RFC' + write_filename_features\n",
    "    RFC_5_Fold(data, target, filename)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    filename = 'BRFC_alpha_0_5' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "    alpha = alpha_balanced\n",
    "    filename = 'BRFC_alpha_balanced' + write_filename_features\n",
    "    BRFC_5_Fold(data, target, alpha, filename)\n",
    "    \n",
    "\n",
    "    alpha = 0.5\n",
    "    filename = 'LogReg_alpha_0_5' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    filename = 'LogReg_alpha_balanced' + write_filename_features\n",
    "    LogisticRegression_5_Fold(data, target, alpha, filename)\n",
    "\n",
    "    AdaBoost_5_Fold(data, target, 'AdaBoost' + write_filename_features)\n",
    "    BalancedBagging_5_Fold(data, target, 'BalBag' + write_filename_features)\n",
    "    EasyEnsemble_5_Fold(data, target, 'EEC' + write_filename_features)\n",
    "    RUSBoost_5_Fold(data, target, 'RUSBoost' + write_filename_features)\n",
    "    \n",
    "    alpha = 0.5\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = alpha_balanced\n",
    "    gamma = 0.0\n",
    "    filename = 'KBFC_alpha_balanced_gamma_0_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 1.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_1_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "\n",
    "    alpha = 0.5\n",
    "    gamma = 2.0\n",
    "    filename = 'KBFC_alpha_0_5_gamma_2_0' + write_filename_features\n",
    "    KBFC_5_Fold(data, target, alpha, gamma, filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6e23c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data()\n",
      "data.shape:  (713566, 78)\n",
      "End Get_Data()\n",
      "\n",
      "Feature_Engineering_Cross_Two\n",
      "\n",
      "Thin_to_Hard_Features()\n",
      "data.shape:  (713566, 40)\n",
      "End Thin_to_Hard_Features()\n",
      "\n",
      "Get_Dummies\n",
      "\n",
      "p =  5.609785468153692\n",
      "alpha_balanced =  0.84870915934896\n",
      "\n",
      "------------------------\n",
      "\n",
      "RFC_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_0_5_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_balanced_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_0_5_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_balanced_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "AdaBoost_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BalBag_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "EEC_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "RUSBoost_Hard_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_0_0_Hard_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 14:36:45.677374: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_balanced_gamma_0_0_Hard_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_1_0_Hard_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_2_0_Hard_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "CPU times: user 4h 45min 26s, sys: 6min 39s, total: 4h 52min 5s\n",
      "Wall time: 4h 30min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Run_with_Hard_Features('_Run_1')\n",
    "# CPU times: user 4h 45min 20s, sys: 7min 22s, total: 4h 52min 43s\n",
    "# Wall time: 4h 30min 47s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebb76860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data()\n",
      "data.shape:  (713566, 78)\n",
      "End Get_Data()\n",
      "\n",
      "Feature_Engineering_Cross_Two\n",
      "\n",
      "Thin_to_Medium_Features()\n",
      "data.shape:  (713566, 22)\n",
      "End Thin_to_Medium_Features()\n",
      "\n",
      "Get_Dummies\n",
      "\n",
      "p =  5.609785468153692\n",
      "alpha_balanced =  0.84870915934896\n",
      "\n",
      "------------------------\n",
      "\n",
      "RFC_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_0_5_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_balanced_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_0_5_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_balanced_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "AdaBoost_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BalBag_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "EEC_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "RUSBoost_Medium_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_0_0_Medium_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_balanced_gamma_0_0_Medium_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_1_0_Medium_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_2_0_Medium_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "CPU times: user 4h 11min 6s, sys: 4min 4s, total: 4h 15min 10s\n",
      "Wall time: 4h 10min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Run_with_Medium_Features('_Run_1')\n",
    "# CPU times: user 2h 21min 16s, sys: 3min 36s, total: 2h 24min 52s\n",
    "# Wall time: 2h 19min 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6534ca51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_Data()\n",
      "data.shape:  (713566, 78)\n",
      "End Get_Data()\n",
      "\n",
      "Feature_Engineering_Cross_Two\n",
      "\n",
      "Thin_to_Easy_Features()\n",
      "data.shape:  (713566, 10)\n",
      "End Thin_to_Easy_Features()\n",
      "\n",
      "Get_Dummies\n",
      "\n",
      "p =  5.609785468153692\n",
      "alpha_balanced =  0.84870915934896\n",
      "\n",
      "------------------------\n",
      "\n",
      "RFC_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_0_5_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BRFC_alpha_balanced_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_0_5_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "LogReg_alpha_balanced_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "AdaBoost_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "BalBag_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "EEC_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "RUSBoost_Easy_Run_1\n",
      "\n",
      "K-fold iteration =  0\n",
      "K-fold iteration =  1\n",
      "K-fold iteration =  2\n",
      "K-fold iteration =  3\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_0_0_Easy_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_balanced_gamma_0_0_Easy_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_1_0_Easy_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "KBFC_alpha_0_5_gamma_2_0_Easy_Run_1\n",
      "\n",
      "\n",
      "\n",
      "K-fold iteration =  0\n",
      "\n",
      "\n",
      "K-fold iteration =  1\n",
      "\n",
      "\n",
      "K-fold iteration =  2\n",
      "\n",
      "\n",
      "K-fold iteration =  3\n",
      "\n",
      "\n",
      "K-fold iteration =  4\n",
      "\n",
      "CPU times: user 3h 43min 35s, sys: 1min 40s, total: 3h 45min 16s\n",
      "Wall time: 3h 44min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Run_with_Easy_Features('_Run_1')\n",
    "# CPU times: user 2h 2min 53s, sys: 1min 36s, total: 2h 4min 29s\n",
    "# Wall time: 2h 3min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb852de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
