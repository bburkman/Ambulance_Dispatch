{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767154e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\tableofcontents\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1d2b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7d47d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b4fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install Packages\n",
      "Python version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "NumPy version: 1.24.2\n",
      "SciPy version:  1.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bburkman/miniforge3/envs/Tensorflow_2_11/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n",
      "Keras version:  2.11.0\n",
      "Pandas version:  1.5.3\n",
      "SciKit-Learn version: 1.2.2\n",
      "Imbalanced-Learn version: 0.10.1\n",
      "Finished Installing Packages\n"
     ]
    }
   ],
   "source": [
    "print ('Install Packages')\n",
    "\n",
    "import sys, copy, math, time, os\n",
    "\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "print ('NumPy version: {}'.format(np.__version__))\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import scipy as sc\n",
    "print ('SciPy version:  {}'.format(sc.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print ('TensorFlow version:  {}'.format(tf.__version__))\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from tensorflow import keras\n",
    "print ('Keras version:  {}'.format(keras.__version__))\n",
    "\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import StringLookup\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "print ('Pandas version:  {}'.format(pd.__version__))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Library for reading Microsoft Access files\n",
    "#import pandas_access as mdb\n",
    "\n",
    "import sklearn\n",
    "print ('SciKit-Learn version: {}'.format(sklearn.__version__))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "print ('Imbalanced-Learn version: {}'.format(imblearn.__version__))\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "#!pip install pydot\n",
    "\n",
    "# Set Randomness.  Copied from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments\n",
    "import random\n",
    "#np.random.seed(42) # NumPy\n",
    "#random.seed(42) # Python\n",
    "#tf.random.set_seed(42) # Tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print ('Finished Installing Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437f109",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919fb2db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Get_Data():\n",
    "    print ('Get_Data()')\n",
    "    data = pd.read_csv(\n",
    "        '../../Big_Files/CRSS_Imputed_All_05_19_23.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Get_Data()')\n",
    "    print ()\n",
    "    return data\n",
    "\n",
    "def Test_Get_Data():\n",
    "    data = Get_Data()\n",
    "    display (data.head())\n",
    "    \n",
    "#Test_Get_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d540640",
   "metadata": {},
   "source": [
    "# Remove_Pedestrian_Crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac62df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Pedestrian_Crashes(data):\n",
    "    print ('Remove_Pedestrian_Crashes()')\n",
    "    display(data.PEDS.value_counts())\n",
    "    n = len(data[data.PEDS>0])\n",
    "    print ('Removing %d crashes that involve a pedestrian.' % n)\n",
    "    data = data[data.PEDS==0]\n",
    "    return data\n",
    "\n",
    "def Test_Remove_Pedestrian_Crashes():\n",
    "    data = Get_Data()\n",
    "    print (len(data))\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    print (len(data))\n",
    "\n",
    "#Test_Remove_Pedestrian_Crashes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb9dce",
   "metadata": {},
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85fa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering_Cross_Two(data):\n",
    "    print ('Feature_Engineering_Cross_Two')\n",
    "    Pairs = [\n",
    "        ['AGE', 'SEX', 'AGE_x_SEX'],\n",
    "        ['AGE', 'SCH_BUS', 'AGE_x_SCH_BUS']\n",
    "    ]\n",
    "    for P in Pairs:\n",
    "        data[P[2]] = data[P[0]].map(str) + '_x_' + data[P[1]].map(str)\n",
    "    \n",
    "    print ()\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900000",
   "metadata": {},
   "source": [
    "## Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_Features(data):\n",
    "    print ('Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "        'PERMVIT',\n",
    "        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "        'RELJCT2',\n",
    "        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "        'VE_FORMS',\n",
    "        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "        'BODY_TYP',\n",
    "        'BUS_USE',\n",
    "        'EMER_USE',\n",
    "        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "        'MODEL',\n",
    "        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "        'LOCATION',\n",
    "        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e759eda",
   "metadata": {},
   "source": [
    "## Really Thin Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4202cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Really_Thin_Features(data):\n",
    "    print ('Really_Thin_Features()')\n",
    "\n",
    "    Merge = [\n",
    "        'CASENUM',\n",
    "        'VEH_NO',\n",
    "        'PER_NO',        \n",
    "    ]\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "        'REL_ROAD',\n",
    "        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "        'VALIGN',\n",
    "        'VNUM_LAN',\n",
    "        'VPROFILE',\n",
    "        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "        'VTRAFCON',\n",
    "        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Really_Thin_Features():\n",
    "    data = Get_Data()\n",
    "    data = Really_Thin_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Really_Thin_Features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11264ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thin_to_Minimal_Features(data):\n",
    "    print ('Thin_to_Minimal_Features()')\n",
    "\n",
    "    Accident = [\n",
    "        'DAY_WEEK',\n",
    "        'HOUR',\n",
    "#        'INT_HWY',\n",
    "#        'LGT_COND',\n",
    "        'MONTH',\n",
    "#        'PEDS',\n",
    "#        'PERMVIT',\n",
    "#        'PERNOTMVIT',\n",
    "        'PJ',\n",
    "        'PSU',\n",
    "#        'PVH_INVL',\n",
    "        'REGION',\n",
    "#        'REL_ROAD',\n",
    "#        'RELJCT1',\n",
    "#        'RELJCT2',\n",
    "#        'SCH_BUS',\n",
    "#        'TYP_INT',\n",
    "        'URBANICITY',\n",
    "#        'VE_FORMS',\n",
    "#        'VE_TOTAL',\n",
    "        'WEATHER',\n",
    "#        'WRK_ZONE',\n",
    "        'YEAR',\n",
    "    ]\n",
    "    \n",
    "    Vehicle = [\n",
    "#        'BODY_TYP',\n",
    "#        'BUS_USE',\n",
    "#        'EMER_USE',\n",
    "#        'MAKE',\n",
    "#        'MOD_YEAR',\n",
    "#        'MODEL',\n",
    "#        'NUMOCCS',\n",
    "#        'VALIGN',\n",
    "#        'VNUM_LAN',\n",
    "#        'VPROFILE',\n",
    "#        'VSPD_LIM',\n",
    "#        'VSURCOND',\n",
    "#        'VTRAFCON',\n",
    "#        'VTRAFWAY',\n",
    "    ]\n",
    "    \n",
    "    Person = [\n",
    "#        'AGE',\n",
    "#        'LOCATION',\n",
    "#        'PER_TYP',\n",
    "#        'SEX',\n",
    "        'HOSPITAL',    \n",
    "    ]\n",
    "\n",
    "    Engineered = [\n",
    "#        'VEH_AGE',\n",
    "#        'AGE_x_SEX',\n",
    "#        'AGE_x_SCH_BUS'\n",
    "    ]\n",
    "    \n",
    "    # Put features in alphabetical order\n",
    "    Features = Accident + Vehicle + Person + Engineered\n",
    "    Features = sorted(Features)\n",
    "#    Features = Merge + Features\n",
    "    \n",
    "    data = data.filter(Features, axis=1)\n",
    "    \n",
    "    print ('data.shape: ', data.shape)\n",
    "    \n",
    "    print ('End Thin_Features()')\n",
    "    print ()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def Test_Thin_to_Minimal_Features():\n",
    "    data = Get_Data()\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    for feature in data:\n",
    "        display(data[feature].value_counts())\n",
    "        \n",
    "#Test_Thin_to_Minimal_Features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52366e1a",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Dummies(data, target):\n",
    "    print ('Get_Dummies')\n",
    "    data = data.astype('category')\n",
    "    Target = data.pop(target)\n",
    "    data_Dummies = pd.get_dummies(data, prefix = data.columns)\n",
    "    data_Dummies = data_Dummies.join(Target)\n",
    "#    for feature in data_Dummies:\n",
    "#        print (feature)\n",
    "    print ()\n",
    "\n",
    "    return data_Dummies\n",
    "\n",
    "def Test_Get_Dummies():\n",
    "    print ('Test_Get_Dummies')\n",
    "    A = pd.DataFrame({\n",
    "        'A': ['a', 'b', 'a'], \n",
    "        'B': ['b', 'a', 'c'], \n",
    "        'C': [1, 2, 3]})\n",
    "    C = Get_Dummies(A, 'C')\n",
    "    display(C)\n",
    "    print ()\n",
    "\n",
    "#Test_Get_Dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d43d6e",
   "metadata": {},
   "source": [
    "## Test-Train Split\n",
    "- We're using sklearn's train_test_split rather than Pandas's sample because the former has a 'stratify' option that will put the same proportion of HOSPITAL==1 into each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Data(data, target, test_size):\n",
    "    print ('Split_Data()')\n",
    "    X = data.drop(columns=[target])\n",
    "    y = data[target]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, \n",
    "        #random_state=42\n",
    "    )\n",
    "    \n",
    "    a = y_train[y_train==1].shape[0]\n",
    "    b = y_test[y_test==1].shape[0]\n",
    "    print (\n",
    "        x_train.shape, \n",
    "        y_train.shape, a, round((a/(a+b)*100),2), '%')\n",
    "    print (\n",
    "        x_test.shape, \n",
    "        y_test.shape, b, round((b/(a+b)*100),2), '%'\n",
    "    )\n",
    "    print ()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b22f0",
   "metadata": {},
   "source": [
    "# Imbalanced Data Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984f24a",
   "metadata": {},
   "source": [
    "## Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a61c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tomek_Links(X_train, y_train):\n",
    "    print ('Tomek_Links()')\n",
    "    M = len(y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('Before Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ()\n",
    "\n",
    "    X_train, y_train = TomekLinks().fit_resample(X_train, y_train)\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    print ('After Tomek Links:')\n",
    "    print ('%d samples, %d hospitalized, %d not hospitalized' % (N, n, N-n))\n",
    "    print ('%f percent of samples hospitalized' % (n/N*100))\n",
    "    print ('There are %f negative samples for each positive.' % ((N-n)/n))\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (M-N, (M-N)/M*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce9a6e",
   "metadata": {},
   "source": [
    "## Condensed Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cfb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Condensed_Nearest_Neighbour(X_train, y_train):\n",
    "    print ('Condensed_Nearest_Neighbour()')\n",
    "    N = X_train.shape[0]\n",
    "    print ('X_train.shape before = ', X_train.shape)\n",
    "    print ('y_train.shape before = ', y_train.shape)\n",
    "    print ()\n",
    "    cnn = CondensedNearestNeighbour(n_neighbors=None)\n",
    "    X_train, y_train = cnn.fit_resample(X_train, y_train)\n",
    "    n = X_train.shape[0]\n",
    "    print ('X_train.shape after = ', X_train.shape)\n",
    "    print ('y_train.shape after = ', y_train.shape)\n",
    "    print ()\n",
    "    print ('Removed %d samples, or %.2f%% of the set.' % (N-n, (N-n)/N*100))\n",
    "    print ()\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdd5a1",
   "metadata": {},
   "source": [
    "# Undersample Data\n",
    "- These functions take the three versions of the dataset, which correspond to these names in the paper:\n",
    "    - Thin (Hard)\n",
    "    - Really_Thin (Medium)\n",
    "    - Thin_to_Minimum (Easy)\n",
    "- runs Tomek Links on them once, then again, and saves the results to file.\n",
    "- Each of the three sets takes about 90 minutes to run on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8628b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def Undersample_Data_Thin(round_text):\n",
    "    print ('Undersample_Data_Thin()')\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 samples before Tomek\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 399,515 samples after Tomek, v1\n",
    "    # 399,714  v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    \n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # 396,511 after Tomek twice v1\n",
    "    # 396,718 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Really_Thin(round_text):\n",
    "    print ('Undersample_Data_Really_Thin()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Really_Thin_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "\n",
    "    # 413,913 Samples\n",
    "\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    \n",
    "    # 406,691 Samples v1\n",
    "    # 406,781 v2\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek' + round_text + '.csv', index=False)\n",
    "    \n",
    "    # 405,288 Samples v1\n",
    "    # 405,368 v2\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Really_Thin_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "def Undersample_Data_Thin_to_Minimal(round_text):\n",
    "    print ('Undersample_Data_Thin_to_Minimal()')\n",
    "\n",
    "    data = Get_Data()\n",
    "    data = data.astype('int64')\n",
    "    target = 'HOSPITAL'\n",
    "    data = Remove_Pedestrian_Crashes(data)\n",
    "    data = Feature_Engineering_Cross_Two(data)\n",
    "    data = Thin_to_Minimal_Features(data)\n",
    "    data = Get_Dummies(data, target)\n",
    "\n",
    "    # Decrease set size, for debugging\n",
    "#    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.80)\n",
    "#    data = X_train\n",
    "#    data[target] = y_train\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Split_Data(data, target, 0.30)\n",
    "\n",
    "    # CNN took 6 minutes at 99% decreased set size.  \n",
    "#    X_train, y_train = Condensed_Nearest_Neighbour(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_before_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    # Two rounds of Tomek took one hour 30 minutes\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    # Write to csv and read back in, \n",
    "    #    so we can play with the stuff later without having to redo the Tomek Links, \n",
    "    #    which can take a long time.\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek' + round_text + '.csv', index=False)\n",
    "\n",
    "\n",
    "    X_train, y_train = Tomek_Links(X_train, y_train)\n",
    "    X_train.to_csv('../../Big_Files/X_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_train.to_csv('../../Big_Files/y_train_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    X_test.to_csv('../../Big_Files/X_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    y_test.to_csv('../../Big_Files/y_test_Thin_to_Minimal_after_Tomek_Twice' + round_text + '.csv', index=False)\n",
    "    print ()\n",
    "    \n",
    "#Undersample_Data_Thin('_v1')\n",
    "#Undersample_Data_Really_Thin('_v1')\n",
    "#Undersample_Data_Thin_to_Minimal('_v1')\n",
    "\n",
    "#Undersample_Data_Thin('_v2')\n",
    "#Undersample_Data_Really_Thin('_v2')\n",
    "#Undersample_Data_Thin_to_Minimal('_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577bab5",
   "metadata": {},
   "source": [
    "## Undersampling Results\n",
    "- Start with 747,342 samples\n",
    "- Remove 33,776 samples iwth pedestrians to get 713,566 samples\n",
    "- Split 70/30 to have 499,496 samples in training set, 214,070 in test set\n",
    "- In training set, 499,496 samples, 78,926 hospitalized, 420,570 not hospitalized\n",
    "\n",
    "\n",
    "| Feature Set | Random Seed | Tomek Round | # Samples Removed | % Samples Removed |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Hard | 1 | 1 | 17,851 | 3.57 |\n",
    "| Hard | 2 | 1 | 7,794 | 3.56 |\n",
    "| Hard | 1 | 2 | 3,664 | 0.76 |\n",
    "| Hard | 2 | 2 | 3,751 | 0.78 |\n",
    "| Medium | 1 | 1 | 8,839 | 1.77 |\n",
    "| Medium | 2 | 1 | 8.825 | 1.77 |\n",
    "| Medium | 1 | 2 | 1,736 | 0.35 |\n",
    "| Medium | 2 | 2 | 1,656 | 0.34 |\n",
    "| Easy | 1 | 1 | 6 | 0.00 |\n",
    "| Easy | 2 | 1 | 3 | 0.00 |\n",
    "| Easy | 1 | 2 | 0 | 0.00 |\n",
    "| Easy | 2 | 2 | 0 | 0.00 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dcfd4d",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf4a61",
   "metadata": {},
   "source": [
    "## Alpha Weighted Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a639e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5):\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51fa3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0):\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    def alpha_weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "        weights = tf.where(tf.equal(y_true,1),weight_1, weight_0)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        loss = keras.backend.mean(product)\n",
    "        return loss\n",
    "    return alpha_weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dedab",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f61cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    # The dataset has  259077  elements.\n",
    "    # The target group has  31891  elements.\n",
    "    # Our target is  12.3095 % of the dataset.\n",
    "    # There are  8.12  negative elements for each positive.    \n",
    "#    p = 8.12\n",
    "    p = 5.94\n",
    "\n",
    "    alpha = (p/(p+1))*1.0\n",
    "\n",
    "    gamma_1 = 0.0 # Must be float for the tf.math.pow() function to work.\n",
    "    gamma_2 = 0.0\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "#    print (binary_crossentropy.numpy())\n",
    "    weights = tf.where(tf.equal(y_true,1),alpha, 1-alpha)\n",
    "#    print (weights.numpy())\n",
    "    focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "    power = tf.where(tf.equal(y_true,1), gamma_1, gamma_2)\n",
    "    focal_power = tf.math.pow(focal,power)\n",
    "#    print (focal.numpy())\n",
    "#    print (power.numpy())\n",
    "#    print (focal_power.numpy())\n",
    "    product = tf.multiply(binary_crossentropy, weights)\n",
    "    focal_power_product = tf.multiply(product, focal_power)\n",
    "#    print (focal_power_product.numpy())\n",
    "    loss = keras.backend.mean(focal_power_product)\n",
    "#    print (loss.numpy())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef565",
   "metadata": {},
   "source": [
    "## Focal Loss with Parameters\n",
    "- Adapted from https://www.kaggle.com/code/abazdyrev/keras-nn-focal-loss-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48a20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters(alpha = 0.5, gamma_0=0.0, gamma_1=0.0):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "#        tf.clip_by_value(y_pred, 0.00001, 0.99999) # Make sure we don't blow up the logarithm\n",
    "        binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.where(tf.equal(y_true,1),alpha, 1.0-alpha)\n",
    "        focal = tf.where(tf.equal(y_true,1), (1.0-y_pred), (y_pred))\n",
    "        power = tf.where(tf.equal(y_true,0), gamma_0, gamma_1)\n",
    "        focal_power = tf.math.pow(focal,power)\n",
    "        product = tf.multiply(binary_crossentropy, weights)\n",
    "        focal_power_product = tf.multiply(product, focal_power)\n",
    "#        tf.clip_by_value(focal_power_product, 0.00001, 0.99999)\n",
    "        loss = keras.backend.mean(focal_power_product)\n",
    "        if math.isnan(loss):\n",
    "            print ('loss is nan')\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "get_custom_objects().update({'focal_loss_with_parameters': focal_loss_with_parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1224812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_with_parameters_2(alpha=.25, gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c9b6f",
   "metadata": {},
   "source": [
    "## Test Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b909c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Loss_Functions():\n",
    "    \n",
    "    ### Data as list y_test and y_prob\n",
    "    y_test = [0.0]*500 + [1.0]*500\n",
    "    y_test_binary = [0]*500 + [1]*500\n",
    "#    y_test = [0.0, 1.0]*5\n",
    "#    y_test_binary = [0,1]*5\n",
    "#    y_prob = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999]\n",
    "    y_prob = [random.random() for x in range (1000)]\n",
    "#    print (y_prob)\n",
    "    \n",
    "    ### Data as tensors y_true and y_pred\n",
    "    y_true = np.array(y_test, dtype=np.float32)\n",
    "    y_true = tf.convert_to_tensor(y_true)\n",
    "    y_pred = np.array(y_prob, dtype=np.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "    ####################################################\n",
    "    print ('Test with p==1.0, alpha = 0.5, gamma = 0.0')\n",
    "\n",
    "    ### Calculate binary crossentropy by hand\n",
    "    BCE = [-(y_test[i] * math.log(y_prob[i]) + (1 - y_test[i]) * math.log(1 - y_prob[i])) for i in range (10)]\n",
    "    Class_Weights = [1.0,1.0]\n",
    "    Weights = [Class_Weights[y_test_binary[i]] for i in range(10)]\n",
    "    Product = [BCE[i] * Weights[i] for i in range (10)]\n",
    "    loss = sum(Product)/len(Product)\n",
    "    print (loss, \"  Hand-calculated BCE loss\")\n",
    "    \n",
    "    ### Calculate binary crossentropy like I did in my custom loss functions\n",
    "    binary_crossentropy = keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "#    display(binary_crossentropy.numpy())\n",
    "    loss = keras.backend.mean(binary_crossentropy).numpy()\n",
    "    print (loss, \"  My custom AWBCE function's no-alpha backend\")\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.5)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 1.0, weight_1 = 1.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using Keras's loss function\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    loss = bce(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BCE function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 3.0, alpha = 0.75, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_parameter(alpha = 0.75)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom one-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate binary crossentropy using my custom loss function\n",
    "    # Weights for each class = (nSamples Total)/(2* (nSamples in Class))\n",
    "    loss_function = alpha_weighted_binary_crossentropy_with_class_weight_parameters(weight_0 = 2.0/3.0, weight_1 = 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My custom two-parameter AWBCE function')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.75,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.75, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=0.0')\n",
    "    \n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 0.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 0.0, 0.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with alpha = 0.8, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.8,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.8, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ####################################################\n",
    "    print ()\n",
    "    print ('Test with p = 1.0, alpha = 0.5, gamma = 2.0')\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 2.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate focal loss using my custom loss function\n",
    "    loss_function = focal_loss_with_parameters(0.5, 2.0, 2.0)\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, '  My BFC function with gamma=2.0')\n",
    "\n",
    "    ##################################################################\n",
    "    print ()\n",
    "    print (\"Test Keras's BFC Function with different values of alpha\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.1,\n",
    "        gamma = 0.0, \n",
    "#        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.5,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "    ### Calculate the same value using the BinaryFocalCrossentropy function\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing = True, \n",
    "        alpha = 0.9,\n",
    "        gamma = 0.0, \n",
    "        from_logits=False\n",
    "    )\n",
    "    loss = loss_function(y_true, y_pred).numpy()\n",
    "    print (loss, \"  Keras's BFC function\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#Test_Loss_Functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c65a77",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f61f36",
   "metadata": {},
   "source": [
    "## Another Keras Binary Classification Model\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c68709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha, gamma, epochs, filename, title):\n",
    "    print ('Keras_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ', gamma = ', gamma)\n",
    "    print ()\n",
    "    loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "        apply_class_balancing=True,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "#        from_logits=False,\n",
    "#        label_smoothing=0.0,\n",
    "#        axis=-1,\n",
    "#        reduction=losses_utils.ReductionV2.AUTO,\n",
    "#        name='binary_focal_crossentropy'\n",
    "    )   \n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "#        F1_Metric,\n",
    "    ]\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = estimator.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d06a6c",
   "metadata": {},
   "source": [
    "## Our Binary Focal Crossentropy Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c65be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha, gamma_0, gamma_1, epochs, filename, title):\n",
    "    print ('Our_Binary_Focal_Crossentropy')\n",
    "    print ('alpha = ', alpha, ' gamma_0 = ', gamma_0, ', gamma_1 = ', gamma_1)\n",
    "\n",
    "#    alpha_target = r_target/(r_target+1)\n",
    "    loss_function = focal_loss_with_parameters(alpha, gamma_0, gamma_1)\n",
    "#    loss_function = focal_loss_with_parameters_2(alpha_target, gamma)\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "#    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    metrics = [\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "#        F1_Metric,\n",
    "    ]\n",
    "    estimator = KerasClassifier(\n",
    "        model=model, \n",
    "#        random_state=42,\n",
    "        metrics=metrics,\n",
    "        batch_size=128, \n",
    "        verbose=0,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    estimator.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = estimator.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = estimator.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de86f",
   "metadata": {},
   "source": [
    "## AdaBoost Model\n",
    "https://stackoverflow.com/questions/39063676/how-to-boost-a-keras-based-neural-network-using-adaboost\n",
    "- model.predict_proba(X_test) returns two columns, \n",
    "    - the first the probability that the sample is in class 0, \n",
    "    - and the second the probability that the sample is in class 1.\n",
    "    - We just want the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "764615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('AdaBoost() ', filename)\n",
    "    model = AdaBoostClassifier(n_estimators=100)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afd759",
   "metadata": {},
   "source": [
    "### Ensembles of Classifiers\n",
    "https://imbalanced-learn.org/stable/ensemble.html#bagging-classifier\n",
    "\n",
    "with arguments based on the documentation examples\n",
    "\n",
    "https://imbalanced-learn.org/stable/auto_examples/ensemble/plot_comparison_ensemble_classifier.html#sphx-glr-auto-examples-ensemble-plot-comparison-ensemble-classifier-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cffa9",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe617891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('Bagging() ', filename)\n",
    "    model = BalancedBaggingClassifier(\n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on Test Set\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "\n",
    "    print ()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421ec7b",
   "metadata": {},
   "source": [
    "## Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee95effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, alpha, filename, title):\n",
    "    print ('Balanced Random Forest Classifier ', filename)\n",
    "    print ('alpha = ', alpha)\n",
    "    print ()\n",
    "    model = BalancedRandomForestClassifier(\n",
    "        bootstrap = True, ccp_alpha = 0.0, criterion = 'gini', \n",
    "        max_depth = None,\n",
    "#        max_depth = 40, \n",
    "        max_features = 'sqrt', \n",
    "        max_leaf_nodes = None,\n",
    "#        max_leaf_nodes = 10000,  \n",
    "        max_samples = None, \n",
    "        min_impurity_decrease = 0.0, \n",
    "        min_samples_leaf = 1, \n",
    "        min_samples_split = 2, \n",
    "        min_weight_fraction_leaf = 0.0, \n",
    "        n_estimators = 100, \n",
    "#        n_estimators = 1000, \n",
    "        n_jobs = None, \n",
    "        oob_score = False, \n",
    "        random_state = None, \n",
    "        replacement = False, \n",
    "        sampling_strategy = 'auto', \n",
    "        verbose = 0, \n",
    "        warm_start = False,\n",
    "        class_weight = {0:1-alpha, 1:alpha}\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    print ()\n",
    "    print ('model.get_params()')\n",
    "    print (model.get_params())\n",
    "    print ()\n",
    "    \n",
    "    print ('[estimator.get_depth() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_depth() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    print ('[estimator.get_n_leaves() for estimator in model.estimators_]')\n",
    "    print ([estimator.get_n_leaves() for estimator in model.estimators_])\n",
    "    print ()\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    \n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a4b47",
   "metadata": {},
   "source": [
    "## RUSBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240eb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUSBoost_Classifier(X_train, X_test, y_train, y_test, estimator, filename, title):\n",
    "    print ('RUSBoost Classifier ', filename)\n",
    "    model = RUSBoostClassifier(\n",
    "        n_estimators=1000, \n",
    "        estimator=estimator,\n",
    "        algorithm='SAMME.R', \n",
    "#        random_state=42\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "    \n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "\n",
    "    print ()\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb5091",
   "metadata": {},
   "source": [
    "## Easy Ensemble Classifier (Adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db654cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Easy_Ensemble_Classifier(X_train, X_test, y_train, y_test, filename, title):\n",
    "    print ('Easy Ensemble Classifier ', filename)\n",
    "    estimator = AdaBoostClassifier(n_estimators=10)\n",
    "    model = EasyEnsembleClassifier(n_estimators=10, estimator=estimator)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592a6f5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9075eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, alpha, filename, title):\n",
    "    print ('Logistic Regression Classifier ', filename)\n",
    "    model = LogisticRegression(\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)}\n",
    "        class_weight = {0:1-alpha, 1:alpha},\n",
    "        max_iter=1000,\n",
    "#        random_state=42,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train.values.ravel(),\n",
    "    )\n",
    "\n",
    "    # Test on training set for overfit\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_train, y_proba, y_pred, filename + '_Train', title)  \n",
    "    \n",
    "    # Test on test data, to test for underfit\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_proba = [x[1] for x in y_proba]\n",
    "    y_pred = K.round(y_proba).numpy()\n",
    "    y_proba = np.array(y_proba)\n",
    "    Chart_and_Plots(y_test, y_proba, y_pred, filename + '_Test', title)\n",
    "    \n",
    "    print ()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44982302",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d83bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chart_and_Plots(y_test, y_proba, y_pred, filename, title):\n",
    "    \n",
    "    Analyze_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)\n",
    "    \n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.0, 1.0)\n",
    "    Filename = filename + '_Transformed_100'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.01,0.99)\n",
    "    Filename = filename + '_Transformed_98'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    y_test, y_proba_New, y_pred = Linear_Transform_y_proba_Specified(y_test, y_proba, 0.025,0.975)\n",
    "    Filename = filename + '_Transformed_95'\n",
    "    Analyze_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction(y_test, y_proba_New, Filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba_New, Filename, title)\n",
    "    ROC(y_test, y_proba_New, [], Filename)\n",
    "\n",
    "    \n",
    "    print ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c31ae0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba(y_test, y_proba):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],0.025)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],0.975)\n",
    "    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c5b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Transform_y_proba_Specified(y_test, y_proba, left, right):\n",
    "    print ('Linear_Transform_y_proba()')\n",
    "    print ()\n",
    "    \n",
    "    # I considered two methods.  \n",
    "    # One was to take the medians of the negative and positive classes and transform them to 0.25 and 0.75.\n",
    "    # That didn't always work the way I wanted.  \n",
    "    # Then I tried taking the 0.05 quantile to 0.05 and the 0.95 quantile to 0.95.\n",
    "    \n",
    "#    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "#    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    center = (N_median + P_median)/2\n",
    "#    print ('N_median = %.3f, P_median = %.3f, center = %.3f' % (N_median, P_median, center))\n",
    "#    y_proba = 0.25/(center - N_median) * (y_proba - center) + 0.5\n",
    "\n",
    "    \n",
    "    a = np.quantile(y_proba[np.array(y_test)==0],left)\n",
    "    b = np.quantile(y_proba[np.array(y_test)==1],right)\n",
    "    print ('a = %.3f, b = %.3f' % (a, b))\n",
    "    y_proba = 1/(b-a) * (y_proba - a)\n",
    "    \n",
    "    y_proba = np.where (y_proba < 0.0, 0.0, y_proba)\n",
    "    y_proba = np.where (y_proba > 1.0, 1.0, y_proba)\n",
    "    y_pred = K.round(y_proba)\n",
    "\n",
    "    print ()\n",
    "    \n",
    "    return y_test, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085583e",
   "metadata": {},
   "source": [
    "## Evaluate_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97848247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_Model(y_test, y_proba, y_pred, center, filename):\n",
    "    print ('Evaluate_Model()')\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = [round(x) for x in y_proba]\n",
    "    y_pred = np.array(y_pred)\n",
    "    print ('np.unique(y_proba) = ', np.unique(y_proba))\n",
    "    print ('np.unique(y_pred) = ', np.unique(y_pred))\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "    print(CM)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    CSV = [[filename, CM[0][0], CM[0][1], CM[1][0], CM[1][1], center, auc_value]]\n",
    "    np.savetxt('./Confusion_Matrices/' + filename + '.csv', \n",
    "        CSV,\n",
    "        delimiter =\", \", \n",
    "        fmt ='% s'\n",
    "              )\n",
    "    print ()\n",
    "    CM = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "    print(CM)\n",
    "    print ()\n",
    "\n",
    "#    y_pred = y_pred.ravel()\n",
    "#    y_test = tf.convert_to_tensor(y_test)\n",
    "#    y_pred = tf.convert_to_tensor(y_pred)\n",
    "\n",
    "#    print ('%.3f & Precision \\cr ' %  Precision_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & Recall \\cr ' %  Recall_Metric(y_test, y_pred).numpy())\n",
    "#    print ('%.3f & F1 \\cr ' %  F1_Metric(y_test, y_pred).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf928217",
   "metadata": {},
   "source": [
    "## Plot Prediction\n",
    "\n",
    "How to insert a .pgf plot into a \\LaTeX document:\n",
    "\n",
    "\\begin{figure}\n",
    "    \\begin{center}\n",
    "        \\input{Plot.pgf}\n",
    "    \\end{center}\n",
    "    \\caption{A PGF histogram from \\texttt{matplotlib}.}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3496ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (0, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "    \n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    plt.xticks(\n",
    "        ticks = [0, 2.5, 5, 7.5, 10], \n",
    "        labels = ['0.0', '0.25', '0.5', '0.75', '1.0'],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b634349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Prediction_Zoom(y_test, y_proba, filename, title, left, right):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    B = B[A['HOSPITAL'] > left]\n",
    "    B = B[A['HOSPITAL'] < right]\n",
    "    A = A[A['HOSPITAL'] > left]\n",
    "    A = A[A['HOSPITAL'] < right]\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    bins = [x*0.05 for x in range (21)]\n",
    "#    bins = [x*0.10 for x in range (11)]\n",
    "    n = 10\n",
    "    bins= [left + (right-left)*x/n for x in range (-1, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=False)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "\n",
    "    ticks = [0, 2.5, 5, 7.5, 10]\n",
    "    labels = [str(round(left + (right-left) * t/10,2)) for t in ticks]\n",
    "    plt.xticks(\n",
    "        ticks = ticks, \n",
    "        labels = labels,\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Zoom.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Zoom.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9eda862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot_Prediction()\n",
      "Test\n",
      "y_test =  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "y_proba =  [0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.9, 0.9, 1.0, 1.0]\n",
      "./Images/Test_Pred_Wide.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Plot_Prediction_Wide(y_test, y_proba, filename, title):\n",
    "    print ('Plot_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "    print ('y_test = ', y_test)\n",
    "    print ('y_proba = ',y_proba)\n",
    "\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "#    print (\"A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\")\n",
    "#    display(A)\n",
    "#    print (\"B = pd.DataFrame(y_test, columns=['HOSPITAL'])\")\n",
    "#    display(B)\n",
    "#    print (\"C = A[B['HOSPITAL']==0]\")\n",
    "#    display(C)\n",
    "#    print (\"D = A[B['HOSPITAL']==1]\")\n",
    "#    display(D)\n",
    "    n = 20\n",
    "#    bins= [x/n - 1/(2*n) for x in range (-1, n+3)]\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "#    print ('Bins = ', bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "#    print (\"E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(E)\n",
    "#    print (\"F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\")\n",
    "#    display(F)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "#    print (\"G = E.value_counts(sort=False)\")\n",
    "#    display(G)\n",
    "#    print (\"H = F.value_counts(sort=False)\")\n",
    "#    display(H)\n",
    "\n",
    "    G = G/len(y_proba)*100\n",
    "    H = H/len(y_proba)*100\n",
    "#    print (\"G = G/len(y_proba)*100\")\n",
    "#    display(G)\n",
    "#    print (\"H = H/len(y_proba)*100\")\n",
    "#    display(H)\n",
    "\n",
    "    fig = plt.figure(figsize=(4.5,1.5)) # Create matplotlib figure\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    \n",
    "    G.plot(kind='bar', fill=False, ax=ax, width=0.4, position=1)\n",
    "    H.plot(kind='bar', color='black', ax=ax, width=0.4, position=0)\n",
    "    ticks = [n/20*i for i in range (-1,22)]\n",
    "#    print ('ticks = ', ticks)\n",
    "    plt.xticks(\n",
    "        ticks = ticks,\n",
    "        labels = ['','0.0', '', '0.1', '', '0.2', '', '0.3', '', '0.4', '', '0.5', '', '0.6', '', '0.7', '', '0.8', '', '0.9', '', '1.0', ''],\n",
    "        rotation=0\n",
    "    )\n",
    "    ax.legend(['Neg', 'Pos'])\n",
    "#    plt.title(title)\n",
    "    plt.xlabel('$p$')\n",
    "    plt.ylabel('Percent of Data Set')\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_Pred_Wide.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_Pred_Wide.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "\n",
    "def Test_Plot_Prediction_Wide():\n",
    "    \n",
    "    y_proba = (\n",
    "        [0.0]*5 + \n",
    "        [0.0]*0 + \n",
    "        [0.1]*6 + \n",
    "        [0.1]*1 + \n",
    "        [0.2]*7 + \n",
    "        [0.2]*2 + \n",
    "        [0.3]*6 + \n",
    "        [0.3]*1 + \n",
    "        [0.4]*8 + \n",
    "        [0.4]*2 + \n",
    "        [0.5]*9 + \n",
    "        [0.5]*2 + \n",
    "        [0.6]*8 + \n",
    "        [0.6]*2 + \n",
    "        [0.7]*6 + \n",
    "        [0.7]*3 + \n",
    "        [0.8]*5 + \n",
    "        [0.8]*3 + \n",
    "        [0.9]*3 + \n",
    "        [0.9]*2 + \n",
    "        [1.0]*0 + \n",
    "        [1.0]*2 \n",
    "    )\n",
    "    y_test = (\n",
    "        [0]*5 + \n",
    "        [1]*0 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*7 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*1 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*9 + \n",
    "        [1]*2 + \n",
    "        [0]*8 + \n",
    "        [1]*2 + \n",
    "        [0]*6 + \n",
    "        [1]*3 + \n",
    "        [0]*5 + \n",
    "        [1]*3 + \n",
    "        [0]*3 + \n",
    "        [1]*2 + \n",
    "        [0]*0 + \n",
    "        [1]*2 \n",
    "    )\n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')\n",
    "    \n",
    "Test_Plot_Prediction_Wide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e84dc",
   "metadata": {},
   "source": [
    "## Switching between FP/TP and Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee52a47",
   "metadata": {},
   "source": [
    "$$\\text{Precision} = \\frac{TP}{FP+TP}$$\n",
    "\n",
    "$$\\frac{1}{\\text{Precision}} = \\frac{FP+TP}{TP} = \\frac{FP}{TP} + \\frac{TP}{TP} = \\frac{FP}{TP} +  1$$\n",
    "\n",
    "$$\\frac{FP}{TP} + 1 = \\frac{1}{\\text{Precision}}$$\n",
    "\n",
    "$$\\frac{FP}{TP} = \\frac{1}{\\text{Precision}} - 1 = \\frac{1}{\\text{Precision}} - \\frac{\\text{Precision}}{\\text{Precision}}  = \\frac{1 - \\text{Precision}}{\\text{Precision}}$$\n",
    "\n",
    "- In a previous version I had wanted $FP/TP$ to equal either 2.0, 1.0, or 0.5, indicating that we were willing to send 2 unnecessary ambulances for each necessary one, etc.  \n",
    "    - $FP/TP = 2.0$ corresponds to precision = 1/3\n",
    "    - $FP/TP = 1.0$ corresponds to precision = 1/2\n",
    "    - $FP/TP = 0.5$ corresponds to precision = 2/3\n",
    "\n",
    "- Neg/Pos corresponds to marginal precision similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d26ec6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_Counts_y_proba\n",
      "430 332\n",
      "310\n",
      "n200 =  200\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def Value_Counts_y_proba(y_proba, filename):\n",
    "#    print ()\n",
    "    print ('Value_Counts_y_proba')\n",
    "#    print (type(y_proba))\n",
    "    Y_proba = pd.Series(y_proba)\n",
    "    A = Y_proba.value_counts().reset_index(drop=True)\n",
    "    n = len(y_proba)\n",
    "    nA = len(A)\n",
    "#    display(Y_proba)\n",
    "#    display(A)\n",
    "    B = A.cumsum()\n",
    "#    display(B)\n",
    "#    print (B[10])\n",
    "#    print ()\n",
    "    cutoff_95 = B.sub(0.95*n).abs().idxmin() + 1\n",
    "    cutoff_90 = B.sub(0.90*n).abs().idxmin() + 1\n",
    "    cutoff_80 = B.sub(0.80*n).abs().idxmin() + 1\n",
    "    print (n, nA)\n",
    "    print (cutoff_95)\n",
    "#    print ()\n",
    "\n",
    "    n100 = min(100, len(B)-1)\n",
    "    n200 = min(200, len(B)-1)\n",
    "    print ('n200 = ', n200)\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'a')\n",
    "    f.write('%s,%d,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f,%d,%0.4f\\n' % (\n",
    "        filename, n, nA, nA/n, \n",
    "        cutoff_95, cutoff_95/n,\n",
    "        cutoff_90, cutoff_90/n,\n",
    "        cutoff_80, cutoff_80/n,\n",
    "        B[10], B[10]/n,\n",
    "        B[20], B[20]/n,\n",
    "        B[n100], B[n100]/n,\n",
    "        B[n200], B[n200]/n,\n",
    "    ))\n",
    "    f.close()\n",
    "    \n",
    "    H = Y_proba.value_counts().head(100)\n",
    "    Filename = './Analyze_Proba/' + filename + '_Value_Counts.csv'\n",
    "    H.to_csv(Filename)\n",
    "    \n",
    "    \n",
    "    print ('Finished')\n",
    "    return 0\n",
    "    \n",
    "def Create_Files_for_Value_Counts_y_proba():\n",
    "    f = open('./Analyze_Proba/Value_Counts_y_proba.csv', 'w')\n",
    "    f.write(\"Filename,n,nUnique,nUnique/n,95%,95%/n,90%,90%/n,80%,80%/n,B[10],B[10]/n,B[20],B[20]/n,B[100],B[100]/n,B[200],B[200]/n,\\n\")\n",
    "    f.close()\n",
    "    \n",
    "#Create_Files_for_Value_Counts_y_proba()\n",
    "    \n",
    "    \n",
    "def Test_Value_Counts_y_proba():\n",
    "    A = [5]*50 + [6]*20 + [i for i in range (10,40)]*2 + [i for i in range (100,400)]\n",
    "    Value_Counts_y_proba(A, 'Test')\n",
    "\n",
    "Test_Value_Counts_y_proba()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de839f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prediction(y_test, y_proba, filename, title):\n",
    "    print ('Analyze_Prediction()')\n",
    "    print (filename)\n",
    "    \n",
    "    Value_Counts_y_proba(y_proba, filename)\n",
    "    \n",
    "#    print (y_test)\n",
    "#    print (y_proba)\n",
    "#    return 0\n",
    "#    y_test = y_test.numpy()\n",
    "    A = pd.DataFrame(y_proba, columns=['HOSPITAL'])\n",
    "    B = pd.DataFrame(y_test, columns=['HOSPITAL'])\n",
    "    B = B.reset_index(drop=True)\n",
    "    C = A[B['HOSPITAL']==0]\n",
    "    D = A[B['HOSPITAL']==1]\n",
    "    print ('print (len(A), len(C), len(D), len(C) + len(D))')\n",
    "    print (len(A), len(C), len(D), len(C) + len(D))\n",
    "\n",
    "    N = len(C)\n",
    "    P = len(D)\n",
    "    \n",
    "    ##### 10 bins\n",
    "    n = 10\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['mPrec'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "    \n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['mPrec']=Analyze['mPrec'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "    print ('./Analyze_Proba/' + filename + '_10.tex')\n",
    "    print (len(y_proba))\n",
    "    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_10.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_10.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ##### 100 bins\n",
    "    n = 100\n",
    "    bins= [x/n for x in range (-1, n+1)]\n",
    "    print (bins)\n",
    "    E = pd.cut(C['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    F = pd.cut(D['HOSPITAL'], bins=bins, include_lowest=True)\n",
    "    \n",
    "    G = E.value_counts(sort=False)\n",
    "    H = F.value_counts(sort=False)\n",
    "\n",
    "    Analyze = pd.DataFrame()\n",
    "    Analyze['Neg'] = G\n",
    "    Analyze['Pos'] = H\n",
    "    Analyze['p'] = bins[1:]\n",
    "#    Analyze['Neg/Pos'] = Analyze['Neg']/Analyze['Pos']\n",
    "    Analyze['mPrec'] = Analyze['Pos']/(Analyze['Pos'] + Analyze['Neg'])\n",
    "    Analyze['TN'] = Analyze['Neg'].cumsum()\n",
    "    Analyze['FP'] = N - Analyze['TN']\n",
    "    Analyze['FN'] = Analyze['Pos'].cumsum()\n",
    "    Analyze['TP'] = P - Analyze['FN']\n",
    "#    Analyze['FP/TP'] = Analyze['FP']/Analyze['TP']\n",
    "#    Analyze['FP+TP'] = Analyze['FP'] + Analyze['TP']\n",
    "    Analyze['Prec'] = Analyze['TP']/(Analyze['FP'] + Analyze['TP'])\n",
    "    Analyze['Rec'] =  Analyze['TP']/(Analyze['FN'] + Analyze['TP'])\n",
    "    Analyze['$\\hat{p}$'] = (Analyze['TP'] + Analyze['FP'])/len(y_proba)\n",
    "    \n",
    "    A = Analyze.copy(deep=True)\n",
    "\n",
    "    Analyze['Neg']=Analyze['Neg'].apply('{:,}'.format)\n",
    "    Analyze['Pos']=Analyze['Pos'].apply('{:,}'.format)\n",
    "    Analyze['TN']=Analyze['TN'].apply('{:,}'.format)\n",
    "    Analyze['FP']=Analyze['FP'].apply('{:,}'.format)\n",
    "    Analyze['FN']=Analyze['FN'].apply('{:,}'.format)\n",
    "    Analyze['TP']=Analyze['TP'].apply('{:,}'.format)\n",
    "#    Analyze['FP+TP']=Analyze['FP+TP'].apply('{:,}'.format)\n",
    "    \n",
    "#    Analyze['Neg/Pos']=Analyze['Neg/Pos'].apply('{:.2f}'.format)\n",
    "    Analyze['mPrec']=Analyze['mPrec'].apply('{:.2f}'.format)\n",
    "#    Analyze['FP/TP']=Analyze['FP/TP'].apply('{:.2f}'.format)\n",
    "    Analyze['Prec']=Analyze['Prec'].apply('{:.2f}'.format)\n",
    "    Analyze['Rec']=Analyze['Rec'].apply('{:.2f}'.format)\n",
    "    Analyze['$\\hat{p}$']=Analyze['$\\hat{p}$'].apply('{:.2f}'.format)\n",
    "        \n",
    "#    Analyze.index.name = 'p'\n",
    "    Analyze.set_index('p', inplace=True)\n",
    "#    print ('./Analyze_Proba/' + filename + '_100.tex')\n",
    "#    print (len(y_proba))\n",
    "#    display(Analyze)\n",
    "    Analyze.to_csv('./Analyze_Proba/' + filename + '_100.csv', index=True)\n",
    "    Analyze.to_latex(\n",
    "        './Analyze_Proba/' + filename + '_100.tex', \n",
    "        index=True, \n",
    "        float_format=\"{:.2f}\".format, \n",
    "        column_format='rrrrrrrrrrrrrr',\n",
    "        escape=False\n",
    "    )\n",
    "    \n",
    "    # Append CSV files with results from multiple models\n",
    "    A.set_index('p', inplace=True)\n",
    "    A.insert(0, 'Filename', filename)\n",
    "    \n",
    "    # Remove rows with negligible number of samples\n",
    "    A = A[A['Neg'] >= 20]\n",
    "    A = A[A['Pos'] >= 20]\n",
    "    \n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['mPrec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/mPrec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.333).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_333.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.5).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_5.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['Prec'] - 0.667).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/Prec_0_667.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.05).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_05.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.10).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_10.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    A_closest = A.iloc[(A['$\\hat{p}$'] - 0.15).abs().argsort()[:1]].head(1)\n",
    "    A_closest.to_csv('./Analyze_Proba/p_hat_0_15.csv', mode='a', index=True, header=False)\n",
    "    \n",
    "    \n",
    "def Create_Files_for_Analyze_Prediction():\n",
    "    f = open('./Analyze_Proba/mPrec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/mPrec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/mPrec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_5.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_667.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/Prec_0_333.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_05.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_10.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "    f = open('./Analyze_Proba/p_hat_0_15.csv', 'w')\n",
    "    f.write(\"p,Filename,Neg,Pos,$m$Prec,TN,FP,FN,TP,Prec,Rec,$\\hat{p}\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#Create_Files_for_Analyze_Prediction()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e092785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Test_Plot_Prediction_Zoom():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)  \n",
    "    \n",
    "    display(y_proba[:20])\n",
    "    display(y_pred[:20])\n",
    "    \n",
    "    Plot_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Wide(y_test, y_proba, 'Test', 'Test')    \n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.45, 0.55)\n",
    "    Analyze_Prediction(y_test, y_proba, 'Test', 'Test')    \n",
    "    \n",
    "#Test_Plot_Prediction_Zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a04113",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y_test, y_proba, p_values, filename):\n",
    "    print ('ROC()')\n",
    "    print (filename)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    N_median = np.median(y_proba[np.array(y_test)==0])\n",
    "    P_median = np.median(y_proba[np.array(y_test)==1])\n",
    "#    print ('N_median, P_median = ', N_median, P_median)\n",
    "\n",
    "    m = np.quantile(y_proba,0.50)\n",
    "    p = np.quantile(y_proba,0.25)\n",
    "    q = np.quantile(y_proba,0.75)\n",
    "    \n",
    "    Y = []\n",
    "    print ('p_values = ', p_values)\n",
    "    for X in p_values:\n",
    "        difference_array = np.absolute(thresholds-X)\n",
    "        index = difference_array.argmin()\n",
    "        F = fpr[index]\n",
    "        T = tpr[index]\n",
    "        Y.append([X,str(round(X,3)),F,T])\n",
    "    \n",
    "    auc_value = auc(fpr, tpr)\n",
    "    auc_value = round(auc_value,3)\n",
    "    fig = plt.figure(figsize=(2.0,1.5)) # Create matplotlib figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, color='black', label='AUC={:.3f}'.format(auc_value))\n",
    "    \n",
    "    for y in Y:\n",
    "#        plt.plot([y[2]], [y[3]], marker=\"o\", markersize=20, markeredgecolor=\"white\", markerfacecolor=\"white\")\n",
    "#        plt.annotate(\n",
    "#            y[1], # this is the text\n",
    "#            (y[2], y[3]), # these are the coordinates to position the label\n",
    "#            ha='center' # horizontal alignment can be left, right or center\n",
    "#        )\n",
    "        plt.text(\n",
    "            y[2], y[3], # these are the coordinates to position the label\n",
    "            y[1], # this is the text\n",
    "            backgroundcolor='white', # horizontal alignment can be left, right or center\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='square,pad=0.3')\n",
    "        )\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "#    plt.title('ROC with AUC {:.3f}'.format(auc_value))\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('./Images/' + filename + '_ROC.png', bbox_inches=\"tight\")\n",
    "    plt.savefig('./Images/' + filename + '_ROC.pgf', bbox_inches=\"tight\")\n",
    "    print ('./Images/' + filename + '_ROC.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print ()\n",
    "    return 0\n",
    "\n",
    "def Test_ROC():\n",
    "    y_test = [0,0,0,0,0,1]*10000\n",
    "#    y_proba = [abs(0.45 - y)+round(0.45*random.random(),2) for y in y_test]\n",
    "    y_proba = [abs(0.45 - y)+round(0.45*random.normalvariate(mu=0.2, sigma=0.2),3) for y in y_test]\n",
    "#    random.normalvariate(mu=0.0, sigma=1.0)\n",
    "    y_test = np.array(y_test)\n",
    "    y_proba = np.array(y_proba)\n",
    "    print (y_test)\n",
    "    print (y_proba)\n",
    "    ROC(y_test, y_proba, [0.5], \"tmp\")\n",
    "    \n",
    "#Test_ROC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb935a",
   "metadata": {},
   "source": [
    "## Build Idealized Results Plots\n",
    "- The Plot_Prediciton and ROC_Curves functions take two lists (or np arrays) and a filename for saving the plots:\n",
    "    - ROC(y_test, y_proba, filename):\n",
    "    - Plot_Prediction(y_test, y_proba, filename):\n",
    "    - y_test is the {0,1} binary and \n",
    "    - y_proba is the (0,1) continuous\n",
    "- The Evaluate_Model(y_test, y_proba, y_pred, filename) takes three lists (or np arrays)\n",
    "    - y_test is the {0,1} binary ground truth,\n",
    "    - y_proba is the (0,1) continuous prediction, and\n",
    "    - y_pred is the discrete {0,1} binary version of y_proba\n",
    "- We want a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e66bb65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idealized_Results()\n",
      "Move_Threshold()\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "150771 26621\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t</td>\n",
       "      <td>TN</td>\n",
       "      <td>FP</td>\n",
       "      <td>FN</td>\n",
       "      <td>TP</td>\n",
       "      <td>TPR</td>\n",
       "      <td>FPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>150771</td>\n",
       "      <td>0</td>\n",
       "      <td>26621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>4459</td>\n",
       "      <td>146312</td>\n",
       "      <td>242</td>\n",
       "      <td>26379</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.970425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>27821</td>\n",
       "      <td>122950</td>\n",
       "      <td>732</td>\n",
       "      <td>25889</td>\n",
       "      <td>0.972503</td>\n",
       "      <td>0.815475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>62627</td>\n",
       "      <td>88144</td>\n",
       "      <td>1826</td>\n",
       "      <td>24795</td>\n",
       "      <td>0.931408</td>\n",
       "      <td>0.584622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4</td>\n",
       "      <td>94723</td>\n",
       "      <td>56048</td>\n",
       "      <td>3446</td>\n",
       "      <td>23175</td>\n",
       "      <td>0.870553</td>\n",
       "      <td>0.371743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>117910</td>\n",
       "      <td>32861</td>\n",
       "      <td>6139</td>\n",
       "      <td>20482</td>\n",
       "      <td>0.769393</td>\n",
       "      <td>0.217953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6</td>\n",
       "      <td>132769</td>\n",
       "      <td>18002</td>\n",
       "      <td>10384</td>\n",
       "      <td>16237</td>\n",
       "      <td>0.609932</td>\n",
       "      <td>0.1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.7</td>\n",
       "      <td>141458</td>\n",
       "      <td>9313</td>\n",
       "      <td>15998</td>\n",
       "      <td>10623</td>\n",
       "      <td>0.399046</td>\n",
       "      <td>0.061769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.8</td>\n",
       "      <td>146091</td>\n",
       "      <td>4680</td>\n",
       "      <td>22005</td>\n",
       "      <td>4616</td>\n",
       "      <td>0.173397</td>\n",
       "      <td>0.03104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9</td>\n",
       "      <td>149553</td>\n",
       "      <td>1218</td>\n",
       "      <td>25905</td>\n",
       "      <td>716</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.008078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>150771</td>\n",
       "      <td>0</td>\n",
       "      <td>26621</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1       2      3      4         5         6\n",
       "0     t      TN      FP     FN     TP       TPR       FPR\n",
       "1   0.0       0  150771      0  26621       1.0       1.0\n",
       "2   0.1    4459  146312    242  26379  0.990909  0.970425\n",
       "3   0.2   27821  122950    732  25889  0.972503  0.815475\n",
       "4   0.3   62627   88144   1826  24795  0.931408  0.584622\n",
       "5   0.4   94723   56048   3446  23175  0.870553  0.371743\n",
       "6   0.5  117910   32861   6139  20482  0.769393  0.217953\n",
       "7   0.6  132769   18002  10384  16237  0.609932    0.1194\n",
       "8   0.7  141458    9313  15998  10623  0.399046  0.061769\n",
       "9   0.8  146091    4680  22005   4616  0.173397   0.03104\n",
       "10  0.9  149553    1218  25905    716  0.026896  0.008078\n",
       "11  1.0  150771       0  26621      0       0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ideal\n",
      "Plot_Prediction()\n",
      "Ideal\n",
      "[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "./Images/Ideal_Pred.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOEAAACwCAYAAAD5YQkoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVJklEQVR4nO3dv3Pa9v8H8CfON/GdmxoFJ0su9gXRDtliDF17F+N2ryFesjWGds21xmTJdbIh3ROgY4YasP+AIHzXoUuxFE/1dZDsxL1kiAsC95xzmlqfwV9UZMAWskACXo877pAQ8IqjF2/p/dOhKIoCQohlhqwOgJBBR0lIiMUoCQmxGCUhIRajJCTEYpSEhFiMkpAQi1ESEmIxSkJCLPZ/Rt60s7ODmzdvAgAqlQo4jsPU1JS67yyCIIDjOABAsVhEOp0GwzAAAEmSkMvlwLIsJElCOBxWXzvN0dERXr9+jY8//hgOh8PAv4oQfRRFwf7+Pq5fv46hIRPKMcWAdDqta18r8Xhc89zr9arb9c9FUVSCwaCuz9zd3VUA0IMeXXvs7u7qPudP41AUfX1HK5UKMpkMHA4H8vk8ZmZmNK/zPI8nT56c+TmCIGB6ehrlchnAccnn8XggiiIAIBQKged59fgrV66ox54VH8Mw2N3dxejoqJ5/EiGGVKtVjI+PQ5ZlOJ3Oc3+e7stRp9OJQCCAeDwOURThdrs1ry8sLOj6HK/Xi3Q6rW7LsgwAcLlcyGQycLlcmuNdLhcEQYDX69XsPzw8xOHhobq9v78PABgdHaUkJF1h1m1PW/eEbrcbT58+RaFQwPT0tOEvDQaD6vOVlRUEAgEwDKMm5EmlUqlh39LSEn744QfDMRBiF4YqZqanp/H48WNsbGxgZWUFhUIBfr+/7RJIlmXkcjnN5Wer406KxWJ48OCBul27ROimV69eYW9vr2H/1atXMTEx0dVYSO8ylISxWAwsyyIQCAA4Tsq1tTV89dVXbX1ONBpFPp9Xaz8Zhmko9UqlUtPa0eHhYQwPDxsJ3xSvXr3CrVu3cHBw0PDayMgItra2KBGJLoaS0OfzYXZ2FoVCwfAXJxIJRKNRsCyrlnSBQADJZLLp99nN3t4eDg4O8OzZM9y6dUvdv7W1hXv37mFvb8/2SagoCj58+IB///3X6lBs5+LFi7hw4UJXvstQEm5vbwPQ3pgWi0XdJWEul4PX61UTMJPJNG0PlCQJPp9PVzuhVW7dutVQadQL3r9/jzdv3jQtycnxuX3jxg1cvny5499lKAknJyfh8/kwNjaGfD4PjuMQj8d1vVeSJIRCIc0+hmEQDocBANlsFtFoFH6/H8ViEdls1kiI5BRHR0fY3t7GhQsXcP36dVy6dIk6ONRRFAVv377Fn3/+iU8//bTjJaLhiplMJoNUKgVFUZBKpTA5OanrvSzL4rSmSZZl1YSur0Ul5nn//j2Ojo4wPj6OkZERq8OxpWvXrmFnZwf//POPPZMQOE6W5eVlM2MhXWZKl6s+1c0rA11JmE6nIUkSxsbGEA6HMTo6irW1NSwtLUGWZQSDQSwtLXU6VtIFrZpdOoGacv6fnr5tHMcpiURCsz00NKSk02lFlmUll8spi4uLpvSjM6pSqSgAlEql0pXv43leAaDwPK9rv528e/dO+f3335V3795p9r98+VIZGRnpWt/LkZER5eXLl7pi5nleWVhYUAAoyWRS3S+KohIOhxWWZTX7O/U3UhTzzzVdJeH29ja+//57dTsejyMYDOL+/fsAgNnZ2aa9Wsjp7NbY36rZpRPabcqpr02PRCK4e/cuGIYBy7JIJpNIJBJq5V6vMXRPyHEcUqmUZh/VrrXHzo39dm52CYVCKJVKmJ+f19Sc27kZ6yy6krB+FEOt83Wtt0xNq36fpLl+aOy3SjqdhtvtRi6X64sadF1JGAwG4fP54HA4IIoistmsOoD3xYsXWFxcbGj7I/rYudSxK4ZhEI/HMT8/3zQJOY6DIAhgWRbFYlFt8srlcpAkCQzDgOd5hEIhCIKgewRQp+hKQrfbjY2NDWxvb2uGMFUqFQCgpgrSdeFwGNlsFpFIRNPVUZIkRKNRdVBAqVRS7xfn5+fVqzqPx4NoNNpwRWeFtocy1XM6nbob6QkxWzKZhMfjQSQS0exzuVzq9CnAcZdKO1faGG6sJ8Rqtd5VoVAI0WhU3e/1ejUlXC0Bw+EwEokEGIZBJBIBy7Jdj7kZSkLSM5o1gy0sLGBlZUWtGJybm8P8/LzmGI7jEAgEMDY2Zvn9XzOUhKTB1taW7b5DEAREo1GUSiXEYjFNhUw6ncbGxgaA41IwHo+rgwCA/2ryRVGEx+MBwzBwuVwIhUK2uEw1JQnX19chy3Lbg3qJvVy9ehUjIyO4d+9eV75vZGQEV69e1XWs1+tFPp9v+Vp9DXMgEGiocOE4Dh6PR63EqVXg1A9Ot4rhJFxbW4MkSQCOh35sbGxQEva4iYkJbG1t9WXf0Xw+j7m5OXWbZVnMzc2p57CVDCXh4uIiZFlGqVTSdCUivW9iYqIvOwnE43EkEglwHKdWyJRKpd69HPV4PJifn8f29jYcDgdu3ryJ9fV1s2MjxFR2rJQBDK5FwbIsXr58qXYdIoQYZ6gklGUZLMuiXC5jb28PX375JRiGwZ07d8yOj5C+ZygJZ2dn1Rm6lpeXUSgUbDkjGiG9wNDl6M7Ojvq8UqlAlmVd60UQQhoZSsL6fnlOpxOzs7OafYQQ/XRfjp5clekknufVkfakd3VzcLaib0Gwvtf1VZkIMUIQBCSTSaRSKSwsLKjL6UmShEgkYnmvl/OwZFUmQtpV6xOaSqUQi8XU6SxkWcaVK1fA83zPDo42dE94MgHX19extrZmSkCEtKM22dPKyorVoRhGfUdJzyuVSvB4PFaHYRj1HSU9S5ZlLC0tIRAIqH1ABUFQ+4dKkoRgMKhZeKj2vH7uGatR31HSc1KplNoJu36EfG14Un3t/dTUFAqFAlKplGbEvZ3myTWUhPV9R3/88Ud89913ZsdFSEvNltEDjueXOVk5w7IsMpkMgsEgpqam1CFMdhg9UWOoYqbWd7Rarap9R5st7kmIXbhcLpTLZaTTafz111+2mqJz4PuO2m0qenK2VkuoN5tfRhAEpNNpLC0tIRKJqKPwez4JT+rVNkM7T0V/lk79eNi1F4sgCGozRDweVxOqXq0tMZFIqBP/ZrNZMAyDsbExcBwHl8uFUqmkGWVvtbaSsFqtIpVKoVgsQpZleL1eRCIRdTbuXtOrU9H38o+HUbUS7Kwazfr5Zeong7Jzjy7d94SPHz8GwzD4+eefoSgKnE4n8vk8pqam8PDhw07G2HG1qehrj06vSHRe9T8ePM+rj2fPnuHg4KBrc8QQc+gqCVdXV1EsFlEul+F0Ohte/+abb7C2tkaN9V1G61j0B10lIcdxyGQyTRMQAJ4+fdpyOjpCyOl0JaGe6cLtMqU4Ib1GVxJeuXLFlGOIvRwdHVkdgm11s5ZY1z2hKIrY398/NTBRFE0LinTWpUuXMDQ0hNevX+PatWu4dOkSrbRcR1EUvH37Fg6HAxcvXuz49+lKwlrbSyuKosDhcGBpacm0wEjnDA0Nwe12482bN3j9+rXV4diSw+HAjRs3cOHChY5/l64kDIfDp7bPKIqCxcVF04IinXfp0iVMTEzgw4cPau8n8p+LFy92JQEBnUkYiURa1ozWH0N6S+1yqxuXXKQ1XRUzelbjpRV7CTHG0CgKQoh5KAkJsZglSSgIAqamphr2S5KERCKBXC6HRCKhLoFMSD87MwkrlQr8fj+q1aopX1hbxUkQhIbXQqEQFhYWEAwGEQwGG8aGEdKPzkzCjY0NZLNZjI6Oqvt++umnhuP0TnkYDAabdjo+uWIqy7I0tT4ZCGc2Ufh8PszPz+Ozzz5TRzNns9mGS8V8Pn+uURS1AZf1XC4XBEFomrSHh4c4PDxUt80qqQnptjNLQqfTiXQ6DbfbjXK5jHK5DEVRGh5//fXXuQJpdf/XalaspaUlOJ1O9TE+Pn6u7yfEKroa62srL9UEAoGGdsFOrQXQKjljsRgePHigblerVUpE0pMMzTEzOTmJarWKTCYDALh79+65G+sZhmko9VpN6AMAw8PDGB4ePtd3EmIHhpootre3cefOHTx//hzPnz/H1NQUNjc3zxVIq5K0V2dxI0QvQyXh6uoqNjY2NPtisRhu377d1ufIsqyWdCcHBUuSBJ/P17IkJKRfGErCk2sTAvpLLI7j1KkwlpaW4Pf71VmxstksotEo/H6/Ol0dIf3OUBKebNMDji9R9ahNSddsaBTLsur++unqCOlnhpIwEAjgiy++ULuecRxnmxVuCOk1hmtHk8mkuv5EKpWioUw9hKb+txfD0+C73W4sLy+bGQvpgkGcvdvuTFmLgvSOXp36v59REg4omr3bPmhQLyEWoyQkxGKGknBnZ0d9XqlUsLq6qtlHCNHPUBLWD7atjbCgAbiEGKO7YqZSqSCTycDhcDRdgYnnedy/f9/U4AgZBLqT0Ol0qt3NRFFs6D9q55VQCbGztpoo3G43nj59ikKh0LPr1BNiN4buCZslIFXMEGKM4cb6zc1NzUj4ZDKJlZUVU4IiZJAYSsK7d+9qBuQCwIsXL8yKiZCBYigJZ2ZmGibmXV1dNSUgQgaNoXtCj8ejax8h5GyGSkJRFJFMJuH3+wEcLxKayWRQLBZNDY6QQWCoJEwmk3C73erEvwBOXc+eENKaoZIwHo83NFN0avJfQvqd4XbCx48fY25uDgBQKBTonpAQgwyVhLFYDCzLqqXf9PQ01tbWzrUgDOkNND+N+Qwloc/nw+zsLAqFgtnxEBuj+Wk6w/A0+ADgcDjUfVQz2v/q56fheV59PHv2DAcHB01LSHI2w1Me+nw+jI2NIZ/P07yjA4bmpzGX4YqZbDaLyclJKIqCVCqFO3fumB0bIQPBUElYm9Li4cOHGB0dRaFQQLVa1SypTQjRx1BJmMlkNNf/09PTNL0FIQYZKgnHxsYaOnATQowxVBL+9ttv2N/f1+yj2lFCjDFUEobDYUxOTsLj8YBhGAiCoC4OQ0i76pu66g1Kf2RDSciyLHieRyaTgSzLWF5ebrpwKBkstaXy6g1KIp2HoST0+/2IxWJ0X0iICQxfjp7sJ7q+vk5theRUrfqdDjpDSehwOPDtt9/C4/GAZVmUSiVks1lbJOHm5iYuX74MgDoV28lp/U4HnaEkXF5eRiAQwN7envrLVj/zmpU+//xz9Tl1KraPs9ZFHGSGkjCZTDYM6rXLiIpffvkFly9fpkUvDep05Qr1O21kKAlrg3o3NjawsrKCQqGgzjdjtdu3b1P3uT7U6n6y2Y8G0Fu1sjSol9hev99P0qBeYnv9fj9pKAlbDeqlkpB0Ur/eT9Kg3j5EPVd6Cw3qJcRiukvCzc1NrKys4JNPPsHXX38Nt9uN5eXlTsZGyEDQlYSFQgEzMzNq75jnz5/TMmgdRpeU5rH7NI26kjCVSqFcLsPpdAIAFhcXsbOzg5s3b5oekCRJyOVyYFkWkiQhHA5rlmAjpB29ME2jriR0u91qAgLH7YSFQqEjSRgKhcDzPIDjhJyfn0c2mzX9ezqNSjJ7OKt5ww49qnQl4ckp7p1OZ8MJtbm5idu3b58rGEmSNNssy9LcNcQUrZo37PBjqSsJJUnC/v6+Jrjt7W11X6lUQjKZxJMnT84VDMdxcLlcmn0ulwuCIDT8AQ8PD3F4eKhuVyoVAMCvv/6Kjz76CH/88QeA5n/k2rF///03AIDnefU5AFPe20y1WlWfn+f99N7uvPes101LVkUHh8OhDA0NaR71+2rPzysejyuBQECzj2VZJZ/PNxz76NEjBQA96GHZQxTFc5/ziqIoukrCcDh8amO8oigdba6QZblhXywWw4MHD9Tto6MjlEoljI2NweFwoFqtYnx8HLu7u7bq0E1xtceOcVUqFUxMTDRctRmlKwkjkYimYqaZ2jJp58EwTMO4xFKp1LR2dHh4GMPDww3vP2l0dNQ2/3n1KK722DGuoSFDfV0aP0fPQZOTk6Ycc5ZWC436fL5zfzYhdmVOKpuEZVnNtiRJ8Pl81E5I+pqhDtydlM1mEY1G4ff7USwWDbcRDg8P49GjRw2XrFajuNpjx7jMjsmhKNSCTIiVbHU5SsggoiQkxGKUhIRYzHYVM+1oZ8RFp0dntPP5giCofWKLxSLS6bR6rCAIAACv1wtJkiDLsmlTOrQbox3iyOVyatPVyWM6GWPt8+fn59UBBa2c+9wypd+NRbxer/pcFEUlGAyacmynY4nH45rn9e8Nh8Nqt6hAIKCUy2VLYrRLHGjSXaz29+tkjNlsVuF5XtGTIuc9t3o2CUVR1PzjFUVRGIY597GdjoXnec1roihq+iEmk0mlXC6bekK1G6Nd4iiXy0o2m9Xsq/8B61SM9c5KQjPOrZ69JzxtxMV5ju10LF6vF+l0Wt2u9Yutfz/DMKZ3UDDyN7BDHMFgUH2ey+U0252KsR1mnFs9e0/YrFM30HxNjHaO7XQsgPbEWllZQSAQUE8kWZaRy+UAHN8vRiKRhp5E3YjRDnHUJ5csyyiVSpoYOhVjO8w4t3o2CVtp9Uc577FGnPX5tZOo/sa//qaeZVnMzMxAFMWux2iXOGqi0WjDSJ5ux9iOds6tnr0cbWfERTvHdjqWetFoFPl8XnNc/ewCtdq2kzMOdCNGu8QBHJ/QHMc1HNOpGNthxrnVs0nYzoiLTo/OMPL5iUQC0WgULMtClmXIsgxBEBpWuwJgyri1dmK0Sxw1GxsbTZsnOhVjO8w4t3o2Cc8acSEIgvqr2OnRGe3EAhxXMHi9XjUBM5kMGIYBy7KaSy6O4xAMBk2Js92/lx3iqBEEoSG5OhnjSScvLc0+t3q6A7ckSUgmk+qIi1gspv7jQ6EQ/H4/FhYWzjy2m7FIktQwcRbDMCiXywD+a8hnGAaiKJq6vEA7fy+7xAEcXzWIoohkMqn5nE7GyHEc8vk8EokEFhYW4Pf71Qo1s8+tnk5CQvpBz16OEtIvKAkJsRglISEWoyQkxGKUhIRYjJKQEItREhJiMUpCQixGSUiIxSgJCbEYJSEhFuu7Qb1En9roDZ7nEQqFAAD5fN6S0emDjkrCAcVxHMLhsDpHSiAQwMzMDKLRqNWhDRxKwgEVDAbVcXK1uTq7PSqdHKMkHGAcx2lGhufzeczMzFgY0WCiJBxgxWIRU1NTAKDOzxIOhy2OavBQxcwA4zgOHo8HuVwOxWIRhULB6pAGEo2sH2Aej8c2UwQOMrocHVAcx5m6eAoxjpJwAEmShHg8DlmWqUbUBuhylBCLUUlIiMUoCQmxGCUhIRajJCTEYpSEhFiMkpAQi1ESEmIxSkJCLPY/4Cobk9PapxIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC()\n",
      "Ideal\n",
      "p_values =  [0.33685739600079256, 0.6559955071426624]\n",
      "./Images/Ideal_ROC.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate_Model()\n",
      "np.unique(y_proba) =  [0.00047927 0.00048926 0.00070029 ... 0.99977329 0.99986356 0.99991617]\n",
      "np.unique(y_pred) =  [0 1]\n",
      "[[117910  32861]\n",
      " [  6139  20482]]\n",
      "\n",
      "[[0.66468612 0.18524511]\n",
      " [0.03460697 0.1154618 ]]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Precision_Metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 156\u001b[0m\n\u001b[1;32m    150\u001b[0m     Evaluate_Model(y_test, y_proba, y_pred, \u001b[38;5;241m0.5\u001b[39m, filename)    \n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m ()\n\u001b[0;32m--> 156\u001b[0m \u001b[43mIdealized_Results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 58\u001b[0m, in \u001b[0;36mIdealized_Results\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m Plot_Prediction(y_test, y_proba, filename, title)\n\u001b[1;32m     57\u001b[0m ROC(y_test, y_proba, [N_median, P_median], filename)    \n\u001b[0;32m---> 58\u001b[0m \u001b[43mEvaluate_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m ()\n\u001b[1;32m     61\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdeal_Left\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m, in \u001b[0;36mEvaluate_Model\u001b[0;34m(y_test, y_proba, y_pred, center, filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m y_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_test)\n\u001b[1;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m & Precision \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcr \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m  \u001b[43mPrecision_Metric\u001b[49m(y_test, y_pred)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m & Recall \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcr \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m  Recall_Metric(y_test, y_pred)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m & F1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcr \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m  F1_Metric(y_test, y_pred)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Precision_Metric' is not defined"
     ]
    }
   ],
   "source": [
    "def Move_Threshold(y_proba, y_test):\n",
    "    print ('Move_Threshold()')\n",
    "    n = 10\n",
    "    T = [x/n for x in range (n+1)]\n",
    "    \n",
    "    print (type(y_proba))\n",
    "    print (type(y_test))\n",
    "    y_test = np.array(y_test)\n",
    "    print (type(y_test))\n",
    "    N = y_proba[y_test==0]\n",
    "    P = y_proba[y_test==1]\n",
    "    print (len(N), len(P))\n",
    "\n",
    "    A = [['t', 'TN', 'FP', 'FN', 'TP', 'TPR', 'FPR']]\n",
    "    for t in T:\n",
    "        TN = len(N[N<t])\n",
    "        FP = len(N[N>t])\n",
    "        FN = len(P[P<t])\n",
    "        TP = len(P[P>t])\n",
    "        TPR = TP/len(P)\n",
    "        FPR = FP/len(N)\n",
    "        A.append([t, TN, FP, FN, TP, TPR, FPR])\n",
    "    display(pd.DataFrame(A))\n",
    "    \n",
    "    print ()\n",
    "\n",
    "\n",
    "def Idealized_Results():\n",
    "    print ('Idealized_Results()')\n",
    "    # Set randomness\n",
    "    np.random.seed(0) # NumPy\n",
    "    random.seed(0) # Python\n",
    "    tf.random.set_seed(0) # Tensorflow    \n",
    "\n",
    "    shape, scale = 3.7, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.gamma(shape, scale, 150771)\n",
    "    a = np.where(a>1.0, random.random(), a)\n",
    "    \n",
    "    shape, scale = 3.8, 0.1 # mean=4, std=2*sqrt(2)\n",
    "    b = np.random.gamma(shape, scale, 26621)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    Y_PROBA = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    N_median = np.median(a)\n",
    "    P_median = np.median(b)\n",
    "    \n",
    "    Move_Threshold(y_proba, y_test)\n",
    "    \n",
    "    filename = 'Ideal'\n",
    "    print (filename)\n",
    "    title = 'Original Example'\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Left'\n",
    "    title = 'Never Ambulance'\n",
    "    print (filename)\n",
    "    y_proba = 0.5 * y_proba\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "\n",
    "    filename = 'Ideal_Left_Shifted'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Shift_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Left_Linear_Transform'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Linear_Transform_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Right'\n",
    "    title = 'Always Ambulance'\n",
    "    print (filename)\n",
    "    y_proba = 0.5 * Y_PROBA + 0.5\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "\n",
    "    filename = 'Ideal_Right_Shifted'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Shift_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    filename = 'Ideal_Right_Linear_Transform'\n",
    "    title = ''\n",
    "    print (filename)\n",
    "    y_test, y_proba, y_pred, filename = Linear_Transform_y_proba(y_test, y_proba, y_pred, filename)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    y_proba = Y_PROBA\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Ideal_Tight'\n",
    "    title = 'Tight'\n",
    "    print (filename)\n",
    "    y_proba = 0.2 * Y_PROBA + 0.4\n",
    "    y_pred = K.round(y_proba)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [N_median, 0.5, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "   \n",
    "    y_proba = Y_PROBA\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Ideal_Shift_to_FP_equals_r_TP'\n",
    "    title = 'Transformed'\n",
    "    print (filename)\n",
    "    Plot_Prediction_Zoom(y_test, y_proba, 'Test', 'Test', 0.53, 0.73)    \n",
    "    y_test, y_proba, y_pred, p_target, filename_tmp = Shift_y_proba_to_FP_equals_r_TP(y_test, y_proba, 2.0, filename)\n",
    "    print ('type(y_test) = ', type(y_test))\n",
    "    N = y_proba[np.array(y_test)==0]\n",
    "    P = y_proba[np.array(y_test)==1]\n",
    "    display(N)\n",
    "    N_median = np.median(N)\n",
    "    P_median = np.median(P)\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Wide(y_test, y_proba, filename, title)\n",
    "    Plot_Prediction_Zoom(y_test, y_proba, filename, title, 0.4, 0.6)\n",
    "    ROC(y_test, y_proba, [N_median, P_median], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    print ()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "Idealized_Results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a285425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Awful_Results():\n",
    "    # Set randomness\n",
    "    np.random.seed(42) # NumPy\n",
    "    random.seed(42) # Python\n",
    "    tf.random.set_seed(42) # Tensorflow    \n",
    "    \n",
    "    \n",
    "    shape, scale = 1.0, 0.5 # mean=4, std=2*sqrt(2)\n",
    "    a = np.random.random(600)   \n",
    "    \n",
    "    b = np.random.random(100)    \n",
    "    b = np.where(b>1.0, random.random(), b)\n",
    "    b = 1-b\n",
    "    \n",
    "    y_proba = np.concatenate((a,b),axis=0)\n",
    "    y_pred = K.round(y_proba)\n",
    "    y_test = [0]*len(a) + [1]*len(b)\n",
    "    \n",
    "    filename = 'Awful'\n",
    "    title = 'Awful'\n",
    "    Plot_Prediction(y_test, y_proba, filename, title)\n",
    "    ROC(y_test, y_proba, [], filename)    \n",
    "    Evaluate_Model(y_test, y_proba, y_pred, 0.5, filename)    \n",
    "    \n",
    "#Awful_Results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4a296",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Models(Features = 'Hard', Tomek = 0, Version = 1):\n",
    "    \n",
    "    if Features == 'Hard':\n",
    "        read_filename_features = '_Thin'\n",
    "        write_filename_features = '_Hard'\n",
    "    if Features == 'Medium':\n",
    "        read_filename_features = '_Really_Thin'\n",
    "        write_filename_features = '_Medium'\n",
    "    if Features == 'Easy':\n",
    "        read_filename_features = '_Thin_to_Minimal'\n",
    "        write_filename_features = '_Easy'\n",
    "    if Tomek==0:\n",
    "        read_filename_tomek = '_before_Tomek'\n",
    "        write_filename_tomek = '_Tomek_0'\n",
    "    if Tomek==1:\n",
    "        read_filename_tomek = '_after_Tomek'\n",
    "        write_filename_tomek = '_Tomek_1'\n",
    "    if Tomek==2:\n",
    "        read_filename_tomek = '_after_Tomek_Twice'\n",
    "        write_filename_tomek = '_Tomek_2'\n",
    "    if Version==1:\n",
    "        filename_version = '_v1'\n",
    "        random_seed = 0\n",
    "    if Version==2:\n",
    "        filename_version = '_v2'\n",
    "        random_seed = 42\n",
    "\n",
    "    X_train = pd.read_csv('../../Big_Files/X_train' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_train = pd.read_csv('../../Big_Files/y_train' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "    X_test = pd.read_csv('../../Big_Files/X_test' + read_filename_features + read_filename_tomek + filename_version + '.csv')\n",
    "    y_test = pd.read_csv('../../Big_Files/y_test' + read_filename_features + read_filename_tomek + filename_version + '.csv').squeeze()\n",
    "\n",
    "    N = len(y_train)\n",
    "    n = len(y_train[y_train==1])\n",
    "    p = (N-n)/n\n",
    "    alpha_balanced = p/(p+1)\n",
    "    print ('p = ', p)\n",
    "    print ('alpha_balanced = ', alpha_balanced)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'LRC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5' + filename_version\n",
    "    title = 'LogReg'\n",
    "    Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, 0.5, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "\n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'LRC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced' + filename_version\n",
    "    title = 'LogReg'\n",
    "    Logistic_Regression_Classifier(X_train, X_test, y_train, y_test, alpha_balanced, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5' + filename_version\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, 0.5, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "     \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'BRFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced' + filename_version\n",
    "    title = 'BRForest'\n",
    "    Balanced_Random_Forest_Classifier(X_train, X_test, y_train, y_test, alpha_balanced, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'AdaBoost'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'AdaBoost'\n",
    "    AdaBoost(X_train, X_test, y_train, y_test, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'Bagging'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'BalBag'\n",
    "    Bagging(X_train, X_test, y_train, y_test, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'RUSBoost'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'RUSBoost'\n",
    "    estimator = DecisionTreeClassifier(\n",
    "        max_depth=1,\n",
    "#        class_weight={0:(1+r_target)/(2*r_target), 1:(1+r_target)/(2*1)},\n",
    "    )\n",
    "    RUSBoost_Classifier(X_train, X_test, y_train, y_test, estimator, filename, title)\n",
    "\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "    \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'EEC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '' + filename_version\n",
    "    title = 'EasyEns'\n",
    "    Easy_Ensemble_Classifier(X_train, X_test, y_train, y_test, filename, title)\n",
    "    \n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_0' + filename_version\n",
    "    title = 'Focal $\\gamma=0.0$'\n",
    "    print (filename)\n",
    "    print ('alpha_balanced = ', alpha_balanced)\n",
    "    gamma = 0.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha_balanced, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    "        \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'KBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5_gamma_0_0' + filename_version\n",
    "    title = 'Focal $\\gamma=0.0$'\n",
    "    print (filename)\n",
    "    gamma = 0.0\n",
    "    epochs=20\n",
    "    Keras_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, 0.5, gamma, epochs, filename, title)\n",
    "\n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_0_5_gamma_0_0_5_gamma_1_1_0' + filename_version\n",
    "    gamma_0 = 0.5\n",
    "    gamma_1 = 1.0\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = 0.5' + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, 0.5, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_1_0_gamma_1_0_5' + filename_version\n",
    "    gamma_0 = 1.0\n",
    "    gamma_1 = 0.5\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = 0.5' + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, 0.5, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_0_5_gamma_1_2_0' + filename_version\n",
    "    gamma_0 = 0.5\n",
    "    gamma_1 = 2.0\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = 0.5' + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, 0.5, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_0_5_gamma_1_1_0' + filename_version\n",
    "    gamma_0 = 0.5\n",
    "    gamma_1 = 1.0\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = ' + \"{:.2f}\".format(alpha_balanced) + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha_balanced, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_1_0_gamma_1_0_5' + filename_version\n",
    "    gamma_0 = 1.0\n",
    "    gamma_1 = 0.5\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = ' + \"{:.2f}\".format(alpha_balanced) + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha_balanced, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "    np.random.seed(random_seed) # NumPy\n",
    "    random.seed(random_seed) # Python\n",
    "    tf.random.set_seed(random_seed) # Tensorflow\n",
    " \n",
    "    print ()\n",
    "    print ('------------------------------------------')\n",
    "    filename_model = 'OBFC'\n",
    "    filename = filename_model + write_filename_features + write_filename_tomek + '_alpha_balanced_gamma_0_0_5_gamma_1_2_0' + filename_version\n",
    "    gamma_0 = 0.5\n",
    "    gamma_1 = 2.0\n",
    "    epochs = 20\n",
    "    title = 'OBFC $\\alpha = ' + \"{:.2f}\".format(alpha_balanced) + '\\gamma_0 = ' + \"{:.2f}\".format(gamma_0) + ' \\gamma_1 = ' + \"{:.2f}\".format(gamma_1) + '$'\n",
    "    print (title)\n",
    "    Our_Binary_Focal_Crossentropy(X_train, X_test, y_train, y_test, alpha_balanced, gamma_0, gamma_1, epochs, filename, title)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7623de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create_Files_for_Value_Counts_y_proba()\n",
    "Create_Files_for_Analyze_Prediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e41e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Run_Models(Features = 'Hard', Tomek = 0, Version = 1)\n",
    "#Run_Models(Features = 'Hard', Tomek = 1, Version = 1)\n",
    "#Run_Models(Features = 'Hard', Tomek = 2, Version = 1)\n",
    "Run_Models(Features = 'Medium', Tomek = 0, Version = 1)\n",
    "#Run_Models(Features = 'Medium', Tomek = 1, Version = 1)\n",
    "#Run_Models(Features = 'Medium', Tomek = 2, Version = 1)\n",
    "Run_Models(Features = 'Easy', Tomek = 0, Version = 1)\n",
    "\n",
    "Run_Models(Features = 'Hard', Tomek = 0, Version = 2)\n",
    "#Run_Models(Features = 'Hard', Tomek = 1, Version = 2)\n",
    "#Run_Models(Features = 'Hard', Tomek = 2, Version = 2)\n",
    "Run_Models(Features = 'Medium', Tomek = 0, Version = 2)\n",
    "#Run_Models(Features = 'Medium', Tomek = 1, Version = 2)\n",
    "#Run_Models(Features = 'Medium', Tomek = 2, Version = 2)\n",
    "Run_Models(Features = 'Easy', Tomek = 0, Version = 2)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8d5ea7d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72178501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_2_11",
   "language": "python",
   "name": "tensorflow_2_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
